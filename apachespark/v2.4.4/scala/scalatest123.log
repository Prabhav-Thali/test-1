Using `mvn` from path: /usr/bin/mvn
[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Build Order:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Spark Project Parent POM                                           [pom]
[[1;34mINFO[m] Spark Project Tags                                                 [jar]
[[1;34mINFO[m] Spark Project Sketch                                               [jar]
[[1;34mINFO[m] Spark Project Local DB                                             [jar]
[[1;34mINFO[m] Spark Project Networking                                           [jar]
[[1;34mINFO[m] Spark Project Shuffle Streaming Service                            [jar]
[[1;34mINFO[m] Spark Project Unsafe                                               [jar]
[[1;34mINFO[m] Spark Project Launcher                                             [jar]
[[1;34mINFO[m] Spark Project Core                                                 [jar]
[[1;34mINFO[m] Spark Project ML Local Library                                     [jar]
[[1;34mINFO[m] Spark Project GraphX                                               [jar]
[[1;34mINFO[m] Spark Project Streaming                                            [jar]
[[1;34mINFO[m] Spark Project Catalyst                                             [jar]
[[1;34mINFO[m] Spark Project SQL                                                  [jar]
[[1;34mINFO[m] Spark Project ML Library                                           [jar]
[[1;34mINFO[m] Spark Project Tools                                                [jar]
[[1;34mINFO[m] Spark Project Hive                                                 [jar]
[[1;34mINFO[m] Spark Project REPL                                                 [jar]
[[1;34mINFO[m] Spark Project Assembly                                             [pom]
[[1;34mINFO[m] Spark Integration for Kafka 0.10                                   [jar]
[[1;34mINFO[m] Kafka 0.10+ Source for Structured Streaming                        [jar]
[[1;34mINFO[m] Spark Project Examples                                             [jar]
[[1;34mINFO[m] Spark Integration for Kafka 0.10 Assembly                          [jar]
[[1;34mINFO[m] Spark Avro                                                         [jar]
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------< [0;36morg.apache.spark:spark-parent_2.11[0;1m >-----------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Parent POM 2.4.4                           [1/24][m
[[1;34mINFO[m] [1m--------------------------------[ pom ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-parent_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-parent_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-parent_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-parent_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-parent_2.11[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-parent_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-parent_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-parent_2.11[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-parent_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-parent_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 67 milliseconds.[0m
[36mRun starting. Expected test count is: 0[0m
[32mDiscoverySuite:[0m
[36mRun completed in 123 milliseconds.[0m
[36mTotal number of tests run: 0[0m
[36mSuites: completed 1, aborted 0[0m
[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m
[33mNo tests were executed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------< [0;36morg.apache.spark:spark-tags_2.11[0;1m >------------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Tags 2.4.4                                 [2/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/common/tags/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/common/tags/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/common/tags/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:11:33 AM [1.117s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/common/tags/src/test/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:11:34 AM [0.138s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-tags_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-tags_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 297 milliseconds.[0m
[36mRun starting. Expected test count is: 0[0m
[32mDiscoverySuite:[0m
[36mRun completed in 336 milliseconds.[0m
[36mTotal number of tests run: 0[0m
[36mSuites: completed 1, aborted 0[0m
[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m
[33mNo tests were executed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------< [0;36morg.apache.spark:spark-sketch_2.11[0;1m >-----------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Sketch 2.4.4                               [3/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/common/sketch/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/common/sketch/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/common/sketch/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:11:44 AM [0.244s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/common/sketch/src/test/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/spark/common/tags/target/scala-2.11/classes
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:11:45 AM [0.364s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-sketch_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-sketch_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 276 milliseconds.[0m
[36mRun starting. Expected test count is: 29[0m
[32mBloomFilterSuite:[0m
[32m- accuracy - Byte[0m
[32m- mergeInPlace - Byte[0m
[32m- accuracy - Short[0m
[32m- mergeInPlace - Short[0m
[32m- accuracy - Int[0m
[32m- mergeInPlace - Int[0m
[32m- accuracy - Long[0m
[32m- mergeInPlace - Long[0m
[32m- accuracy - String[0m
[32m- mergeInPlace - String[0m
[32m- incompatible merge[0m
[32mBitArraySuite:[0m
[32m- error case when create BitArray[0m
[32m- bitSize[0m
[32m- set[0m
[32m- normal operation[0m
[32m- merge[0m
[32mCountMinSketchSuite:[0m
[32m- accuracy - Byte[0m
[32m- mergeInPlace - Byte[0m
[32m- accuracy - Short[0m
[32m- mergeInPlace - Short[0m
[32m- accuracy - Int[0m
[32m- mergeInPlace - Int[0m
[32m- accuracy - Long[0m
[32m- mergeInPlace - Long[0m
[32m- accuracy - String[0m
[32m- mergeInPlace - String[0m
[32m- accuracy - Byte array[0m
[32m- mergeInPlace - Byte array[0m
[32m- incompatible merge[0m
[36mRun completed in 23 seconds, 672 milliseconds.[0m
[36mTotal number of tests run: 29[0m
[36mSuites: completed 4, aborted 0[0m
[36mTests: succeeded 29, failed 0, canceled 0, ignored 0, pending 0[0m
[32mAll tests passed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------< [0;36morg.apache.spark:spark-kvstore_2.11[0;1m >-----------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Local DB 2.4.4                             [4/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/common/kvstore/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/common/kvstore/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/common/kvstore/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:12:28 AM [0.193s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:12:29 AM [0.245s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-kvstore_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 228 milliseconds.[0m
[36mRun starting. Expected test count is: 0[0m
[32mDiscoverySuite:[0m
[36mRun completed in 326 milliseconds.[0m
[36mTotal number of tests run: 0[0m
[36mSuites: completed 1, aborted 0[0m
[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m
[33mNo tests were executed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------< [0;36morg.apache.spark:spark-network-common_2.11[0;1m >-------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Networking 2.4.4                           [5/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/common/network-common/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/common/network-common/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/common/network-common/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:12:33 AM [0.360s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:12:34 AM [0.258s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.0.2:test-jar[m [1m(prepare-test-jar)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/test/spark/common/network-common/target/spark-network-common_2.11-2.4.4-tests.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-network-common_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-network-common_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 247 milliseconds.[0m
[36mRun starting. Expected test count is: 0[0m
[32mDiscoverySuite:[0m
[36mRun completed in 284 milliseconds.[0m
[36mTotal number of tests run: 0[0m
[36mSuites: completed 1, aborted 0[0m
[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m
[33mNo tests were executed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------< [0;36morg.apache.spark:spark-network-shuffle_2.11[0;1m >-------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Shuffle Streaming Service 2.4.4            [6/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/common/network-shuffle/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/common/network-shuffle/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/common/network-shuffle/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:12:45 AM [0.210s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/spark/common/network-common/target/spark-network-common_2.11-2.4.4-tests.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:12:45 AM [0.177s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-network-shuffle_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 245 milliseconds.[0m
[36mRun starting. Expected test count is: 0[0m
[32mDiscoverySuite:[0m
[36mRun completed in 335 milliseconds.[0m
[36mTotal number of tests run: 0[0m
[36mSuites: completed 1, aborted 0[0m
[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m
[33mNo tests were executed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------< [0;36morg.apache.spark:spark-unsafe_2.11[0;1m >-----------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Unsafe 2.4.4                               [7/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/common/unsafe/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/common/unsafe/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/common/unsafe/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:12:52 AM [0.092s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/common/unsafe/src/test/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:12:52 AM [0.221s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-unsafe_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 401 milliseconds.[0m
[36mRun starting. Expected test count is: 19[0m
[32mUTF8StringPropertyCheckSuite:[0m
[32m- toString[0m
[32m- numChars[0m
[32m- startsWith[0m
[32m- endsWith[0m
[32m- toUpperCase[0m
[32m- toLowerCase[0m
[32m- compare[0m
[32m- substring[0m
[32m- contains[0m
[32m- trim, trimLeft, trimRight[0m
[32m- reverse[0m
[32m- indexOf[0m
[32m- repeat[0m
[32m- lpad, rpad[0m
[32m- concat[0m
[32m- concatWs[0m
[33m- split !!! IGNORED !!![0m
[32m- levenshteinDistance[0m
[32m- hashCode[0m
[32m- equals[0m
[36mRun completed in 1 second, 269 milliseconds.[0m
[36mTotal number of tests run: 19[0m
[36mSuites: completed 2, aborted 0[0m
[36mTests: succeeded 19, failed 0, canceled 0, ignored 1, pending 0[0m
[32mAll tests passed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------< [0;36morg.apache.spark:spark-launcher_2.11[0;1m >----------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Launcher 2.4.4                             [8/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/launcher/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/launcher/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/launcher/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:13:17 AM [0.163s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:13:18 AM [0.251s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-launcher_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-launcher_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 198 milliseconds.[0m
[36mRun starting. Expected test count is: 0[0m
[32mDiscoverySuite:[0m
[36mRun completed in 231 milliseconds.[0m
[36mTotal number of tests run: 0[0m
[36mSuites: completed 1, aborted 0[0m
[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m
[33mNo tests were executed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------< [0;36morg.apache.spark:spark-core_2.11[0;1m >------------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Core 2.4.4                                 [9/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/core/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/core/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(default)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 37 resources
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:13:33 AM [3.793s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 68 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-leg-rc/2.52.0/selenium-leg-rc-2.52.0.jar:/home/test/.m2/repository/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/home/test/.m2/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpmime/4.5.6/httpmime-4.5.6.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/net/sourceforge/htmlunit/htmlunit-core-js/2.17/htmlunit-core-js-2.17.jar:/home/test/.m2/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-firefox-driver/2.52.0/selenium-firefox-driver-2.52.0.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.2.12.v20150709/websocket-common-9.2.12.v20150709.jar:/home/test/.m2/repository/org/apache/commons/commons-exec/1.3/commons-exec-1.3.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/calcite/calcite-avatica/1.2.0-incubating/calcite-avatica-1.2.0-incubating.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/net/java/dev/jna/jna-platform/4.1.0/jna-platform-4.1.0.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-api/2.52.0/selenium-api-2.52.0.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-edge-driver/2.52.0/selenium-edge-driver-2.52.0.jar:/home/test/.m2/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-htmlunit-driver/2.52.0/selenium-htmlunit-driver-2.52.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/derby/derby/10.12.1.1/derby-10.12.1.1.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-chrome-driver/2.52.0/selenium-chrome-driver-2.52.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-support/2.52.0/selenium-support-2.52.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-safari-driver/2.52.0/selenium-safari-driver-2.52.0.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-java/2.52.0/selenium-java-2.52.0.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/webbitserver/webbit/0.4.14/webbit-0.4.14.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/home/test/.m2/repository/net/sourceforge/nekohtml/nekohtml/1.9.22/nekohtml-1.9.22.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/test/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.2.12.v20150709/websocket-api-9.2.12.v20150709.jar:/home/test/.m2/repository/org/w3c/css/sac/1.3/sac-1.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/xalan/serializer/2.7.2/serializer-2.7.2.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/xalan/xalan/2.7.2/xalan-2.7.2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/iq80/snappy/snappy/0.2/snappy-0.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/org/apache/curator/curator-test/2.6.0/curator-test-2.6.0.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/net/sourceforge/htmlunit/htmlunit/2.18/htmlunit-2.18.jar:/home/test/.m2/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/home/test/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/test/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.2.12.v20150709/websocket-client-9.2.12.v20150709.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-library/1.3/hamcrest-library-1.3.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-ie-driver/2.52.0/selenium-ie-driver-2.52.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/cglib/cglib-nodep/2.1_3/cglib-nodep-2.1_3.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/net/sourceforge/cssparser/cssparser/0.9.16/cssparser-0.9.16.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/org/spark-project/hive/hive-metastore/1.2.1.spark2/hive-metastore-1.2.1.spark2.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-remote-driver/2.52.0/selenium-remote-driver-2.52.0.jar:/home/test/spark/launcher/target/scala-2.11/test-classes:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/net/java/dev/jna/jna/4.1.0/jna-4.1.0.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:13:40 AM [5.862s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-core_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-core_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 13 seconds, 184 milliseconds.[0m
[36mRun starting. Expected test count is: 2209[0m
[32mExternalSorterSuite:[0m
[32m- empty data stream with kryo ser[0m
[32m- empty data stream with java ser[0m
[32m- few elements per partition with kryo ser[0m
[32m- few elements per partition with java ser[0m
[32m- empty partitions with spilling with kryo ser[0m
[32m- empty partitions with spilling with java ser[0m
[32m- spilling in local cluster with kryo ser[0m
[32m- spilling in local cluster with java ser[0m
[32m- spilling in local cluster with many reduce tasks with kryo ser[0m
[32m- spilling in local cluster with many reduce tasks with java ser[0m
[32m- cleanup of intermediate files in sorter[0m
[32m- cleanup of intermediate files in sorter with failures[0m
[32m- cleanup of intermediate files in shuffle[0m
[32m- cleanup of intermediate files in shuffle with failures[0m
[32m- no sorting or partial aggregation with kryo ser[0m
[32m- no sorting or partial aggregation with java ser[0m
[32m- no sorting or partial aggregation with spilling with kryo ser[0m
[32m- no sorting or partial aggregation with spilling with java ser[0m
[32m- sorting, no partial aggregation with kryo ser[0m
[32m- sorting, no partial aggregation with java ser[0m
[32m- sorting, no partial aggregation with spilling with kryo ser[0m
[32m- sorting, no partial aggregation with spilling with java ser[0m
[32m- partial aggregation, no sorting with kryo ser[0m
[32m- partial aggregation, no sorting with java ser[0m
[32m- partial aggregation, no sorting with spilling with kryo ser[0m
[32m- partial aggregation, no sorting with spilling with java ser[0m
[32m- partial aggregation and sorting with kryo ser[0m
[32m- partial aggregation and sorting with java ser[0m
[32m- partial aggregation and sorting with spilling with kryo ser[0m
[32m- partial aggregation and sorting with spilling with java ser[0m
[32m- sort without breaking sorting contracts with kryo ser[0m
[32m- sort without breaking sorting contracts with java ser[0m
[33m- sort without breaking timsort contracts for large arrays !!! IGNORED !!![0m
[32m- spilling with hash collisions[0m
[32m- spilling with many hash collisions[0m
[31m- spilling with hash collisions using the Int.MaxValue key *** FAILED ***[0m
[31m  java.lang.IllegalStateException: failed to create a child event loop[0m
[31m  at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:88)[0m
[31m  at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:58)[0m
[31m  at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:47)[0m
[31m  at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:59)[0m
[31m  at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:77)[0m
[31m  at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:72)[0m
[31m  at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:59)[0m
[31m  at org.apache.spark.network.util.NettyUtils.createEventLoop(NettyUtils.java:50)[0m
[31m  at org.apache.spark.network.server.TransportServer.init(TransportServer.java:95)[0m
[31m  at org.apache.spark.network.server.TransportServer.<init>(TransportServer.java:75)[0m
[31m  ...[0m
[31m  Cause: io.netty.channel.ChannelException: failed to open a new selector[0m
[31m  at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:175)[0m
[31m  at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:149)[0m
[31m  at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:127)[0m
[31m  at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:36)[0m
[31m  at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:84)[0m
[31m  at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:58)[0m
[31m  at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:47)[0m
[31m  at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:59)[0m
[31m  at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:77)[0m
[31m  at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:72)[0m
[31m  ...[0m
[31m  Cause: java.io.IOException: Too many open files[0m
[31m  at sun.nio.ch.IOUtil.makePipe(Native Method)[0m
[31m  at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:65)[0m
[31m  at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)[0m
[31m  at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:173)[0m
[31m  at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:149)[0m
[31m  at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:127)[0m
[31m  at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:36)[0m
[31m  at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:84)[0m
[31m  at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:58)[0m
[31m  at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:47)[0m
[31m  ...[0m
[32m- spilling with null keys and values[0m
[32m- sorting updates peak execution memory[0m
[32m- force to spill for external sorter[0m
[32mDAGSchedulerSuite:[0m
[32m- [SPARK-3353] parent stage should have lower stage id[0m
[32m- [SPARK-13902] Ensure no duplicate stages are created[0m
[32m- All shuffle files on the slave should be cleaned up when slave lost[0m
[32m- zero split job[0m
[32m- run trivial job[0m
[32m- run trivial job w/ dependency[0m
[32m- equals and hashCode AccumulableInfo[0m
[32m- cache location preferences w/ dependency[0m
[32m- regression test for getCacheLocs[0m
[32m- getMissingParentStages should consider all ancestor RDDs' cache statuses[0m
[32m- avoid exponential blowup when getting preferred locs list[0m
[32m- unserializable task[0m
[32m- trivial job failure[0m
[32m- trivial job cancellation[0m
[32m- job cancellation no-kill backend[0m
[32m- run trivial shuffle[0m
[32m- run trivial shuffle with fetch failure[0m
[32m- shuffle files not lost when slave lost with shuffle service[0m
[32m- shuffle files lost when worker lost with shuffle service[0m
[32m- shuffle files lost when worker lost without shuffle service[0m
[32m- shuffle files not lost when executor failure with shuffle service[0m
[32m- shuffle files lost when executor failure without shuffle service[0m
[32m- Single stage fetch failure should not abort the stage.[0m
[32m- Multiple consecutive stage fetch failures should lead to job being aborted.[0m
[32m- Failures in different stages should not trigger an overall abort[0m
[32m- Non-consecutive stage failures don't trigger abort[0m
[32m- trivial shuffle with multiple fetch failures[0m
[32m- Retry all the tasks on a resubmitted attempt of a barrier stage caused by FetchFailure[0m
[32m- Retry all the tasks on a resubmitted attempt of a barrier stage caused by TaskKilled[0m
[32m- Fail the job if a barrier ResultTask failed[0m
[32m- late fetch failures don't cause multiple concurrent attempts for the same map stage[0m
[32m- extremely late fetch failures don't cause multiple concurrent attempts for the same stage[0m
[32m- task events always posted in speculation / when stage is killed[0m
[32m- ignore late map task completions[0m
[32m- run shuffle with map stage failure[0m
[32m- shuffle fetch failure in a reused shuffle dependency[0m
[32m- don't submit stage until its dependencies map outputs are registered (SPARK-5259)[0m
[32m- register map outputs correctly after ExecutorLost and task Resubmitted[0m
[32m- failure of stage used by two jobs[0m
[32m- stage used by two jobs, the first no longer active (SPARK-6880)[0m
[32m- stage used by two jobs, some fetch failures, and the first job no longer active (SPARK-6880)[0m
[32m- run trivial shuffle with out-of-band executor failure and retry[0m
[32m- recursive shuffle failures[0m
[32m- cached post-shuffle[0m
[32m- misbehaved accumulator should not crash DAGScheduler and SparkContext[0m
[32m- misbehaved accumulator should not impact other accumulators[0m
[32m- misbehaved resultHandler should not crash DAGScheduler and SparkContext[0m
[32m- getPartitions exceptions should not crash DAGScheduler and SparkContext (SPARK-8606)[0m
[32m- getPreferredLocations errors should not crash DAGScheduler and SparkContext (SPARK-8606)[0m
[32m- accumulator not calculated for resubmitted result stage[0m
[32m- accumulator not calculated for resubmitted task in result stage[0m
[32m- accumulators are updated on exception failures and task killed[0m
[32m- reduce tasks should be placed locally with map output[0m
[32m- reduce task locality preferences should only include machines with largest map outputs[0m
[32m- stages with both narrow and shuffle dependencies use narrow ones for locality[0m
[32m- Spark exceptions should include call site in stack trace[0m
[32m- catch errors in event loop[0m
[32m- simple map stage submission[0m
[32m- map stage submission with reduce stage also depending on the data[0m
[32m- map stage submission with fetch failure[0m
[32m- map stage submission with multiple shared stages and failures[0m
[32m- Trigger mapstage's job listener in submitMissingTasks[0m
[32m- map stage submission with executor failure late map task completions[0m
[32m- getShuffleDependencies correctly returns only direct shuffle parents[0m
[32m- SPARK-17644: After one stage is aborted for too many failed attempts, subsequent stagesstill behave correctly on fetch failures[0m
[32m- [SPARK-19263] DAGScheduler should not submit multiple active tasksets, even with late completions from earlier stage attempts[0m
[32m- task end event should have updated accumulators (SPARK-20342)[0m
[31m- Barrier task failures from the same stage attempt don't trigger multiple stage retries *** FAILED ***[0m
[31m  ArrayBuffer(0) did not equal List(0) (DAGSchedulerSuite.scala:2618)[0m
[32m- Barrier task failures from a previous stage attempt don't trigger stage retry[0m
[32m- SPARK-23207: retry all the succeeding stages when the map stage is indeterminate[0m
[32m- SPARK-23207: cannot rollback a result stage[0m
[32m- SPARK-23207: local checkpoint fail to rollback (checkpointed before)[0m
[32m- SPARK-23207: local checkpoint fail to rollback (checkpointing now)[0m
[32m- SPARK-23207: reliable checkpoint can avoid rollback (checkpointed before)[0m
[32m- SPARK-23207: reliable checkpoint fail to rollback (checkpointing now)[0m
[32m- SPARK-28699: abort stage if parent stage is indeterminate stage[0m
[32mFsHistoryProviderSuite:[0m
[32m- Parse application logs (inMemory = true)[0m
[32m- Parse application logs (inMemory = false)[0m
[32m- SPARK-3697: ignore files that cannot be read.[0m
[32m- history file is renamed from inprogress to completed[0m
[32m- Parse logs that application is not started[0m
[32m- SPARK-5582: empty log directory[0m
[32m- apps with multiple attempts with order[0m
[32m- log cleaner[0m
[32m- should not clean inprogress application with lastUpdated time less than maxTime[0m
[32m- log cleaner for inProgress files[0m
[32m- Event log copy[0m
[32m- SPARK-8372: new logs with no app ID are ignored[0m
[32m- provider correctly checks whether fs is in safe mode[0m
[32m- provider waits for safe mode to finish before initializing[0m
[32m- provider reports error after FS leaves safe mode[0m
[32m- ignore hidden files[0m
[32m- support history server ui admin acls[0m
[32m- mismatched version discards old listing[0m
[32m- invalidate cached UI[0m
[32m- clean up stale app information[0m
[32m- SPARK-21571: clean up removes invalid history files[0m
[32m- always find end event for finished apps[0m
[32m- parse event logs with optimizations off[0m
[32m- SPARK-24948: blacklist files we don't have read permission on[0m
[32m- check in-progress event logs absolute length[0m
[32mRDDSuite:[0m
[32m- basic operations[0m
[32m- serialization[0m
[32m- countApproxDistinct[0m
[32m- SparkContext.union[0m
[32m- SparkContext.union parallel partition listing[0m
[32m- SparkContext.union creates UnionRDD if at least one RDD has no partitioner[0m
[32m- SparkContext.union creates PartitionAwareUnionRDD if all RDDs have partitioners[0m
[32m- PartitionAwareUnionRDD raises exception if at least one RDD has no partitioner[0m
[32m- SPARK-23778: empty RDD in union should not produce a UnionRDD[0m
[32m- partitioner aware union[0m
[32m- UnionRDD partition serialized size should be small[0m
[32m- fold[0m
[32m- fold with op modifying first arg[0m
[32m- aggregate[0m
[32m- treeAggregate[0m
[32m- treeAggregate with ops modifying first args[0m
[32m- treeReduce[0m
[32m- basic caching[0m
[32m- caching with failures[0m
[32m- empty RDD[0m
[32m- repartitioned RDDs[0m
[32m- repartitioned RDDs perform load balancing[0m
[32m- coalesced RDDs[0m
[32m- coalesced RDDs with locality[0m
[32m- coalesced RDDs with partial locality[0m
[32m- coalesced RDDs with locality, large scale (10K partitions)[0m
[32m- coalesced RDDs with partial locality, large scale (10K partitions)[0m
[32m- coalesced RDDs with locality, fail first pass[0m
[32m- zipped RDDs[0m
[32m- partition pruning[0m
[32m- collect large number of empty partitions[0m
[32m- take[0m
[32m- top with predefined ordering[0m
[32m- top with custom ordering[0m
[32m- takeOrdered with predefined ordering[0m
[32m- takeOrdered with limit 0[0m
[32m- takeOrdered with custom ordering[0m
[32m- isEmpty[0m
[32m- sample preserves partitioner[0m
[32m- takeSample[0m
[32m- takeSample from an empty rdd[0m
[32m- randomSplit[0m
[32m- runJob on an invalid partition[0m
[32m- sort an empty RDD[0m
[32m- sortByKey[0m
[32m- sortByKey ascending parameter[0m
[32m- sortByKey with explicit ordering[0m
[32m- repartitionAndSortWithinPartitions[0m
[32m- cartesian on empty RDD[0m
[32m- cartesian on non-empty RDDs[0m
[32m- intersection[0m
[32m- intersection strips duplicates in an input[0m
[32m- zipWithIndex[0m
[32m- zipWithIndex with a single partition[0m
[32m- zipWithIndex chained with other RDDs (SPARK-4433)[0m
[32m- zipWithUniqueId[0m
[32m- retag with implicit ClassTag[0m
[32m- parent method[0m
[32m- getNarrowAncestors[0m
[32m- getNarrowAncestors with multiple parents[0m
[32m- getNarrowAncestors with cycles[0m
[32m- task serialization exception should not hang scheduler[0m
[32m- RDD.partitions() fails fast when partitions indicies are incorrect (SPARK-13021)[0m
[32m- nested RDDs are not supported (SPARK-5063)[0m
[32m- actions cannot be performed inside of transformations (SPARK-5063)[0m
[32m- custom RDD coalescer[0m
[32m- SPARK-18406: race between end-of-task and completion iterator read lock release[0m
[32m- SPARK-23496: order of input partitions can result in severe skew in coalesce[0m
[32m- cannot run actions after SparkContext has been stopped (SPARK-5063)[0m
[32m- cannot call methods on a stopped SparkContext (SPARK-5063)[0m
[32mExecutorSuite:[0m
[32m- SPARK-15963: Catch `TaskKilledException` correctly in Executor.TaskRunner[0m
[32m- SPARK-19276: Handle FetchFailedExceptions that are hidden by user exceptions[0m
[32m- Executor's worker threads should be UninterruptibleThread[0m
[32m- SPARK-19276: OOMs correctly handled with a FetchFailure[0m
[32m- SPARK-23816: interrupts are not masked by a FetchFailure[0m
[32m- Gracefully handle error in task deserialization[0m
[32mSerDeUtilSuite:[0m
[32m- Converting an empty pair RDD to python does not throw an exception (SPARK-5441)[0m
[32m- Converting an empty python RDD to pair RDD does not throw an exception (SPARK-5441)[0m
[32mUtilsSuite:[0m
[32m- truncatedString[0m
[32m- timeConversion[0m
[32m- Test byteString conversion[0m
[32m- bytesToString[0m
[32m- copyStream[0m
[32m- memoryStringToMb[0m
[32m- splitCommandString[0m
[32m- string formatting of time durations[0m
[32m- reading offset bytes of a file[0m
[32m- reading offset bytes of a file (compressed)[0m
[32m- reading offset bytes across multiple files[0m
[32m- reading offset bytes across multiple files (compressed)[0m
[32m- deserialize long value[0m
[32m- writeByteBuffer should not change ByteBuffer position[0m
[32m- get iterator size[0m
[32m- getIteratorZipWithIndex[0m
[32m- doesDirectoryContainFilesNewerThan[0m
[32m- resolveURI[0m
[32m- resolveURIs with multiple paths[0m
[32m- nonLocalPaths[0m
[32m- isBindCollision[0m
[32m- log4j log level change[0m
[32m- deleteRecursively[0m
[32m- loading properties from file[0m
[32m- timeIt with prepare[0m
[32m- fetch hcfs dir[0m
[32m- shutdown hook manager[0m
[32m- isInDirectory[0m
[32m- circular buffer: if nothing was written to the buffer, display nothing[0m
[32m- circular buffer: if the buffer isn't full, print only the contents written[0m
[32m- circular buffer: data written == size of the buffer[0m
[32m- circular buffer: multiple overflow[0m
[32m- nanSafeCompareDoubles[0m
[32m- nanSafeCompareFloats[0m
[32m- isDynamicAllocationEnabled[0m
[32m- getDynamicAllocationInitialExecutors[0m
[32m- Set Spark CallerContext[0m
[32m- encodeFileNameToURIRawPath[0m
[32m- decodeFileNameInURI[0m
[32m- Kill process[0m
[32m- chi square test of randomizeInPlace[0m
[32m- redact sensitive information[0m
[32m- tryWithSafeFinally[0m
[32m- tryWithSafeFinallyAndFailureCallbacks[0m
[32m- load extensions[0m
[32m- check Kubernetes master URL[0m
[31m- Safe getSimpleName *** FAILED ***[0m
[31m  Expected exception java.lang.InternalError to be thrown, but no exception was thrown (UtilsSuite.scala:1179)[0m
[32m- stringHalfWidth[0m
[32m- trimExceptCRLF standalone[0m
[32mPagedDataSourceSuite:[0m
[32m- basic[0m
[32mSortingSuite:[0m
[32m- sortByKey[0m
[32m- large array[0m
[32m- large array with one split[0m
[32m- large array with many partitions[0m
[32m- sort descending[0m
[32m- sort descending with one split[0m
[32m- sort descending with many partitions[0m
[32m- more partitions than elements[0m
[32m- empty RDD[0m
[32m- partition balancing[0m
[32m- partition balancing for descending sort[0m
[32m- get a range of elements in a sorted RDD that is on one partition[0m
[32m- get a range of elements over multiple partitions in a descendingly sorted RDD[0m
[32m- get a range of elements in an array not partitioned by a range partitioner[0m
[32m- get a range of elements over multiple partitions but not taking up full partitions[0m
[32mRpcAddressSuite:[0m
[32m- hostPort[0m
[32m- fromSparkURL[0m
[32m- fromSparkURL: a typo url[0m
[32m- fromSparkURL: invalid scheme[0m
[32m- toSparkURL[0m
[32mJavaSerializerSuite:[0m
[32m- JavaSerializer instances are serializable[0m
[32m- Deserialize object containing a primitive Class as attribute[0m
[32mLocalDirsSuite:[0m
[32m- Utils.getLocalDir() returns a valid directory, even if some local dirs are missing[0m
[32m- SPARK_LOCAL_DIRS override also affects driver[0m
[32m- Utils.getLocalDir() throws an exception if any temporary directory cannot be retrieved[0m
[32mTaskContextSuite:[0m
[32m- provide metrics sources[0m
[32m- calls TaskCompletionListener after failure[0m
[32m- calls TaskFailureListeners after failure[0m
[32m- all TaskCompletionListeners should be called even if some fail[0m
[32m- all TaskFailureListeners should be called even if some fail[0m
[32m- TaskContext.attemptNumber should return attempt number, not task id (SPARK-4014)[0m
[32m- TaskContext.stageAttemptNumber getter[0m
[32m- accumulators are updated on exception failures[0m
[32m- failed tasks collect only accumulators whose values count during failures[0m
[32m- only updated internal accumulators will be sent back to driver[0m
[32m- localProperties are propagated to executors correctly[0m
[32m- immediately call a completion listener if the context is completed[0m
[32m- immediately call a failure listener if the context has failed[0m
[32m- TaskCompletionListenerException.getMessage should include previousError[0m
[32m- all TaskCompletionListeners should be called even if some fail or a task[0m
[32mHistoryServerSuite:[0m
[32m- application list json[0m
[32m- completed app list json[0m
[32m- running app list json[0m
[32m- minDate app list json[0m
[32m- maxDate app list json[0m
[32m- maxDate2 app list json[0m
[32m- minEndDate app list json[0m
[32m- maxEndDate app list json[0m
[32m- minEndDate and maxEndDate app list json[0m
[32m- minDate and maxEndDate app list json[0m
[32m- limit app list json[0m
[32m- one app json[0m
[32m- one app multi-attempt json[0m
[32m- job list json[0m
[32m- job list from multi-attempt app json(1)[0m
[32m- job list from multi-attempt app json(2)[0m
[32m- one job json[0m
[32m- succeeded job list json[0m
[32m- succeeded&failed job list json[0m
[32m- executor list json[0m
[32m- stage list json[0m
[32m- complete stage list json[0m
[32m- failed stage list json[0m
[32m- one stage json[0m
[32m- one stage attempt json[0m
[32m- stage task summary w shuffle write[0m
[32m- stage task summary w shuffle read[0m
[32m- stage task summary w/ custom quantiles[0m
[32m- stage task list[0m
[32m- stage task list w/ offset & length[0m
[32m- stage task list w/ sortBy[0m
[32m- stage task list w/ sortBy short names: -runtime[0m
[32m- stage task list w/ sortBy short names: runtime[0m
[32m- stage list with accumulable json[0m
[32m- stage with accumulable json[0m
[32m- stage task list from multi-attempt app json(1)[0m
[32m- stage task list from multi-attempt app json(2)[0m
[32m- blacklisting for stage[0m
[32m- blacklisting node for stage[0m
[32m- rdd list storage json[0m
[32m- executor node blacklisting[0m
[32m- executor node blacklisting unblacklisting[0m
[32m- executor memory usage[0m
[32m- app environment[0m
[32m- download all logs for app with multiple attempts[0m
[32m- download one log for app with multiple attempts[0m
[32m- response codes on bad paths[0m
[32m- automatically retrieve uiRoot from request through Knox[0m
[32m- static relative links are prefixed with uiRoot (spark.ui.proxyBase)[0m
[32m- /version api endpoint[0m
[32m- ajax rendered relative links are prefixed with uiRoot (spark.ui.proxyBase)[0m
[32m- security manager starts with spark.authenticate set[0m
[32m- incomplete apps get refreshed[0m
[32m- ui and api authorization checks[0m
[32m- access history application defaults to the last attempt id[0m
[32mNextIteratorSuite:[0m
[32m- one iteration[0m
[32m- two iterations[0m
[32m- empty iteration[0m
[32m- close is called once for empty iterations[0m
[32m- close is called once for non-empty iterations[0m
[32mParallelCollectionSplitSuite:[0m
[32m- one element per slice[0m
[32m- one slice[0m
[32m- equal slices[0m
[32m- non-equal slices[0m
[32m- splitting exclusive range[0m
[32m- splitting inclusive range[0m
[32m- empty data[0m
[32m- zero slices[0m
[32m- negative number of slices[0m
[32m- exclusive ranges sliced into ranges[0m
[32m- inclusive ranges sliced into ranges[0m
[32m- identical slice sizes between Range and NumericRange[0m
[32m- identical slice sizes between List and NumericRange[0m
[32m- large ranges don't overflow[0m
[32m- random array tests[0m
[32m- random exclusive range tests[0m
[32m- random inclusive range tests[0m
[32m- exclusive ranges of longs[0m
[32m- inclusive ranges of longs[0m
[32m- exclusive ranges of doubles[0m
[32m- inclusive ranges of doubles[0m
[32m- inclusive ranges with Int.MaxValue and Int.MinValue[0m
[32m- empty ranges with Int.MaxValue and Int.MinValue[0m
[32mUISeleniumSuite:[0m
[32m- effects of unpersist() / persist() should be reflected[0m
[32m- failed stages should not appear to be active[0m
[32m- spark.ui.killEnabled should properly control kill button display[0m
[32m- jobs page should not display job group name unless some job was submitted in a job group[0m
[32m- job progress bars should handle stage / task failures[0m
[32m- job details page should display useful information for stages that haven't started[0m
[32m- job progress bars / cells reflect skipped stages / tasks[0m
[32m- stages that aren't run appear as 'skipped stages' after a job finishes[0m
[32m- jobs with stages that are skipped should show correct link descriptions on all jobs page[0m
[32m- attaching and detaching a new tab[0m
[32m- kill stage POST/GET response is correct[0m
[32m- kill job POST/GET response is correct[0m
[32m- stage & job retention[0m
[32m- live UI json application list[0m
[32m- job stages should have expected dotfile under DAG visualization[0m
[32m- stages page should show skipped stages[0m
[32m- Staleness of Spark UI should not last minutes or hours[0m
[32mHadoopDelegationTokenManagerSuite:[0m
[32m- Correctly load default credential providers[0m
[32m- disable hive credential provider[0m
[32m- using deprecated configurations[0m
[32m- verify no credentials are obtained[0m
[32m- obtain tokens For HiveMetastore[0m
[32m- Obtain tokens For HBase[0m
[32m- SPARK-23209: obtain tokens when Hive classes are not available[0m
[32mRandomBlockReplicationPolicyBehavior:[0m
[32m- block replication - random block replication policy[0m
[32mExecutorRunnerTest:[0m
[32m- command includes appId[0m
[32mBlockTransferServiceSuite:[0m
[32m- fetchBlockSync should not hang when BlockFetchingListener.onBlockFetchSuccess fails[0m
[32mEventLoggingListenerSuite:[0m
[32m- Verify log file exist[0m
[32m- Basic event logging[0m
[31m*** RUN ABORTED ***[0m
[31m  java.lang.UnsatisfiedLinkError: zstd-jni (Not found in java.library.path)[0m
[31mUnsupported OS/arch, cannot find /linux/s390x/libzstd-jni.so or load zstd-jni from system libraries. Please try building from source the jar or providing libzstd-jni in you system.[0m
[31m  at java.lang.ClassLoader.loadLibraryWithPath(ClassLoader.java:1434)[0m
[31m  at java.lang.ClassLoader.loadLibraryWithClassLoader(ClassLoader.java:1404)[0m
[31m  at java.lang.System.loadLibrary(System.java:540)[0m
[31m  at com.github.luben.zstd.util.Native.load(Native.java:69)[0m
[31m  at com.github.luben.zstd.ZstdOutputStream.<clinit>(ZstdOutputStream.java:17)[0m
[31m  at org.apache.spark.io.ZStdCompressionCodec.compressedOutputStream(CompressionCodec.scala:245)[0m
[31m  at org.apache.spark.scheduler.EventLoggingListener$$anonfun$2.apply(EventLoggingListener.scala:122)[0m
[31m  at org.apache.spark.scheduler.EventLoggingListener$$anonfun$2.apply(EventLoggingListener.scala:122)[0m
[31m  at scala.Option.map(Option.scala:146)[0m
[31m  at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:122)[0m
[31m  ...[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--------------< [0;36morg.apache.spark:spark-mllib-local_2.11[0;1m >---------------[m
[[1;34mINFO[m] [1mBuilding Spark Project ML Local Library 2.4.4                    [10/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/mllib-local/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/mllib-local/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/mllib-local/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:17:56 AM [0.096s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/mllib-local/src/test/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:17:56 AM [0.128s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-mllib-local_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 457 milliseconds.[0m
[36mRun starting. Expected test count is: 85[0m
[32mBLASSuite:[0m
[32m- copy[0m
Oct 21, 2019 4:17:58 AM com.github.fommil.netlib.BLAS <clinit>
WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
Oct 21, 2019 4:17:58 AM com.github.fommil.netlib.BLAS <clinit>
WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
[32m- scal[0m
[32m- axpy[0m
[32m- dot[0m
[32m- spr[0m
[32m- syr[0m
[32m- gemm[0m
[32m- gemv[0m
[32m- spmv[0m
[32mUtilsSuite:[0m
[32m- EPSILON[0m
[32mTestingUtilsSuite:[0m
[32m- Comparing doubles using relative error.[0m
[32m- Comparing doubles using absolute error.[0m
[32m- Comparing vectors using relative error.[0m
[32m- Comparing vectors using absolute error.[0m
[32m- Comparing Matrices using absolute error.[0m
[32m- Comparing Matrices using relative error.[0m
[32mBreezeMatrixConversionSuite:[0m
[32m- dense matrix to breeze[0m
[32m- dense breeze matrix to matrix[0m
[32m- sparse matrix to breeze[0m
[32m- sparse breeze matrix to sparse matrix[0m
[32mBreezeVectorConversionSuite:[0m
[32m- dense to breeze[0m
[32m- sparse to breeze[0m
[32m- dense breeze to vector[0m
[32m- sparse breeze to vector[0m
[32m- sparse breeze with partially-used arrays to vector[0m
Oct 21, 2019 4:17:59 AM com.github.fommil.netlib.LAPACK <clinit>
WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
Oct 21, 2019 4:17:59 AM com.github.fommil.netlib.LAPACK <clinit>
WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
[32mMultivariateGaussianSuite:[0m
[32m- univariate[0m
[32m- multivariate[0m
[32m- multivariate degenerate[0m
[32m- SPARK-11302[0m
[32mMatricesSuite:[0m
[32m- dense matrix construction[0m
[32m- dense matrix construction with wrong dimension[0m
[32m- sparse matrix construction[0m
[32m- sparse matrix construction with wrong number of elements[0m
[32m- index in matrices incorrect input[0m
[32m- equals[0m
[32m- matrix copies are deep copies[0m
[32m- matrix indexing and updating[0m
[32m- dense to dense[0m
[32m- dense to sparse[0m
[32m- sparse to sparse[0m
[32m- sparse to dense[0m
[32m- compressed dense[0m
[32m- compressed sparse[0m
[32m- map, update[0m
[32m- transpose[0m
[32m- foreachActive[0m
[32m- horzcat, vertcat, eye, speye[0m
[32m- zeros[0m
[32m- ones[0m
[32m- eye[0m
[32m- rand[0m
[32m- randn[0m
[32m- diag[0m
[32m- sprand[0m
[32m- sprandn[0m
[32m- toString[0m
[32m- numNonzeros and numActives[0m
[32m- fromBreeze with sparse matrix[0m
[32m- row/col iterator[0m
[32mVectorsSuite:[0m
[32m- dense vector construction with varargs[0m
[32m- dense vector construction from a double array[0m
[32m- sparse vector construction[0m
[32m- sparse vector construction with unordered elements[0m
[32m- sparse vector construction with mismatched indices/values array[0m
[32m- sparse vector construction with too many indices vs size[0m
[32m- sparse vector construction with negative indices[0m
[32m- dense to array[0m
[32m- dense argmax[0m
[32m- sparse to array[0m
[32m- sparse argmax[0m
[32m- vector equals[0m
[32m- vectors equals with explicit 0[0m
[32m- indexing dense vectors[0m
[32m- indexing sparse vectors[0m
[32m- zeros[0m
[32m- Vector.copy[0m
[32m- fromBreeze[0m
[32m- sqdist[0m
[32m- foreachActive[0m
[32m- vector p-norm[0m
[32m- Vector numActive and numNonzeros[0m
[32m- Vector toSparse and toDense[0m
[32m- Vector.compressed[0m
[32m- SparseVector.slice[0m
[32m- sparse vector only support non-negative length[0m
[36mRun completed in 1 second, 784 milliseconds.[0m
[36mTotal number of tests run: 85[0m
[36mSuites: completed 9, aborted 0[0m
[36mTests: succeeded 85, failed 0, canceled 0, ignored 0, pending 0[0m
[32mAll tests passed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------< [0;36morg.apache.spark:spark-graphx_2.11[0;1m >-----------------[m
[[1;34mINFO[m] [1mBuilding Spark Project GraphX 2.4.4                              [11/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/graphx/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/graphx/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/spark/mllib-local/target/scala-2.11/classes:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/graphx/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:18:03 AM [1.792s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/spark/mllib-local/target/scala-2.11/classes:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/core/target/scala-2.11/test-classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:18:09 AM [5.361s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-graphx_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-graphx_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 498 milliseconds.[0m
[36mRun starting. Expected test count is: 110[0m
[32mPregelSuite:[0m
[32m- 1 iteration[0m
[32m- chain propagation[0m
[32mConnectedComponentsSuite:[0m
[32m- Grid Connected Components[0m
[32m- Reverse Grid Connected Components[0m
[32m- Chain Connected Components[0m
[32m- Reverse Chain Connected Components[0m
[32m- Connected Components on a Toy Connected Graph[0m
[32mShortestPathsSuite:[0m
[32m- Shortest Path Computations[0m
[32mPeriodicGraphCheckpointerSuite:[0m
[32m- Persisting[0m
[32m- Checkpointing[0m
[32mPageRankSuite:[0m
[32m- Star PageRank[0m
[32m- Star PersonalPageRank[0m
[32m- Grid PageRank[0m
[32m- Chain PageRank[0m
[32m- Chain PersonalizedPageRank[0m
[32m- Loop with source PageRank[0m
[32m- Loop with sink PageRank[0m
[32mEdgeRDDSuite:[0m
[32m- cache, getStorageLevel[0m
[32m- checkpointing[0m
[32m- count[0m
[32mBytecodeUtilsSuite:[0m
[32m- closure invokes a method[0m
[32m- closure inside a closure invokes a method[0m
[32m- closure inside a closure inside a closure invokes a method[0m
[32m- closure calling a function that invokes a method[0m
[32m- closure calling a function that invokes a method which uses another closure[0m
[32m- nested closure[0m
[32mGraphOpsSuite:[0m
[32m- joinVertices[0m
[32m- collectNeighborIds[0m
[32m- removeSelfEdges[0m
[32m- filter[0m
[32m- convertToCanonicalEdges[0m
[32m- collectEdgesCycleDirectionOut[0m
[32m- collectEdgesCycleDirectionIn[0m
[32m- collectEdgesCycleDirectionEither[0m
[32m- collectEdgesChainDirectionOut[0m
[32m- collectEdgesChainDirectionIn[0m
[32m- collectEdgesChainDirectionEither[0m
[32mVertexPartitionSuite:[0m
[32m- isDefined, filter[0m
[32m- map[0m
[32m- diff[0m
[32m- leftJoin[0m
[32m- innerJoin[0m
[32m- createUsingIndex[0m
[32m- innerJoinKeepLeft[0m
[32m- aggregateUsingIndex[0m
[32m- reindex[0m
[32m- serialization[0m
[32mGraphGeneratorsSuite:[0m
[32m- GraphGenerators.generateRandomEdges[0m
[32m- GraphGenerators.sampleLogNormal[0m
[32m- GraphGenerators.logNormalGraph[0m
[32m- SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound[0m
[32mVertexRDDSuite:[0m
[32m- filter[0m
[32m- mapValues[0m
[32m- minus[0m
[32m- minus with RDD[(VertexId, VD)][0m
[32m- minus with non-equal number of partitions[0m
[32m- diff[0m
[32m- diff with RDD[(VertexId, VD)][0m
[32m- diff vertices with non-equal number of partitions[0m
[32m- leftJoin[0m
[32m- leftJoin vertices with non-equal number of partitions[0m
[32m- innerJoin[0m
[32m- innerJoin vertices with the non-equal number of partitions[0m
[32m- aggregateUsingIndex[0m
[32m- mergeFunc[0m
[32m- cache, getStorageLevel[0m
[32m- checkpoint[0m
[32m- count[0m
[32mGraphLoaderSuite:[0m
[32m- GraphLoader.edgeListFile[0m
[32mStronglyConnectedComponentsSuite:[0m
[32m- Island Strongly Connected Components[0m
[32m- Cycle Strongly Connected Components[0m
[32m- 2 Cycle Strongly Connected Components[0m
[32mTriangleCountSuite:[0m
[32m- Count a single triangle[0m
[32m- Count two triangles[0m
[32m- Count two triangles with bi-directed edges[0m
[32m- Count a single triangle with duplicate edges[0m
[32mEdgePartitionSuite:[0m
[32m- reverse[0m
[32m- map[0m
[32m- filter[0m
[32m- groupEdges[0m
[32m- innerJoin[0m
[32m- isActive, numActives, replaceActives[0m
[32m- tripletIterator[0m
[32m- serialization[0m
[32mEdgeSuite:[0m
[32m- compare[0m
[32mLabelPropagationSuite:[0m
[32m- Label Propagation[0m
[32mSVDPlusPlusSuite:[0m
[32m- Test SVD++ with mean square error on training set[0m
[32m- Test SVD++ with no edges[0m
[32mGraphSuite:[0m
[32m- Graph.fromEdgeTuples[0m
[32m- Graph.fromEdges[0m
[32m- Graph.apply[0m
[32m- triplets[0m
[32m- partitionBy[0m
[32m- mapVertices[0m
[32m- mapVertices changing type with same erased type[0m
[32m- mapEdges[0m
[32m- mapTriplets[0m
[32m- reverse[0m
[32m- reverse with join elimination[0m
[32m- subgraph[0m
[32m- mask[0m
[32m- groupEdges[0m
[32m- aggregateMessages[0m
[32m- outerJoinVertices[0m
[32m- more edge partitions than vertex partitions[0m
[32m- checkpoint[0m
[32m- cache, getStorageLevel[0m
[32m- non-default number of edge partitions[0m
[32m- unpersist graph RDD[0m
[32m- SPARK-14219: pickRandomVertex[0m
[36mRun completed in 2 minutes, 26 seconds.[0m
[36mTotal number of tests run: 110[0m
[36mSuites: completed 20, aborted 0[0m
[36mTests: succeeded 110, failed 0, canceled 0, ignored 0, pending 0[0m
[32mAll tests passed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------< [0;36morg.apache.spark:spark-streaming_2.11[0;1m >----------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Streaming 2.4.4                           [12/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/streaming/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/streaming/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/xerces/xercesImpl/2.11.0/xercesImpl-2.11.0.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:20:49 AM [2.607s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/w3c/css/sac/1.3/sac-1.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-leg-rc/2.52.0/selenium-leg-rc-2.52.0.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpmime/4.5.6/httpmime-4.5.6.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/net/sourceforge/htmlunit/htmlunit-core-js/2.17/htmlunit-core-js-2.17.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-firefox-driver/2.52.0/selenium-firefox-driver-2.52.0.jar:/home/test/.m2/repository/xalan/serializer/2.7.2/serializer-2.7.2.jar:/home/test/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/xalan/xalan/2.7.2/xalan-2.7.2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.2.12.v20150709/websocket-common-9.2.12.v20150709.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/org/apache/commons/commons-exec/1.3/commons-exec-1.3.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/xerces/xercesImpl/2.11.0/xercesImpl-2.11.0.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/core/target/scala-2.11/test-classes:/home/test/.m2/repository/net/sourceforge/htmlunit/htmlunit/2.18/htmlunit-2.18.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/net/java/dev/jna/jna-platform/4.1.0/jna-platform-4.1.0.jar:/home/test/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.2.12.v20150709/websocket-client-9.2.12.v20150709.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-api/2.52.0/selenium-api-2.52.0.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-ie-driver/2.52.0/selenium-ie-driver-2.52.0.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-edge-driver/2.52.0/selenium-edge-driver-2.52.0.jar:/home/test/.m2/repository/cglib/cglib-nodep/2.1_3/cglib-nodep-2.1_3.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/net/sourceforge/cssparser/cssparser/0.9.16/cssparser-0.9.16.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-htmlunit-driver/2.52.0/selenium-htmlunit-driver-2.52.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-chrome-driver/2.52.0/selenium-chrome-driver-2.52.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-support/2.52.0/selenium-support-2.52.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-safari-driver/2.52.0/selenium-safari-driver-2.52.0.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-java/2.52.0/selenium-java-2.52.0.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/webbitserver/webbit/0.4.14/webbit-0.4.14.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/.m2/repository/org/seleniumhq/selenium/selenium-remote-driver/2.52.0/selenium-remote-driver-2.52.0.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/net/sourceforge/nekohtml/nekohtml/1.9.22/nekohtml-1.9.22.jar:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/net/java/dev/jna/jna/4.1.0/jna-4.1.0.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/test/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.2.12.v20150709/websocket-api-9.2.12.v20150709.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:20:56 AM [6.883s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-streaming_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-streaming_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 2 seconds, 999 milliseconds.[0m
[36mRun starting. Expected test count is: 334[0m
[32mJobGeneratorSuite:[0m
[32m- SPARK-6222: Do not clear received block data too soon[0m
[32mReceiverInputDStreamSuite:[0m
[32m- Without WAL enabled: createBlockRDD creates empty BlockRDD when no block info[0m
[32m- Without WAL enabled: createBlockRDD creates correct BlockRDD with block info[0m
[32m- Without WAL enabled: createBlockRDD filters non-existent blocks before creating BlockRDD[0m
[32m- With WAL enabled: createBlockRDD creates empty WALBackedBlockRDD when no block info[0m
[32m- With WAL enabled: createBlockRDD creates correct WALBackedBlockRDD with all block info having WAL info[0m
[32m- With WAL enabled: createBlockRDD creates BlockRDD when some block info don't have WAL info[0m
[32mFileBasedWriteAheadLogWithFileCloseAfterWriteSuite:[0m
[32m- FileBasedWriteAheadLog - read all logs[0m
[32m- FileBasedWriteAheadLog - write logs[0m
[32m- FileBasedWriteAheadLog - read all logs after write[0m
[32m- FileBasedWriteAheadLog - clean old logs[0m
[32m- FileBasedWriteAheadLog - clean old logs synchronously[0m
[32m- FileBasedWriteAheadLog - handling file errors while reading rotating logs[0m
[32m- FileBasedWriteAheadLog - do not create directories or files unless write[0m
[32m- FileBasedWriteAheadLog - parallel recovery not enabled if closeFileAfterWrite = false[0m
[32m- FileBasedWriteAheadLog - close after write flag[0m
[32mRateLimiterSuite:[0m
[32m- rate limiter initializes even without a maxRate set[0m
[32m- rate limiter updates when below maxRate[0m
[32m- rate limiter stays below maxRate despite large updates[0m
[32mReceivedBlockTrackerSuite:[0m
[32m- block addition, and block to batch allocation[0m
[32m- block addition, and block to batch allocation with many blocks[0m
[32m- recovery with write ahead logs should remove only allocated blocks from received queue[0m
[32m- block allocation to batch should not loose blocks from received queue[0m
[32m- recovery and cleanup with write ahead logs[0m
[32m- disable write ahead log when checkpoint directory is not set[0m
[32m- parallel file deletion in FileBasedWriteAheadLog is robust to deletion error[0m
[32mReceivedBlockHandlerWithEncryptionSuite:[0m
[32m- BlockManagerBasedBlockHandler - store blocks[0m
[32m- BlockManagerBasedBlockHandler - handle errors in storing block[0m
[32m- WriteAheadLogBasedBlockHandler - store blocks[0m
[32m- WriteAheadLogBasedBlockHandler - handle errors in storing block[0m
[32m- WriteAheadLogBasedBlockHandler - clean old blocks[0m
[32m- Test Block - count messages[0m
[32m- Test Block - isFullyConsumed[0m
[32mReceiverSchedulingPolicySuite:[0m
[32m- rescheduleReceiver: empty executors[0m
[32m- rescheduleReceiver: receiver preferredLocation[0m
[32m- rescheduleReceiver: return all idle executors if there are any idle executors[0m
[32m- rescheduleReceiver: return all executors that have minimum weight if no idle executors[0m
[32m- scheduleReceivers: schedule receivers evenly when there are more receivers than executors[0m
[32m- scheduleReceivers: schedule receivers evenly when there are more executors than receivers[0m
[32m- scheduleReceivers: schedule receivers evenly when the preferredLocations are even[0m
[32m- scheduleReceivers: return empty if no receiver[0m
[32m- scheduleReceivers: return empty scheduled executors if no executors[0m
[32mMapWithStateSuite:[0m
[32m- state - get, exists, update, remove, [0m
Exception in thread "pool-200-thread-1" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:122)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-2" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:122)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-3" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:122)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-4" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-5" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-6" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-7" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-8" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-9" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-10" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-11" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-12" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-13" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-200-thread-14" scala.MatchError: d18a6bf4-cbb3-47e3-8f18-c897e532c244 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
[32m- mapWithState - basic operations with simple API[0m
[32m- mapWithState - basic operations with advanced API[0m
[32m- mapWithState - type inferencing and class tags[0m
[32m- mapWithState - states as mapped data[0m
Exception in thread "pool-207-thread-1" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:122)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-2" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:122)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-3" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-4" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:356)
	at java.util.TimSort.sort(TimSort.java:220)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-5" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:221)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-6" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:221)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-7" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:221)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-8" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:221)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-9" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:221)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-10" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:221)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-11" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:221)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-12" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:221)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-13" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:221)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Exception in thread "pool-207-thread-14" scala.MatchError: f1917537-402c-4861-8128-65f666642369 (of class java.lang.String)
	at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:133)
	at scala.math.Ordering$$anon$9.compare(Ordering.scala:200)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:221)
	at java.util.Arrays.sort(Arrays.java:1438)
	at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)
	at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186)
	at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601)
	at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186)
	at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:133)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:263)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
[32m- mapWithState - initial states, with nothing returned as from mapping function[0m
[32m- mapWithState - state removing[0m
[32m- mapWithState - state timing out[0m
[32m- mapWithState - checkpoint durations[0m
-------------------------------------------
Time: 1000 ms
-------------------------------------------

-------------------------------------------
Time: 2000 ms
-------------------------------------------
(a,1)

-------------------------------------------
Time: 3000 ms
-------------------------------------------
(a,2)
(b,1)

-------------------------------------------
Time: 3000 ms
-------------------------------------------
(a,2)
(b,1)

-------------------------------------------
Time: 4000 ms
-------------------------------------------
(a,3)
(b,2)
(c,1)

-------------------------------------------
Time: 5000 ms
-------------------------------------------
(c,1)
(a,4)
(b,3)

-------------------------------------------
Time: 6000 ms
-------------------------------------------
(b,3)
(c,1)
(a,5)

-------------------------------------------
Time: 7000 ms
-------------------------------------------
(a,5)
(b,3)
(c,1)

[32m- mapWithState - driver failure recovery[0m
[32mJavaStreamingListenerWrapperSuite:[0m
[32m- basic[0m
[32mDurationSuite:[0m
[32m- less[0m
[32m- lessEq[0m
[32m- greater[0m
[32m- greaterEq[0m
[32m- plus[0m
[32m- minus[0m
[32m- times[0m
[32m- div[0m
[32m- isMultipleOf[0m
[32m- min[0m
[32m- max[0m
[32m- isZero[0m
[32m- Milliseconds[0m
[32m- Seconds[0m
[32m- Minutes[0m
[32mPIDRateEstimatorSuite:[0m
[32m- the right estimator is created[0m
[32m- estimator checks ranges[0m
[32m- first estimate is None[0m
[32m- second estimate is not None[0m
[32m- no estimate when no time difference between successive calls[0m
[32m- no estimate when no records in previous batch[0m
[32m- no estimate when there is no processing delay[0m
[32m- estimate is never less than min rate[0m
[32m- with no accumulated or positive error, |I| > 0, follow the processing speed[0m
[32m- with no accumulated but some positive error, |I| > 0, follow the processing speed[0m
[32m- with some accumulated and some positive error, |I| > 0, stay below the processing speed[0m
[32mWindowOperationsSuite:[0m
[32m- window - basic window[0m
[32m- window - tumbling window[0m
[32m- window - larger window[0m
[32m- window - non-overlapping window[0m
[32m- window - persistence level[0m
[32m- reduceByKeyAndWindow - basic reduction[0m
[32m- reduceByKeyAndWindow - key already in window and new value added into window[0m
[32m- reduceByKeyAndWindow - new key added into window[0m
[32m- reduceByKeyAndWindow - key removed from window[0m
[32m- reduceByKeyAndWindow - larger slide time[0m
[32m- reduceByKeyAndWindow - big test[0m
[32m- reduceByKeyAndWindow with inverse function - basic reduction[0m
[32m- reduceByKeyAndWindow with inverse function - key already in window and new value added into window[0m
[32m- reduceByKeyAndWindow with inverse function - new key added into window[0m
[32m- reduceByKeyAndWindow with inverse function - key removed from window[0m
[32m- reduceByKeyAndWindow with inverse function - larger slide time[0m
[32m- reduceByKeyAndWindow with inverse function - big test[0m
[32m- reduceByKeyAndWindow with inverse and filter functions - big test[0m
[32m- groupByKeyAndWindow[0m
[32m- countByWindow[0m
[32m- countByValueAndWindow[0m
[32mTimeSuite:[0m
[32m- less[0m
[32m- lessEq[0m
[32m- greater[0m
[32m- greaterEq[0m
[32m- plus[0m
[32m- minus Time[0m
[32m- minus Duration[0m
[32m- floor[0m
[32m- isMultipleOf[0m
[32m- min[0m
[32m- max[0m
[32m- until[0m
[32m- to[0m
[32mDStreamScopeSuite:[0m
[32m- dstream without scope[0m
[32m- input dstream without scope[0m
[32m- scoping simple operations[0m
[32m- scoping nested operations[0m
[32m- transform should allow RDD operations to be captured in scopes[0m
[32m- foreachRDD should allow RDD operations to be captured in scope[0m
[32mStreamingContextSuite:[0m
[32m- from no conf constructor[0m
[32m- from no conf + spark home[0m
[32m- from no conf + spark home + env[0m
[32m- from conf with settings[0m
[32m- from existing SparkContext[0m
[32m- from existing SparkContext with settings[0m
[32m- from checkpoint[0m
[32m- checkPoint from conf[0m
[32m- state matching[0m
[32m- start and stop state check[0m
[32m- start with non-serializable DStream checkpoints[0m
[32m- start failure should stop internal components[0m
[32m- start should set local properties of streaming jobs correctly[0m
[32m- start multiple times[0m
[32m- stop multiple times[0m
[32m- stop before start[0m
[32m- start after stop[0m
[32m- stop only streaming context[0m
[32m- stop(stopSparkContext=true) after stop(stopSparkContext=false)[0m
[32m- stop gracefully[0m
[32m- stop gracefully even if a receiver misses StopReceiver[0m
[32m- stop slow receiver gracefully[0m
[32m- registering and de-registering of streamingSource[0m
[32m- awaitTermination[0m
[32m- awaitTermination after stop[0m
[32m- awaitTermination with error in task[0m
[32m- awaitTermination with error in job generation[0m
[32m- awaitTerminationOrTimeout[0m
[32m- getOrCreate[0m
[32m- getActive and getActiveOrCreate[0m
[32m- getActiveOrCreate with checkpoint[0m
[32m- multiple streaming contexts[0m
[32m- DStream and generated RDD creation sites[0m
[32m- throw exception on using active or stopped context[0m
[32m- queueStream doesn't support checkpointing[0m
[32m- Creating an InputDStream but not using it should not crash[0m
Exception in thread "streaming-job-executor-0" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:206)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:222)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:157)
	at org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:243)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:728)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.streaming.StreamingContextSuite$$anonfun$55$$anonfun$apply$21.apply(StreamingContextSuite.scala:824)
	at org.apache.spark.streaming.StreamingContextSuite$$anonfun$55$$anonfun$apply$21.apply(StreamingContextSuite.scala:822)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
[32m- SPARK-18560 Receiver data should be deserialized properly.[0m
[32mDStreamClosureSuite:[0m
[32m- user provided closures are actually cleaned[0m
[32mBatchedWriteAheadLogSuite:[0m
[32m- BatchedWriteAheadLog - read all logs[0m
[32m- BatchedWriteAheadLog - write logs[0m
[32m- BatchedWriteAheadLog - read all logs after write[0m
[32m- BatchedWriteAheadLog - clean old logs[0m
[32m- BatchedWriteAheadLog - clean old logs synchronously[0m
[32m- BatchedWriteAheadLog - handling file errors while reading rotating logs[0m
[32m- BatchedWriteAheadLog - do not create directories or files unless write[0m
[32m- BatchedWriteAheadLog - parallel recovery not enabled if closeFileAfterWrite = false[0m
[32m- BatchedWriteAheadLog - serializing and deserializing batched records[0m
[32m- BatchedWriteAheadLog - failures in wrappedLog get bubbled up[0m
[32m- BatchedWriteAheadLog - name log with the highest timestamp of aggregated entries[0m
[32m- BatchedWriteAheadLog - shutdown properly[0m
[32m- BatchedWriteAheadLog - fail everything in queue during shutdown[0m
[32mReceiverTrackerSuite:[0m
[32m- send rate update to receivers[0m
[32m- should restart receiver after stopping it[0m
[32m- SPARK-11063: TaskSetManager should use Receiver RDD's preferredLocations[0m
[32m- get allocated executors[0m
[32mStateMapSuite:[0m
[32m- EmptyStateMap[0m
[32m- OpenHashMapBasedStateMap - put, get, getByTime, getAll, remove[0m
[32m- OpenHashMapBasedStateMap - put, get, getByTime, getAll, remove with copy[0m
[32m- OpenHashMapBasedStateMap - serializing and deserializing[0m
[32m- OpenHashMapBasedStateMap - serializing and deserializing with compaction[0m
[32m- OpenHashMapBasedStateMap - all possible sequences of operations with copies [0m
[32m- OpenHashMapBasedStateMap - serializing and deserializing with KryoSerializable states[0m
[32m- EmptyStateMap - serializing and deserializing[0m
[32m- MapWithStateRDDRecord - serializing and deserializing with KryoSerializable states[0m
[32mRecurringTimerSuite:[0m
[32m- basic[0m
[32m- SPARK-10224: call 'callback' after stopping[0m
[32mCheckpointSuite:[0m
[32m- non-existent checkpoint dir[0m
[32m- basic rdd checkpoints + dstream graph checkpoint recovery[0m
[32m- recovery of conf through checkpoints[0m
[32m- get correct spark.driver.[host|port] from checkpoint[0m
-------------------------------------------
Time: 500 ms
-------------------------------------------
(a,2)
(b,1)

-------------------------------------------
Time: 1000 ms
-------------------------------------------
(,2)

-------------------------------------------
Time: 1500 ms
-------------------------------------------

-------------------------------------------
Time: 1500 ms
-------------------------------------------

-------------------------------------------
Time: 2000 ms
-------------------------------------------
(a,2)
(b,1)

-------------------------------------------
Time: 2500 ms
-------------------------------------------
(,2)

-------------------------------------------
Time: 3000 ms
-------------------------------------------

[32m- recovery with map and reduceByKey operations[0m
-------------------------------------------
Time: 500 ms
-------------------------------------------
(a,1)

-------------------------------------------
Time: 1000 ms
-------------------------------------------
(a,2)

-------------------------------------------
Time: 1500 ms
-------------------------------------------
(a,3)

-------------------------------------------
Time: 2000 ms
-------------------------------------------
(a,4)

-------------------------------------------
Time: 2500 ms
-------------------------------------------
(a,4)

-------------------------------------------
Time: 3000 ms
-------------------------------------------
(a,4)

-------------------------------------------
Time: 3500 ms
-------------------------------------------
(a,4)

-------------------------------------------
Time: 3500 ms
-------------------------------------------
(a,4)

-------------------------------------------
Time: 4000 ms
-------------------------------------------
(a,4)

-------------------------------------------
Time: 4500 ms
-------------------------------------------
(a,4)

-------------------------------------------
Time: 5000 ms
-------------------------------------------
(a,4)

[32m- recovery with invertible reduceByKeyAndWindow operation[0m
-------------------------------------------
Time: 500 ms
-------------------------------------------
(a,2)
(b,1)

-------------------------------------------
Time: 1000 ms
-------------------------------------------
(,2)

-------------------------------------------
Time: 1500 ms
-------------------------------------------

-------------------------------------------
Time: 1500 ms
-------------------------------------------

-------------------------------------------
Time: 2000 ms
-------------------------------------------
(a,2)
(b,1)

-------------------------------------------
Time: 2500 ms
-------------------------------------------
(,2)

-------------------------------------------
Time: 3000 ms
-------------------------------------------

[32m- recovery with saveAsHadoopFiles operation[0m
-------------------------------------------
Time: 500 ms
-------------------------------------------
(a,2)
(b,1)

-------------------------------------------
Time: 1000 ms
-------------------------------------------
(,2)

-------------------------------------------
Time: 1500 ms
-------------------------------------------

-------------------------------------------
Time: 1500 ms
-------------------------------------------

-------------------------------------------
Time: 2000 ms
-------------------------------------------
(a,2)
(b,1)

-------------------------------------------
Time: 2500 ms
-------------------------------------------
(,2)

-------------------------------------------
Time: 3000 ms
-------------------------------------------

[32m- recovery with saveAsNewAPIHadoopFiles operation[0m
-------------------------------------------
Time: 500 ms
-------------------------------------------
(b,1)
(a,2)

-------------------------------------------
Time: 1000 ms
-------------------------------------------
(,2)

-------------------------------------------
Time: 1500 ms
-------------------------------------------

-------------------------------------------
Time: 1500 ms
-------------------------------------------

-------------------------------------------
Time: 2000 ms
-------------------------------------------
(b,1)
(a,2)

-------------------------------------------
Time: 2500 ms
-------------------------------------------
(,2)

-------------------------------------------
Time: 3000 ms
-------------------------------------------

[32m- recovery with saveAsHadoopFile inside transform operation[0m
-------------------------------------------
Time: 500 ms
-------------------------------------------
(a,1)

-------------------------------------------
Time: 1000 ms
-------------------------------------------
(a,2)

-------------------------------------------
Time: 1500 ms
-------------------------------------------
(a,3)

-------------------------------------------
Time: 2000 ms
-------------------------------------------
(a,4)

-------------------------------------------
Time: 2500 ms
-------------------------------------------
(a,5)

-------------------------------------------
Time: 3000 ms
-------------------------------------------
(a,6)

-------------------------------------------
Time: 3500 ms
-------------------------------------------
(a,7)

-------------------------------------------
Time: 3500 ms
-------------------------------------------
(a,7)

-------------------------------------------
Time: 4000 ms
-------------------------------------------
(a,8)

-------------------------------------------
Time: 4500 ms
-------------------------------------------
(a,9)

-------------------------------------------
Time: 5000 ms
-------------------------------------------
(a,10)

[32m- recovery with updateStateByKey operation[0m
[32m- recovery maintains rate controller[0m
Exception in thread "streaming-job-executor-0" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:206)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:222)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:157)
	at org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:243)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:728)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.streaming.TestOutputStream$$anonfun$$lessinit$greater$1.apply(TestSuiteBase.scala:100)
	at org.apache.spark.streaming.TestOutputStream$$anonfun$$lessinit$greater$1.apply(TestSuiteBase.scala:99)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
[32m- recovery with file input stream[0m
[32m- DStreamCheckpointData.restore invoking times[0m
[32m- recovery from checkpoint contains array object[0m
[32m- SPARK-11267: the race condition of two checkpoints in a batch[0m
[32m- SPARK-6847: stack overflow when updateStateByKey is followed by a checkpointed dstream[0m
[32mFailureSuite:[0m
[32m- multiple failures with map[0m
[32m- multiple failures with updateStateByKey[0m
[32mWriteAheadLogUtilsSuite:[0m
[32m- log selection and creation[0m
[32m- wrap WriteAheadLog in BatchedWriteAheadLog when batching is enabled[0m
[32m- batching is enabled by default in WriteAheadLog[0m
[32m- closeFileAfterWrite is disabled by default in WriteAheadLog[0m
[32mBlockGeneratorSuite:[0m
[32m- block generation and data callbacks[0m
[32m- stop ensures correct shutdown[0m
[32m- block push errors are reported[0m
[32mUISeleniumSuite:[0m
[32m- attaching and detaching a Streaming tab[0m
[32mExecutorAllocationManagerSuite:[0m
[32m- basic functionality[0m
[32m- requestExecutors policy[0m
[32m- killExecutor policy[0m
[32m- parameter validation[0m
[32m- enabling and disabling[0m
[32mStreamingJobProgressListenerSuite:[0m
[32m- onBatchSubmitted, onBatchStarted, onBatchCompleted, onReceiverStarted, onReceiverError, onReceiverStopped[0m
[32m- Remove the old completed batches when exceeding the limit[0m
[32m- out-of-order onJobStart and onBatchXXX[0m
[32m- detect memory leak[0m
[32mReceiverSuite:[0m
[32m- receiver life cycle[0m
[33m- block generator throttling !!! IGNORED !!![0m
[32m- write ahead log - generating and cleaning[0m
[32mStreamingListenerSuite:[0m
[32m- batch info reporting[0m
[32m- receiver info reporting[0m
[32m- output operation reporting[0m
[32m- don't call ssc.stop in listener[0m
[32m- onBatchCompleted with successful batch[0m
[32m- onBatchCompleted with failed batch and one failed job[0m
[32m- onBatchCompleted with failed batch and multiple failed jobs[0m
[32m- StreamingListener receives no events after stopping StreamingListenerBus[0m
[32mMapWithStateRDDSuite:[0m
[32m- creation from pair RDD[0m
[32m- updating state and generating mapped data in MapWithStateRDDRecord[0m
[32m- states generated by MapWithStateRDD[0m
[32m- checkpointing[0m
[32m- checkpointing empty state RDD[0m
[32mBasicOperationsSuite:[0m
[32m- map[0m
[32m- flatMap[0m
[32m- filter[0m
[32m- glom[0m
[32m- mapPartitions[0m
[32m- repartition (more partitions)[0m
[32m- repartition (fewer partitions)[0m
[32m- groupByKey[0m
[32m- reduceByKey[0m
[32m- reduce[0m
[32m- count[0m
[32m- countByValue[0m
[32m- mapValues[0m
[32m- flatMapValues[0m
[32m- union[0m
[32m- union with input stream return None[0m
[32m- StreamingContext.union[0m
[32m- transform[0m
[32m- transform with NULL[0m
[32m- transform with input stream return None[0m
[32m- transformWith[0m
[32m- transformWith with input stream return None[0m
[32m- StreamingContext.transform[0m
[32m- StreamingContext.transform with input stream return None[0m
[32m- cogroup[0m
[32m- join[0m
[32m- leftOuterJoin[0m
[32m- rightOuterJoin[0m
[32m- fullOuterJoin[0m
[32m- updateStateByKey[0m
[32m- updateStateByKey - simple with initial value RDD[0m
[32m- updateStateByKey - testing time stamps as input[0m
[32m- updateStateByKey - with initial value RDD[0m
[32m- updateStateByKey - object lifecycle[0m
[32m- slice[0m
[32m- slice - has not been initialized[0m
[32m- rdd cleanup - map and window[0m
[32m- rdd cleanup - updateStateByKey[0m
Exception in thread "receiver-supervisor-future-0" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.InterruptedException
	at java.lang.Thread.sleep(Native Method)
	at java.lang.Thread.sleep(Thread.java:943)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply$mcV$sp(ReceiverSupervisor.scala:196)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply(ReceiverSupervisor.scala:189)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply(ReceiverSupervisor.scala:189)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
[32m- rdd cleanup - input blocks and persisted RDDs[0m
[32mInputStreamsSuite:[0m
Exception in thread "receiver-supervisor-future-0" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.InterruptedException
	at java.lang.Thread.sleep(Native Method)
	at java.lang.Thread.sleep(Thread.java:943)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply$mcV$sp(ReceiverSupervisor.scala:196)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply(ReceiverSupervisor.scala:189)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply(ReceiverSupervisor.scala:189)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
[32m- socket input stream[0m
[32m- socket input stream - no block in a batch[0m
[32m- binary records stream[0m
[32m- file input stream - newFilesOnly = true[0m
[32m- file input stream - newFilesOnly = false[0m
[32m- file input stream - wildcard[0m
[32m- multi-thread receiver[0m
[32m- queue input stream - oneAtATime = true[0m
[32m- queue input stream - oneAtATime = false[0m
[32m- test track the number of input stream[0m
[32mUIUtilsSuite:[0m
[32m- shortTimeUnitString[0m
[32m- normalizeDuration[0m
[32m- convertToTimeUnit[0m
[32m- formatBatchTime[0m
[32mRateLimitedOutputStreamSuite:[0m
[32m- write[0m
[32mFileBasedWriteAheadLogSuite:[0m
[32m- FileBasedWriteAheadLog - read all logs[0m
[32m- FileBasedWriteAheadLog - write logs[0m
[32m- FileBasedWriteAheadLog - read all logs after write[0m
[32m- FileBasedWriteAheadLog - clean old logs[0m
[32m- FileBasedWriteAheadLog - clean old logs synchronously[0m
[32m- FileBasedWriteAheadLog - handling file errors while reading rotating logs[0m
[32m- FileBasedWriteAheadLog - do not create directories or files unless write[0m
[32m- FileBasedWriteAheadLog - parallel recovery not enabled if closeFileAfterWrite = false[0m
[32m- FileBasedWriteAheadLog - seqToParIterator[0m
[32m- FileBasedWriteAheadLogWriter - writing data[0m
[32m- FileBasedWriteAheadLogWriter - syncing of data by writing and reading immediately[0m
[32m- FileBasedWriteAheadLogReader - sequentially reading data[0m
[32m- FileBasedWriteAheadLogReader - sequentially reading data written with writer[0m
[32m- FileBasedWriteAheadLogReader - reading data written with writer after corrupted write[0m
[32m- FileBasedWriteAheadLogReader - handles errors when file doesn't exist[0m
[32m- FileBasedWriteAheadLogRandomReader - reading data using random reader[0m
[32m- FileBasedWriteAheadLogRandomReader- reading data using random reader written with writer[0m
[32mRateControllerSuite:[0m
[32m- RateController - rate controller publishes updates after batches complete[0m
[32m- ReceiverRateController - published rates reach receivers[0m
[32mInputInfoTrackerSuite:[0m
[32m- test report and get InputInfo from InputInfoTracker[0m
[32m- test cleanup InputInfo from InputInfoTracker[0m
[32mWriteAheadLogBackedBlockRDDSuite:[0m
[32m- Read data available in both block manager and write ahead log[0m
[32m- Read data available only in block manager, not in write ahead log[0m
[32m- Read data available only in write ahead log, not in block manager[0m
[32m- Read data with partially available in block manager, and rest in write ahead log[0m
[32m- Test isBlockValid skips block fetching from BlockManager[0m
[32m- Test whether RDD is valid after removing blocks from block manager[0m
[32m- Test storing of blocks recovered from write ahead log back into block manager[0m
Exception in thread "block-manager-slave-async-thread-pool-0" java.lang.Error: java.lang.InterruptedException
Exception in thread "block-manager-slave-async-thread-pool-2" 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)java.lang.Error: java.lang.InterruptedException
Exception in thread "block-manager-slave-async-thread-pool-1" 
java.lang.Error: java.lang.InterruptedException	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)

	at java.lang.Thread.run(Thread.java:813)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)

	at java.lang.Thread.run(Thread.java:813)Caused by: 
java.lang.InterruptedExceptionCaused by: 
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)java.lang.InterruptedException

	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)	at java.lang.Object.wait(Native Method)

	at java.lang.Object.wait(Native Method)	at java.lang.Object.wait(Object.java:189)	at java.lang.Object.wait(Object.java:189)

	at org.apache.spark.storage.BlockInfoManager.lockForWriting(BlockInfoManager.scala:236)	at org.apache.spark.storage.BlockInfoManager.lockForWriting(BlockInfoManager.scala:236)

	at org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1571)	at org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1571)


	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1.apply$mcZ$sp(BlockManagerSlaveEndpoint.scala:47)	at java.lang.Object.wait(Object.java:189)

	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1.apply$mcZ$sp(BlockManagerSlaveEndpoint.scala:47)	at org.apache.spark.storage.BlockInfoManager.lockForWriting(BlockInfoManager.scala:236)	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1.apply(BlockManagerSlaveEndpoint.scala:46)

	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1.apply(BlockManagerSlaveEndpoint.scala:46)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1.apply(BlockManagerSlaveEndpoint.scala:46)	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$1.apply(BlockManagerSlaveEndpoint.scala:86)


	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1.apply(BlockManagerSlaveEndpoint.scala:46)	at org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1571)

	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$1.apply(BlockManagerSlaveEndpoint.scala:86)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1.apply$mcZ$sp(BlockManagerSlaveEndpoint.scala:47)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1.apply(BlockManagerSlaveEndpoint.scala:46)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1.apply(BlockManagerSlaveEndpoint.scala:46)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$1.apply(BlockManagerSlaveEndpoint.scala:86)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@362b9ee7 rejected from java.util.concurrent.ThreadPoolExecutor@2acc209b[Shutting down, pool size = 2, active threads = 2, queued tasks = 0, completed tasks = 3]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:157)
	at scala.concurrent.Promise$class.failure(Promise.scala:104)
	at scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:157)
	at scala.concurrent.Future$$anonfun$failed$1.apply(Future.scala:194)
	at scala.concurrent.Future$$anonfun$failed$1.apply(Future.scala:192)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:157)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@8ad062d3 rejected from java.util.concurrent.ThreadPoolExecutor@2acc209b[Shutting down, pool size = 2, active threads = 2, queued tasks = 0, completed tasks = 3]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:157)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@7e5ba562 rejected from java.util.concurrent.ThreadPoolExecutor@2acc209b[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 4]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:157)
	at scala.concurrent.Promise$class.failure(Promise.scala:104)
	at scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:157)
	at scala.concurrent.Future$$anonfun$failed$1.apply(Future.scala:194)
	at scala.concurrent.Future$$anonfun$failed$1.apply(Future.scala:192)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:157)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@44f21a9f rejected from java.util.concurrent.ThreadPoolExecutor@2acc209b[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 4]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:157)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
[32m- read data in block manager and WAL with encryption on[0m
[32mBatchedWriteAheadLogWithCloseFileAfterWriteSuite:[0m
[32m- BatchedWriteAheadLog - read all logs[0m
[32m- BatchedWriteAheadLog - write logs[0m
[32m- BatchedWriteAheadLog - read all logs after write[0m
[32m- BatchedWriteAheadLog - clean old logs[0m
[32m- BatchedWriteAheadLog - clean old logs synchronously[0m
[32m- BatchedWriteAheadLog - handling file errors while reading rotating logs[0m
[32m- BatchedWriteAheadLog - do not create directories or files unless write[0m
[32m- BatchedWriteAheadLog - parallel recovery not enabled if closeFileAfterWrite = false[0m
[32m- BatchedWriteAheadLog - close after write flag[0m
[32mReceivedBlockHandlerSuite:[0m
[32m- BlockManagerBasedBlockHandler - store blocks[0m
[32m- BlockManagerBasedBlockHandler - handle errors in storing block[0m
[32m- WriteAheadLogBasedBlockHandler - store blocks[0m
[32m- WriteAheadLogBasedBlockHandler - handle errors in storing block[0m
[32m- WriteAheadLogBasedBlockHandler - clean old blocks[0m
[32m- Test Block - count messages[0m
[32m- Test Block - isFullyConsumed[0m
[36mRun completed in 5 minutes, 27 seconds.[0m
[36mTotal number of tests run: 334[0m
[36mSuites: completed 41, aborted 0[0m
[36mTests: succeeded 334, failed 0, canceled 0, ignored 1, pending 0[0m
[32mAll tests passed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------< [0;36morg.apache.spark:spark-catalyst_2.11[0;1m >----------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Catalyst 2.4.4                            [13/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/sql/catalyst/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/sql/catalyst/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mantlr4-maven-plugin:4.7:antlr4[m [1m(default)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] No grammars to process
[[1;34mINFO[m] ANTLR 4: Processing source directory /home/test/spark/sql/catalyst/src/main/antlr4
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/sql/catalyst/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:26:32 AM [4.133s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/core/target/scala-2.11/test-classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:26:55 AM [22.990s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.0.2:test-jar[m [1m(prepare-test-jar)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/test/spark/sql/catalyst/target/spark-catalyst_2.11-2.4.4-tests.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-catalyst_2.11[0;1m ---[m
[36mDiscovery starting.[0m
line 1:0 mismatched input '<EOF>' expecting {'(', 'SELECT', 'FROM', 'ADD', 'DESC', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'INSERT', 'DELETE', 'DESCRIBE', 'EXPLAIN', 'SHOW', 'USE', 'DROP', 'ALTER', 'MAP', 'SET', 'RESET', 'START', 'COMMIT', 'ROLLBACK', 'REDUCE', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'DFS', 'TRUNCATE', 'ANALYZE', 'LIST', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'EXPORT', 'IMPORT', 'LOAD'}
[36mDiscovery completed in 20 seconds, 297 milliseconds.[0m
[36mRun starting. Expected test count is: 3052[0m
[32mAnalysisHelperSuite:[0m
[32m- setAnalyze is recursive[0m
[32m- resolveOperator runs on operators recursively[0m
[32m- resolveOperatorsDown runs on operators recursively[0m
[32m- resolveExpressions runs on operators recursively[0m
[32m- resolveOperator skips all ready resolved plans[0m
[32m- resolveOperatorsDown skips all ready resolved plans[0m
[32m- resolveExpressions skips all ready resolved plans[0m
[32m- resolveOperator skips partially resolved plans[0m
[32m- resolveOperatorsDown skips partially resolved plans[0m
[32m- resolveExpressions skips partially resolved plans[0m
[32m- do not allow transform in analyzer[0m
[32m- allow transform in resolveOperators in the analyzer[0m
[32m- allow transform with allowInvokingTransformsInAnalyzer in the analyzer[0m
[32mAggregateEstimationSuite:[0m
[32m- set an upper bound if the product of ndv's of group-by columns is too large[0m
[32m- data contains all combinations of distinct values of group-by columns.[0m
[32m- empty group-by column[0m
[32m- aggregate on empty table - with or without group-by column[0m
[32m- group-by column with only null value[0m
[32m- group-by column with null value[0m
[32m- non-cbo estimation[0m
[32mLiteralExpressionSuite:[0m
[32m- null[0m
[32m- default[0m
[32m- boolean literals[0m
[32m- int literals[0m
[32m- double literals[0m
[32m- string literals[0m
[32m- sum two literals[0m
[32m- binary literals[0m
[32m- decimal[0m
[32m- array[0m
[32m- seq[0m
[32m- map[0m
[32m- struct[0m
[32m- unsupported types (map and struct) in Literal.apply[0m
[32m- SPARK-24571: char literals[0m
[32mMiscExpressionsSuite:[0m
[32m- assert_true[0m
[32m- uuid[0m
[32m- PrintToStderr[0m
[32mReplaceOperatorSuite:[0m
[32m- replace Intersect with Left-semi Join[0m
[32m- replace Except with Filter while both the nodes are of type Filter[0m
[32m- replace Except with Filter while only right node is of type Filter[0m
[32m- replace Except with Filter while both the nodes are of type Project[0m
[32m- replace Except with Filter while only right node is of type Project[0m
[32m- replace Except with Filter while left node is Project and right node is Filter[0m
[32m- replace Except with Left-anti Join[0m
[32m- replace Except with Filter when only right filter can be applied to the left[0m
[32m- replace Distinct with Aggregate[0m
[32m- replace batch Deduplicate with Aggregate[0m
[32m- add one grouping key if necessary when replace Deduplicate with Aggregate[0m
[32m- don't replace streaming Deduplicate[0m
[32m- SPARK-26366: ReplaceExceptWithFilter should handle properly NULL[0m
[32m- SPARK-26366: ReplaceExceptWithFilter should not transform non-detrministic[0m
[32mSameResultSuite:[0m
[32m- relations[0m
[32m- projections[0m
[32m- filters[0m
[32m- sorts[0m
[32m- union[0m
[32m- hint[0m
[32mUnsupportedOperationsSuite:[0m
[32m- batch plan - local relation: supported[0m
[32m- batch plan - streaming source: not supported[0m
[32m- batch plan - select on streaming source: not supported[0m
[32m- streaming plan - no streaming source[0m
[32m- streaming plan - commmands: not supported[0m
[32m- streaming plan - aggregate - multiple batch aggregations: supported[0m
[32m- streaming plan - aggregate - multiple aggregations but only one streaming aggregation: supported[0m
[32m- streaming plan - aggregate - multiple streaming aggregations: not supported[0m
[32m- streaming plan - aggregate - streaming aggregations in update mode: supported[0m
[32m- streaming plan - aggregate - streaming aggregations in complete mode: supported[0m
[32m- streaming plan - aggregate - streaming aggregations with watermark in append mode: supported[0m
[32m- streaming plan - aggregate - streaming aggregations without watermark in append mode: not supported[0m
[32m- streaming plan - distinct aggregate - aggregate on batch relation: supported[0m
[32m- streaming plan - distinct aggregate - aggregate on streaming relation: not supported[0m
[32m- batch plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on batch relation: supported[0m
[32m- batch plan - flatMapGroupsWithState - multiple flatMapGroupsWithState(Append)s on batch relation: supported[0m
[32m- batch plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on batch relation: supported[0m
[32m- batch plan - flatMapGroupsWithState - multiple flatMapGroupsWithState(Update)s on batch relation: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation without aggregation in update mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation without aggregation in append mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation without aggregation in complete mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation with aggregation in Append mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation with aggregation in Update mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation with aggregation in Complete mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation without aggregation in append mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation without aggregation in update mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation before aggregation in Append mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation before aggregation in Update mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation before aggregation in Complete mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation after aggregation in Append mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation after aggregation in Update mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation in complete mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on batch relation inside streaming relation in Append output mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on batch relation inside streaming relation in Update output mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on batch relation inside streaming relation in Append output mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on batch relation inside streaming relation in Update output mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - multiple flatMapGroupsWithStates on streaming relation and all are in append mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState -  multiple flatMapGroupsWithStates on s streaming relation but some are not in append mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState on streaming relation without aggregation in append mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState on streaming relation without aggregation in complete mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState on streaming relation with aggregation in Append mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState on streaming relation with aggregation in Update mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState on streaming relation with aggregation in Complete mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - multiple mapGroupsWithStates on streaming relation and all are in append mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mixing mapGroupsWithStates and flatMapGroupsWithStates on streaming relation: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState with event time timeout without watermark: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState with event time timeout with watermark: supported[0m
[32m- streaming plan - Deduplicate - Deduplicate on streaming relation before aggregation: supported[0m
[32m- streaming plan - Deduplicate - Deduplicate on streaming relation after aggregation: not supported[0m
[32m- streaming plan - Deduplicate - Deduplicate on batch relation inside a streaming query: supported[0m
[32m- streaming plan - single inner join in append mode with stream-stream relations: supported[0m
[32m- streaming plan - single inner join in append mode with stream-batch relations: supported[0m
[32m- streaming plan - single inner join in append mode with batch-stream relations: supported[0m
[32m- streaming plan - single inner join in append mode with batch-batch relations: supported[0m
[32m- streaming plan - multiple inner joins in append mode with stream-stream relations: supported[0m
[32m- streaming plan - multiple inner joins in append mode with stream-batch relations: supported[0m
[32m- streaming plan - multiple inner joins in append mode with batch-stream relations: supported[0m
[32m- streaming plan - multiple inner joins in append mode with batch-batch relations: supported[0m
[32m- streaming plan - inner join in update mode with stream-stream relations: not supported[0m
[32m- streaming plan - inner join in update mode with stream-batch relations: supported[0m
[32m- streaming plan - inner join in update mode with batch-stream relations: supported[0m
[32m- streaming plan - inner join in update mode with batch-batch relations: supported[0m
[32m- streaming plan - full outer join with stream-stream relations: not supported[0m
[32m- streaming plan - full outer join with stream-batch relations: not supported[0m
[32m- streaming plan - full outer join with batch-stream relations: not supported[0m
[32m- streaming plan - full outer join with batch-batch relations: supported[0m
[32m- streaming plan - left outer join with stream-stream relations: not supported[0m
[32m- streaming plan - left outer join with stream-batch relations: supported[0m
[32m- streaming plan - left outer join with batch-stream relations: not supported[0m
[32m- streaming plan - left outer join with batch-batch relations: supported[0m
[32m- streaming plan - left outer join with stream-stream relations and join on attribute with left watermark: supported[0m
[32m- streaming plan - left outer join with stream-stream relations and join on attribute with right watermark: supported[0m
[32m- streaming plan - left outer join with stream-stream relations and join on non-watermarked attribute: not supported[0m
[32m- streaming plan - left outer join with stream-stream relations and state value watermark: supported[0m
[32m- streaming plan - left outer join with stream-stream relations and state value watermark: not supported[0m
[32m- streaming plan - left semi join with stream-stream relations: not supported[0m
[32m- streaming plan - left semi join with stream-batch relations: supported[0m
[32m- streaming plan - left semi join with batch-stream relations: not supported[0m
[32m- streaming plan - left semi join with batch-batch relations: supported[0m
[32m- streaming plan - left anti join with stream-stream relations: not supported[0m
[32m- streaming plan - left anti join with stream-batch relations: supported[0m
[32m- streaming plan - left anti join with batch-stream relations: not supported[0m
[32m- streaming plan - left anti join with batch-batch relations: supported[0m
[32m- streaming plan - right outer join with stream-stream relations: not supported[0m
[32m- streaming plan - right outer join with stream-batch relations: not supported[0m
[32m- streaming plan - right outer join with batch-stream relations: supported[0m
[32m- streaming plan - right outer join with batch-batch relations: supported[0m
[32m- streaming plan - right outer join with stream-stream relations and join on attribute with left watermark: supported[0m
[32m- streaming plan - right outer join with stream-stream relations and join on attribute with right watermark: supported[0m
[32m- streaming plan - right outer join with stream-stream relations and join on non-watermarked attribute: not supported[0m
[32m- streaming plan - right outer join with stream-stream relations and state value watermark: supported[0m
[32m- streaming plan - right outer join with stream-stream relations and state value watermark: not supported[0m
[32m- streaming plan - cogroup with stream-stream relations: not supported[0m
[32m- streaming plan - cogroup with stream-batch relations: not supported[0m
[32m- streaming plan - cogroup with batch-stream relations: not supported[0m
[32m- streaming plan - cogroup with batch-batch relations: supported[0m
[32m- streaming plan - union with stream-stream relations: supported[0m
[32m- streaming plan - union with stream-batch relations: not supported[0m
[32m- streaming plan - union with batch-stream relations: not supported[0m
[32m- streaming plan - union with batch-batch relations: supported[0m
[32m- streaming plan - except with stream-stream relations: not supported[0m
[32m- streaming plan - except with stream-batch relations: supported[0m
[32m- streaming plan - except with batch-stream relations: not supported[0m
[32m- streaming plan - except with batch-batch relations: supported[0m
[32m- streaming plan - intersect with stream-stream relations: not supported[0m
[32m- streaming plan - intersect with stream-batch relations: supported[0m
[32m- streaming plan - intersect with batch-stream relations: supported[0m
[32m- streaming plan - intersect with batch-batch relations: supported[0m
[32m- streaming plan - sort with stream relation: not supported[0m
[32m- streaming plan - sort with batch relation: supported[0m
[32m- streaming plan - sort - sort after aggregation in Complete output mode: supported[0m
[32m- streaming plan - sort - sort before aggregation in Complete output mode: not supported[0m
[32m- streaming plan - sort - sort over aggregated data in Update output mode: not supported[0m
[32m- streaming plan - sample with stream relation: not supported[0m
[32m- streaming plan - sample with batch relation: supported[0m
[32m- streaming plan - window with stream relation: not supported[0m
[32m- streaming plan - window with batch relation: supported[0m
[32m- streaming plan - Append output mode - aggregation: not supported[0m
[32m- streaming plan - Append output mode - no aggregation: supported[0m
[32m- streaming plan - Update output mode - aggregation: supported[0m
[32m- streaming plan - Update output mode - no aggregation: supported[0m
[32m- streaming plan - Complete output mode - aggregation: supported[0m
[32m- streaming plan - Complete output mode - no aggregation: not supported[0m
[32m- streaming plan - MonotonicallyIncreasingID: not supported[0m
[32m- continuous processing - TypedFilter: supported[0m
[32mStarJoinReorderSuite:[0m
[32m- Test 1: Selective star-join on all dimensions[0m
[32m- Test 2: Star join on a subset of dimensions due to inequality joins[0m
[32m- Test 3:  Star join on a subset of dimensions since join column is not unique[0m
[32m- Test 4: Star join on a subset of dimensions since join column is nullable[0m
[32m- Test 5: Table stats not available for some of the joined tables[0m
[32m- Test 6: Join with complex plans[0m
[32m- Test 7: Comparable fact table sizes[0m
[32m- Test 8: No RI joins[0m
[32m- Test 9: Complex join predicates[0m
[32m- Test 10: Less than two dimensions[0m
[32m- Test 11: Expanding star join[0m
[32m- Test 12: Non selective star join[0m
[32mCodeFormatterSuite:[0m
[32m- removing overlapping comments[0m
[32m- removing extra new lines and comments[0m
[32m- basic example[0m
[32m- nested example[0m
[32m- single line[0m
[32m- if else on the same line[0m
[32m- function calls[0m
[32m- function calls with maxLines=0[0m
[32m- function calls with maxLines=2[0m
[32m- single line comments[0m
[32m- single line comments /* */ [0m
[32m- multi-line comments[0m
[32m- reduce empty lines[0m
[32m- comment place holder[0m
[32mMathExpressionsSuite:[0m
[32m- conv[0m
[32m- e[0m
[32m- pi[0m
[32m- sin[0m
[32m- asin[0m
[32m- sinh[0m
[32m- cos[0m
[32m- acos[0m
[32m- cosh[0m
[32m- tan[0m
[32m- cot[0m
[32m- atan[0m
[32m- tanh[0m
[32m- toDegrees[0m
[32m- toRadians[0m
[32m- cbrt[0m
[32m- ceil[0m
[32m- floor[0m
[32m- factorial[0m
[32m- rint[0m
[32m- exp[0m
[32m- expm1[0m
[32m- signum[0m
[32m- log[0m
[32m- log10[0m
[32m- log1p[0m
[32m- bin[0m
[32m- log2[0m
[32m- sqrt[0m
[32m- pow[0m
[32m- shift left[0m
[32m- shift right[0m
[32m- shift right unsigned[0m
[32m- hex[0m
[32m- unhex[0m
[32m- hypot[0m
[32m- atan2[0m
[32m- binary log[0m
[32m- round/bround[0m
[32mCheckCartesianProductsSuite:[0m
[32m- CheckCartesianProducts doesn't throw an exception if cross joins are enabled)[0m
[32m- CheckCartesianProducts throws an exception for join types that require a join condition[0m
[32m- CheckCartesianProducts doesn't throw an exception if a join condition is present[0m
[32m- CheckCartesianProducts doesn't throw an exception if join types don't require conditions[0m
[32mProjectEstimationSuite:[0m
[32m- project with alias[0m
[32m- project on empty table[0m
[32m- test row size estimation[0m
[32mEncoderResolutionSuite:[0m
[32m- real type doesn't match encoder schema but they are compatible: product[0m
[32m- real type doesn't match encoder schema but they are compatible: nested product[0m
[32m- real type doesn't match encoder schema but they are compatible: tupled encoder[0m
[32m- real type doesn't match encoder schema but they are compatible: primitive array[0m
[32m- the real type is not compatible with encoder schema: primitive array[0m
[32m- real type doesn't match encoder schema but they are compatible: array[0m
[32m- real type doesn't match encoder schema but they are compatible: nested array[0m
[32m- the real type is not compatible with encoder schema: non-array field[0m
[32m- the real type is not compatible with encoder schema: array element type[0m
[32m- the real type is not compatible with encoder schema: nested array element type[0m
[32m- nullability of array type element should not fail analysis[0m
[32m- the real number of fields doesn't match encoder schema: tuple encoder[0m
[32m- the real number of fields doesn't match encoder schema: nested tuple encoder[0m
[32m- nested case class can have different number of fields from the real schema[0m
[32m- throw exception if real type is not compatible with encoder schema[0m
[32m- cast from int to Long should success[0m
[32m- cast from date to java.sql.Timestamp should success[0m
[32m- cast from bigint to String should success[0m
[32m- cast from int to java.math.BigDecimal should success[0m
[32m- cast from bigint to java.math.BigDecimal should success[0m
[32m- cast from bigint to Int should fail[0m
[32m- cast from timestamp to java.sql.Date should fail[0m
[32m- cast from decimal(38,18) to Double should fail[0m
[32m- cast from double to java.math.BigDecimal should fail[0m
[32m- cast from decimal(38,18) to Int should fail[0m
[32m- cast from string to Long should fail[0m
[32mCollectionExpressionsSuite:[0m
[32m- Array and Map Size - legacy[0m
[32m- Array and Map Size[0m
[32m- MapKeys/MapValues[0m
[32m- Map Concat[0m
[32m- MapFromEntries[0m
[32m- Sort Array[0m
[32m- Array contains[0m
[32m- ArraysOverlap[0m
[32m- Slice[0m
[32m- ArrayJoin[0m
[32m- ArraysZip[0m
[32m- Array Min[0m
[32m- Array max[0m
[32m- Sequence of numbers[0m
[32m- Sequence of timestamps[0m
[32m- Sequence on DST boundaries[0m
[32m- Sequence of dates[0m
[32m- Sequence with default step[0m
[32m- Reverse[0m
[32m- Array Position[0m
[32m- elementAt[0m
[32m- Concat[0m
[32m- Flatten[0m
[32m- ArrayRepeat[0m
[32m- Array remove[0m
[32m- Array Distinct[0m
[32m- Array Union[0m
[32m- Shuffle[0m
[32m- Array Except[0m
[32m- Array Intersect[0m
[32mQueryPlanSuite:[0m
[32m- origin remains the same after mapExpressions (SPARK-23823)[0m
[32mUDFXPathUtilSuite:[0m
[32m- illegal arguments[0m
[32m- generic eval[0m
[32m- boolean eval[0m
[32m- string eval[0m
[32m- embedFailure[0m
[32m- number eval[0m
[32m- node eval[0m
[32m- node list eval[0m
[32mPullOutNondeterministicSuite:[0m
[32m- no-op on filter[0m
[32m- sort[0m
[32m- aggregate[0m
[32mStarJoinCostBasedReorderSuite:[0m
[32m- Test 1: Star query with two dimensions and two regular tables[0m
[32m- Test 2: Star with a linear branch[0m
[32m- Test 3: Star with derived branches[0m
[32m- Test 4: Star with several branches[0m
[32m- Test 5: RI star only[0m
[32m- Test 6: No RI star[0m
[32mOptimizerStructuralIntegrityCheckerSuite:[0m
[32m- check for invalid plan after execution of rule[0m
[32mOuterJoinEliminationSuite:[0m
[32m- joins: full outer to inner[0m
[32m- joins: full outer to right[0m
[32m- joins: full outer to left[0m
[32m- joins: right to inner[0m
[32m- joins: left to inner[0m
[32m- joins: left to inner with complicated filter predicates #1[0m
[32m- joins: left to inner with complicated filter predicates #2[0m
[32m- joins: left to inner with complicated filter predicates #3[0m
[32m- joins: left to inner with complicated filter predicates #4[0m
[32m- joins: no outer join elimination if the filter is not NULL eliminated[0m
[32m- joins: no outer join elimination if the filter's constraints are not NULL eliminated[0m
[32m- no outer join elimination if constraint propagation is disabled[0m
[32mObjectExpressionsSuite:[0m
[32m- SPARK-16622: The returned value of the called method in Invoke can be null[0m
[32m- MapObjects should make copies of unsafe-backed data[0m
[32m- SPARK-23582: StaticInvoke should support interpreted execution[0m
[32m- SPARK-23583: Invoke should support interpreted execution[0m
[32m- SPARK-23593: InitializeJavaBean should support interpreted execution[0m
[32m- InitializeJavaBean doesn't call setters if input in null[0m
[32m- SPARK-23585: UnwrapOption should support interpreted execution[0m
[32m- SPARK-23586: WrapOption should support interpreted execution[0m
[32m- SPARK-23590: CreateExternalRow should support interpreted execution[0m
[32m- SPARK-23594 GetExternalRowField should support interpreted execution[0m
[32m- SPARK-23591: EncodeUsingSerializer should support interpreted execution[0m
[32m- SPARK-23587: MapObjects should support interpreted execution[0m
[32m- SPARK-23592: DecodeUsingSerializer should support interpreted execution[0m
[32m- SPARK-23584 NewInstance should support interpreted execution[0m
[32m- LambdaVariable should support interpreted execution[0m
[32m- SPARK-23588 CatalystToExternalMap should support interpreted execution[0m
[32m- SPARK-23595 ValidateExternalType should support interpreted execution[0m
[32m- SPARK-23589 ExternalMapToCatalyst should support interpreted execution[0m
[32mAnalysisSuite:[0m
[32m- union project *[0m
[32m- check project's resolved[0m
[32m- analyze project[0m
[32m- resolve sort references - filter/limit[0m
[32m- resolve sort references - join[0m
[32m- resolve sort references - aggregate[0m
[32m- resolve relations[0m
[32m- divide should be casted into fractional types[0m
[32m- pull out nondeterministic expressions from RepartitionByExpression[0m
[32m- pull out nondeterministic expressions from Sort[0m
[32m- SPARK-9634: cleanup unnecessary Aliases in LogicalPlan[0m
[32m- Analysis may leave unnecessary aliases[0m
[32m- SPARK-10534: resolve attribute references in order by clause[0m
[32m- self intersect should resolve duplicate expression IDs[0m
[32m- SPARK-8654: invalid CAST in NULL IN(...) expression[0m
[32m- SPARK-8654: different types in inlist but can be converted to a common type[0m
[32m- SPARK-8654: check type compatibility error[0m
[32m- SPARK-11725: correctly handle null inputs for ScalaUDF[0m
[32m- SPARK-24891 Fix HandleNullInputsForUDF rule[0m
[32m- SPARK-11863 mixture of aliases and real columns in order by clause - tpcds 19,55,71[0m
[32m- Eliminate the unnecessary union[0m
[32m- SPARK-12102: Ignore nullablity when comparing two sides of case[0m
[32m- Keep attribute qualifiers after dedup[0m
[32m- SPARK-15776: test whether Divide expression's data type can be deduced correctly by analyzer[0m
[32m- SPARK-18058: union and set operations shall not care about the nullability when comparing column types[0m
[32m- resolve as with an already existed alias[0m
[32m- SPARK-20311 range(N) as alias[0m
[32m- SPARK-20841 Support table column aliases in FROM clause[0m
[32m- SPARK-20962 Support subquery column aliases in FROM clause[0m
[32m- SPARK-20963 Support aliases for join relations in FROM clause[0m
[32m- SPARK-22614 RepartitionByExpression partitioning[0m
[32m- SPARK-24208: analysis fails on self-join with FlatMapGroupsInPandas[0m
[32m- SPARK-24488 Generator with multiple aliases[0m
[32m- SPARK-24151: CURRENT_DATE, CURRENT_TIMESTAMP should be case insensitive[0m
[32mNondeterministicSuite:[0m
[32m- MonotonicallyIncreasingID[0m
[32m- SparkPartitionID[0m
[32m- InputFileName[0m
[32mExpressionParserSuite:[0m
[32m- star expressions[0m
[32m- named expressions[0m
[32m- binary logical expressions[0m
[32m- long binary logical expressions[0m
[32m- not expressions[0m
[32m- exists expression[0m
[32m- comparison expressions[0m
[32m- between expressions[0m
[32m- in expressions[0m
[32m- in sub-query[0m
[32m- like expressions[0m
[32m- like expressions with ESCAPED_STRING_LITERALS = true[0m
[32m- is null expressions[0m
[32m- is distinct expressions[0m
[32m- binary arithmetic expressions[0m
[32m- unary arithmetic expressions[0m
[32m- cast expressions[0m
[32m- function expressions[0m
[32m- lambda functions[0m
[32m- window function expressions[0m
[32m- range/rows window function expressions[0m
[32m- row constructor[0m
[32m- scalar sub-query[0m
[32m- case when[0m
[32m- dereference[0m
[32m- reference[0m
[32m- subscript[0m
[32m- parenthesis[0m
[32m- type constructors[0m
[32m- literals[0m
[32m- strings[0m
[32m- intervals[0m
[32m- composed expressions[0m
[32m- SPARK-17364, fully qualified column name which starts with number[0m
[32m- SPARK-17832 function identifier contains backtick[0m
[32m- SPARK-19526 Support ignore nulls keywords for first and last[0m
[32mFilterEstimationSuite:[0m
[32m- true[0m
[32m- false[0m
[32m- null[0m
[32m- Not(null)[0m
[32m- Not(Not(null))[0m
[32m- cint < 3 AND null[0m
[32m- cint < 3 OR null[0m
[32m- Not(cint < 3 AND null)[0m
[32m- Not(cint < 3 OR null)[0m
[32m- Not(cint < 3 AND Not(null))[0m
[32m- cint = 2[0m
[32m- cint <=> 2[0m
[32m- cint = 0[0m
[32m- cint < 3[0m
[32m- cint < 0[0m
[32m- cint <= 3[0m
[32m- cint > 6[0m
[32m- cint > 10[0m
[32m- cint >= 6[0m
[32m- cint IS NULL[0m
[32m- cint IS NOT NULL[0m
[32m- cint IS NOT NULL && null[0m
[32m- cint > 3 AND cint <= 6[0m
[32m- cint = 3 OR cint = 6[0m
[32m- Not(cint > 3 AND cint <= 6)[0m
[32m- Not(cint <= 3 OR cint > 6)[0m
[32m- Not(cint = 3 AND cstring < 'A8')[0m
[32m- Not(cint = 3 OR cstring < 'A8')[0m
[32m- cint IN (3, 4, 5)[0m
[32m- evaluateInSet with all zeros[0m
[32m- evaluateInSet with string[0m
[32m- cint NOT IN (3, 4, 5)[0m
[32m- cbool IN (true)[0m
[32m- cbool = true[0m
[32m- cbool > false[0m
[32m- cdate = cast('2017-01-02' AS DATE)[0m
[32m- cdate < cast('2017-01-03' AS DATE)[0m
[32m- cdate IN ( cast('2017-01-03' AS DATE),[0m
[32m      cast('2017-01-04' AS DATE), cast('2017-01-05' AS DATE) )[0m
[32m- cdecimal = 0.400000000000000000[0m
[32m- cdecimal < 0.60 [0m
[32m- cdouble < 3.0[0m
[32m- cstring = 'A2'[0m
[32m- cstring < 'A2' - unsupported condition[0m
[32m- cint IN (1, 2, 3, 4, 5)[0m
[32m- don't estimate IsNull or IsNotNull if the child is a non-leaf node[0m
[32m- cint = cint2[0m
[32m- cint > cint2[0m
[32m- cint < cint2[0m
[32m- cint = cint4[0m
[32m- cint < cint4[0m
[32m- cint = cint3[0m
[32m- cint < cint3[0m
[32m- cint > cint3[0m
[32m- update ndv for columns based on overall selectivity[0m
[32m- Not(cintHgm < 3 AND null)[0m
[32m- cintHgm = 5[0m
[32m- cintHgm = 0[0m
[32m- cintHgm < 3[0m
[32m- cintHgm < 0[0m
[32m- cintHgm <= 3[0m
[32m- cintHgm > 6[0m
[32m- cintHgm > 10[0m
[32m- cintHgm >= 6[0m
[32m- cintHgm > 3 AND cintHgm <= 6[0m
[32m- cintHgm = 3 OR cintHgm = 6[0m
[32m- Not(cintSkewHgm < 3 AND null)[0m
[32m- cintSkewHgm = 5[0m
[32m- cintSkewHgm = 0[0m
[32m- cintSkewHgm < 3[0m
[32m- cintSkewHgm < 0[0m
[32m- cintSkewHgm <= 3[0m
[32m- cintSkewHgm > 6[0m
[32m- cintSkewHgm > 10[0m
[32m- cintSkewHgm >= 6[0m
[32m- cintSkewHgm > 3 AND cintSkewHgm <= 6[0m
[32m- cintSkewHgm = 3 OR cintSkewHgm = 6[0m
[32m- ColumnStatsMap tests[0m
[32mDataTypeSuite:[0m
[32m- construct an ArrayType[0m
[32m- construct an MapType[0m
[32m- construct with add[0m
[32m- construct with add from StructField[0m
[32m- construct with add from StructField with comments[0m
[32m- construct with String DataType[0m
[32m- extract fields from a StructType[0m
[32m- extract field index from a StructType[0m
[32m- fieldsMap returns map of name to StructField[0m
[32m- fieldNames and names returns field names[0m
[32m- merge where right contains type conflict[0m
[32m- existsRecursively[0m
[32m- from Json - NullType[0m
[32m- from Json - BooleanType[0m
[32m- from DDL - BooleanType[0m
[32m- from Json - ByteType[0m
[32m- from DDL - ByteType[0m
[32m- from Json - ShortType[0m
[32m- from DDL - ShortType[0m
[32m- from Json - IntegerType[0m
[32m- from DDL - IntegerType[0m
[32m- from Json - LongType[0m
[32m- from DDL - LongType[0m
[32m- from Json - FloatType[0m
[32m- from DDL - FloatType[0m
[32m- from Json - DoubleType[0m
[32m- from DDL - DoubleType[0m
[32m- from Json - DecimalType(10,5)[0m
[32m- from DDL - DecimalType(10,5)[0m
[32m- from Json - DecimalType(38,18)[0m
[32m- from DDL - DecimalType(38,18)[0m
[32m- from Json - DateType[0m
[32m- from DDL - DateType[0m
[32m- from Json - TimestampType[0m
[32m- from DDL - TimestampType[0m
[32m- from Json - StringType[0m
[32m- from DDL - StringType[0m
[32m- from Json - BinaryType[0m
[32m- from DDL - BinaryType[0m
[32m- from Json - ArrayType(DoubleType,true)[0m
[32m- from DDL - ArrayType(DoubleType,true)[0m
[32m- from Json - ArrayType(StringType,false)[0m
[32m- from DDL - ArrayType(StringType,false)[0m
[32m- from Json - MapType(IntegerType,StringType,true)[0m
[32m- from DDL - MapType(IntegerType,StringType,true)[0m
[32m- from Json - MapType(IntegerType,ArrayType(DoubleType,true),false)[0m
[32m- from DDL - MapType(IntegerType,ArrayType(DoubleType,true),false)[0m
[32m- from Json - StructType(StructField(a,IntegerType,true), StructField(b,ArrayType(DoubleType,true),false), StructField(c,DoubleType,false))[0m
[32m- from DDL - StructType(StructField(a,IntegerType,true), StructField(b,ArrayType(DoubleType,true),false), StructField(c,DoubleType,false))[0m
[32m- fromJson throws an exception when given type string is invalid[0m
[32m- Check the default size of NullType[0m
[32m- Check the default size of BooleanType[0m
[32m- Check the default size of ByteType[0m
[32m- Check the default size of ShortType[0m
[32m- Check the default size of IntegerType[0m
[32m- Check the default size of LongType[0m
[32m- Check the default size of FloatType[0m
[32m- Check the default size of DoubleType[0m
[32m- Check the default size of DecimalType(10,5)[0m
[32m- Check the default size of DecimalType(38,18)[0m
[32m- Check the default size of DateType[0m
[32m- Check the default size of TimestampType[0m
[32m- Check the default size of StringType[0m
[32m- Check the default size of BinaryType[0m
[32m- Check the default size of ArrayType(DoubleType,true)[0m
[32m- Check the default size of ArrayType(StringType,false)[0m
[32m- Check the default size of MapType(IntegerType,StringType,true)[0m
[32m- Check the default size of MapType(IntegerType,ArrayType(DoubleType,true),false)[0m
[32m- Check the default size of StructType(StructField(a,IntegerType,true), StructField(b,ArrayType(DoubleType,true),false), StructField(c,DoubleType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,true), to: ArrayType(DoubleType,true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,false), to: ArrayType(DoubleType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,false), to: ArrayType(DoubleType,true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,true), to: ArrayType(DoubleType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,false), to: ArrayType(StringType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,true), to: MapType(StringType,DoubleType,true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,false), to: MapType(StringType,DoubleType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,false), to: MapType(StringType,DoubleType,true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,true), to: MapType(StringType,DoubleType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,ArrayType(IntegerType,true),true), to: MapType(StringType,ArrayType(IntegerType,false),true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,ArrayType(IntegerType,false),true), to: MapType(StringType,ArrayType(IntegerType,true),true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,true)), to: StructType(StructField(a,StringType,true)))[0m
[32m- equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,false)), to: StructType(StructField(a,StringType,false)))[0m
[32m- equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,false)), to: StructType(StructField(a,StringType,true)))[0m
[32m- equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,true)), to: StructType(StructField(a,StringType,false)))[0m
[32m- equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,false), StructField(b,StringType,true)), to: StructType(StructField(a,StringType,false), StructField(b,StringType,false)))[0m
[32m- catalogString: BooleanType[0m
[32m- catalogString: ByteType[0m
[32m- catalogString: ShortType[0m
[32m- catalogString: IntegerType[0m
[32m- catalogString: LongType[0m
[32m- catalogString: FloatType[0m
[32m- catalogString: DoubleType[0m
[32m- catalogString: DecimalType(10,5)[0m
[32m- catalogString: BinaryType[0m
[32m- catalogString: StringType[0m
[32m- catalogString: DateType[0m
[32m- catalogString: TimestampType[0m
[32m- catalogString: StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))[0m
[32m- catalogString: StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true), StructField(col4,IntegerType,true), StructField(col5,IntegerType,true), StructField(col6,IntegerType,true), StructField(col7,IntegerType,true), StructField(col8,IntegerType,true), StructField(col9,IntegerType,true), StructField(col10,IntegerType,true), StructField(col11,IntegerType,true), StructField(col12,IntegerType,true), StructField(col13,IntegerType,true), StructField(col14,IntegerType,true), StructField(col15,IntegerType,true), StructField(col16,IntegerType,true), StructField(col17,IntegerType,true), StructField(col18,IntegerType,true), StructField(col19,IntegerType,true), StructField(col20,IntegerType,true), StructField(col21,IntegerType,true), StructField(col22,IntegerType,true), StructField(col23,IntegerType,true), StructField(col24,IntegerType,true), StructField(col25,IntegerType,true), StructField(col26,IntegerType,true), StructField(col27,IntegerType,true), StructField(col28,IntegerType,true), StructField(col29,IntegerType,true), StructField(col30,IntegerType,true), StructField(col31,IntegerType,true), StructField(col32,IntegerType,true), StructField(col33,IntegerType,true), StructField(col34,IntegerType,true), StructField(col35,IntegerType,true), StructField(col36,IntegerType,true), StructField(col37,IntegerType,true), StructField(col38,IntegerType,true), StructField(col39,IntegerType,true))[0m
[32m- catalogString: ArrayType(IntegerType,true)[0m
[32m- catalogString: ArrayType(StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true), StructField(col4,IntegerType,true), StructField(col5,IntegerType,true), StructField(col6,IntegerType,true), StructField(col7,IntegerType,true), StructField(col8,IntegerType,true), StructField(col9,IntegerType,true), StructField(col10,IntegerType,true), StructField(col11,IntegerType,true), StructField(col12,IntegerType,true), StructField(col13,IntegerType,true), StructField(col14,IntegerType,true), StructField(col15,IntegerType,true), StructField(col16,IntegerType,true), StructField(col17,IntegerType,true), StructField(col18,IntegerType,true), StructField(col19,IntegerType,true), StructField(col20,IntegerType,true), StructField(col21,IntegerType,true), StructField(col22,IntegerType,true), StructField(col23,IntegerType,true), StructField(col24,IntegerType,true), StructField(col25,IntegerType,true), StructField(col26,IntegerType,true), StructField(col27,IntegerType,true), StructField(col28,IntegerType,true), StructField(col29,IntegerType,true), StructField(col30,IntegerType,true), StructField(col31,IntegerType,true), StructField(col32,IntegerType,true), StructField(col33,IntegerType,true), StructField(col34,IntegerType,true), StructField(col35,IntegerType,true), StructField(col36,IntegerType,true), StructField(col37,IntegerType,true), StructField(col38,IntegerType,true), StructField(col39,IntegerType,true)),true)[0m
[32m- catalogString: MapType(IntegerType,StringType,true)[0m
[32m- catalogString: MapType(IntegerType,StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true), StructField(col4,IntegerType,true), StructField(col5,IntegerType,true), StructField(col6,IntegerType,true), StructField(col7,IntegerType,true), StructField(col8,IntegerType,true), StructField(col9,IntegerType,true), StructField(col10,IntegerType,true), StructField(col11,IntegerType,true), StructField(col12,IntegerType,true), StructField(col13,IntegerType,true), StructField(col14,IntegerType,true), StructField(col15,IntegerType,true), StructField(col16,IntegerType,true), StructField(col17,IntegerType,true), StructField(col18,IntegerType,true), StructField(col19,IntegerType,true), StructField(col20,IntegerType,true), StructField(col21,IntegerType,true), StructField(col22,IntegerType,true), StructField(col23,IntegerType,true), StructField(col24,IntegerType,true), StructField(col25,IntegerType,true), StructField(col26,IntegerType,true), StructField(col27,IntegerType,true), StructField(col28,IntegerType,true), StructField(col29,IntegerType,true), StructField(col30,IntegerType,true), StructField(col31,IntegerType,true), StructField(col32,IntegerType,true), StructField(col33,IntegerType,true), StructField(col34,IntegerType,true), StructField(col35,IntegerType,true), StructField(col36,IntegerType,true), StructField(col37,IntegerType,true), StructField(col38,IntegerType,true), StructField(col39,IntegerType,true)),true)[0m
[32m- equalsStructurally: (from: BooleanType, to: BooleanType)[0m
[32m- equalsStructurally: (from: IntegerType, to: IntegerType)[0m
[32m- equalsStructurally: (from: IntegerType, to: LongType)[0m
[32m- equalsStructurally: (from: ArrayType(IntegerType,true), to: ArrayType(IntegerType,true))[0m
[32m- equalsStructurally: (from: ArrayType(IntegerType,true), to: ArrayType(IntegerType,false))[0m
[32m- equalsStructurally: (from: StructType(StructField(f1,IntegerType,true)), to: StructType(StructField(f2,IntegerType,true)))[0m
[32m- equalsStructurally: (from: StructType(StructField(f1,IntegerType,true)), to: StructType(StructField(f2,IntegerType,false)))[0m
[32m- equalsStructurally: (from: StructType(StructField(f1,IntegerType,true), StructField(f,StructType(StructField(f2,StringType,true)),true)), to: StructType(StructField(f2,IntegerType,true), StructField(g,StructType(StructField(f1,StringType,true)),true)))[0m
[32m- equalsStructurally: (from: StructType(StructField(f1,IntegerType,true), StructField(f,StructType(StructField(f2,StringType,false)),true)), to: StructType(StructField(f2,IntegerType,true), StructField(g,StructType(StructField(f1,StringType,true)),true)))[0m
[32m- SPARK-25031: MapType should produce current formatted string for complex types[0m
[32mFoldablePropagationSuite:[0m
[32m- Propagate from subquery[0m
[32m- Propagate to select clause[0m
[32m- Propagate to where clause[0m
[32m- Propagate to orderBy clause[0m
[32m- Propagate to groupBy clause[0m
[32m- Propagate in a complex query[0m
[32m- Propagate in subqueries of Union queries[0m
[32m- Propagate in inner join[0m
[32m- Propagate in expand[0m
[32m- Propagate above outer join[0m
[32mInMemoryCatalogSuite:[0m
[32m- basic create and list databases[0m
[32m- get database when a database exists[0m
[32m- get database should throw exception when the database does not exist[0m
[32m- list databases without pattern[0m
[32m- list databases with pattern[0m
[32m- drop database[0m
[32m- drop database when the database is not empty[0m
[32m- drop database when the database does not exist[0m
[32m- alter database[0m
[32m- alter database should throw exception when the database does not exist[0m
[32m- the table type of an external table should be EXTERNAL_TABLE[0m
[32m- create table when the table already exists[0m
[32m- drop table[0m
[32m- drop table when database/table does not exist[0m
[32m- rename table[0m
[32m- rename table when database/table does not exist[0m
[32m- rename table when destination table already exists[0m
[32m- alter table[0m
[32m- alter table when database/table does not exist[0m
[32m- alter table schema[0m
[32m- alter table stats[0m
[32m- get table[0m
[32m- get table when database/table does not exist[0m
[32m- list tables without pattern[0m
[32m- list tables with pattern[0m
[32m- column names should be case-preserving and column nullability should be retained[0m
[32m- basic create and list partitions[0m
[32m- create partitions when database/table does not exist[0m
[32m- create partitions that already exist[0m
[32m- create partitions without location[0m
[32m- create/drop partitions in managed tables with location[0m
[32m- list partition names[0m
[32m- list partition names with partial partition spec[0m
[32m- list partitions with partial partition spec[0m
[32m- SPARK-21457: list partitions with special chars[0m
[32m- list partitions by filter[0m
[32m- drop partitions[0m
[32m- drop partitions when database/table does not exist[0m
[32m- drop partitions that do not exist[0m
[32m- get partition[0m
[32m- get partition when database/table does not exist[0m
[32m- rename partitions[0m
[32m- rename partitions should update the location for managed table[0m
[32m- rename partitions when database/table does not exist[0m
[32m- rename partitions when the new partition already exists[0m
[32m- alter partitions[0m
[32m- alter partitions when database/table does not exist[0m
[32m- basic create and list functions[0m
[32m- create function when database does not exist[0m
[32m- create function that already exists[0m
[32m- drop function[0m
[32m- drop function when database does not exist[0m
[32m- drop function that does not exist[0m
[32m- get function[0m
[32m- get function when database does not exist[0m
[32m- rename function[0m
[32m- rename function when database does not exist[0m
[32m- rename function when new function already exists[0m
[32m- alter function[0m
[32m- list functions[0m
[32m- create/drop database should create/delete the directory[0m
[32m- create/drop/rename table should create/delete/rename the directory[0m
[32m- create/drop/rename partitions should create/delete/rename the directory[0m
[32m- drop partition from external table should not delete the directory[0m
[32mTypedFilterOptimizationSuite:[0m
[32m- filter after serialize with the same object type[0m
[32m- filter after serialize with different object types[0m
[32m- filter before deserialize with the same object type[0m
[32m- filter before deserialize with different object types[0m
[32m- back to back filter with the same object type[0m
[32m- back to back filter with different object types[0m
[32m- back to back FilterFunction with the same object type[0m
[32m- back to back FilterFunction with different object types[0m
[32m- FilterFunction and filter with the same object type[0m
[32m- FilterFunction and filter with different object types[0m
[32m- filter and FilterFunction with the same object type[0m
[32m- filter and FilterFunction with different object types[0m
[32mReorderAssociativeOperatorSuite:[0m
[32m- Reorder associative operators[0m
[32m- nested expression with aggregate operator[0m
[32mCombiningLimitsSuite:[0m
[32m- limits: combines two limits[0m
[32m- limits: combines three limits[0m
[32m- limits: combines two limits after ColumnPruning[0m
[32mResolveHintsSuite:[0m
[32m- invalid hints should be ignored[0m
[32m- case-sensitive or insensitive parameters[0m
[32m- multiple broadcast hint aliases[0m
[32m- do not traverse past existing broadcast hints[0m
[32m- should work for subqueries[0m
[32m- do not traverse past subquery alias[0m
[32m- should work for CTE[0m
[32m- should not traverse down CTE[0m
[32m- coalesce and repartition hint[0m
[32mPruneFiltersSuite:[0m
[32m- Constraints of isNull + LeftOuter[0m
[32m- Constraints of unionall[0m
[32m- Pruning multiple constraints in the same run[0m
[32m- Partial pruning[0m
[32m- No predicate is pruned[0m
[32m- Nondeterministic predicate is not pruned[0m
[32m- No pruning when constraint propagation is disabled[0m
[32mInMemorySessionCatalogSuite:[0m
[32m- basic create and list databases[0m
[32m- create databases using invalid names[0m
[32m- get database when a database exists[0m
[32m- get database should throw exception when the database does not exist[0m
[32m- list databases without pattern[0m
[32m- list databases with pattern[0m
[32m- drop database[0m
[32m- drop database when the database is not empty[0m
[32m- drop database when the database does not exist[0m
[32m- drop current database and drop default database[0m
[32m- alter database[0m
[32m- alter database should throw exception when the database does not exist[0m
[32m- get/set current database[0m
[32m- create table[0m
[32m- create tables using invalid names[0m
[32m- create table when database does not exist[0m
[32m- create temp view[0m
[32m- drop table[0m
[32m- drop table when database/table does not exist[0m
[32m- drop temp table[0m
[32m- rename table[0m
[32m- rename tables to an invalid name[0m
[32m- rename table when database/table does not exist[0m
[32m- rename temp table[0m
[32m- alter table[0m
[32m- alter table when database/table does not exist[0m
[32m- alter table stats[0m
[32m- alter table add columns[0m
[32m- alter table drop columns[0m
[32m- get table[0m
[32m- get table when database/table does not exist[0m
[32m- lookup table relation[0m
[32m- look up view relation[0m
[32m- table exists[0m
[32m- getTempViewOrPermanentTableMetadata on temporary views[0m
[32m- list tables without pattern[0m
[32m- list tables with pattern[0m
[32m- basic create and list partitions[0m
[32m- create partitions when database/table does not exist[0m
[32m- create partitions that already exist[0m
[32m- create partitions with invalid part spec[0m
[32m- drop partitions[0m
[32m- drop partitions when database/table does not exist[0m
[32m- drop partitions that do not exist[0m
[32m- drop partitions with invalid partition spec[0m
[32m- get partition[0m
[32m- get partition when database/table does not exist[0m
[32m- get partition with invalid partition spec[0m
[32m- rename partitions[0m
[32m- rename partitions when database/table does not exist[0m
[32m- rename partition with invalid partition spec[0m
[32m- alter partitions[0m
[32m- alter partitions when database/table does not exist[0m
[32m- alter partition with invalid partition spec[0m
[32m- list partition names[0m
[32m- list partition names with partial partition spec[0m
[32m- list partition names with invalid partial partition spec[0m
[32m- list partitions[0m
[32m- list partitions with partial partition spec[0m
[32m- list partitions with invalid partial partition spec[0m
[32m- list partitions when database/table does not exist[0m
[32m- basic create and list functions[0m
[32m- create function when database does not exist[0m
[32m- create function that already exists[0m
[32m- create temp function[0m
[32m- isTemporaryFunction[0m
[32m- isRegisteredFunction[0m
[32m- isPersistentFunction[0m
[32m- drop function[0m
[32m- drop function when database/function does not exist[0m
[32m- drop temp function[0m
[32m- get function[0m
[32m- get function when database/function does not exist[0m
[32m- lookup temp function[0m
[32m- list functions[0m
[32m- list functions when database does not exist[0m
[32m- copy SessionCatalog state - temp views[0m
[32m- copy SessionCatalog state - current db[0m
[32m- SPARK-19737: detect undefined functions without triggering relation resolution[0m
[32mComplexTypeSuite:[0m
[32m- GetArrayItem[0m
[32m- GetMapValue[0m
[32m- GetStructField[0m
[32m- GetArrayStructFields[0m
[32m- CreateArray[0m
[32m- CreateMap[0m
[32m- MapFromArrays[0m
[32m- CreateStruct[0m
[32m- CreateNamedStruct[0m
[32m- test dsl for complex type[0m
[32m- error message of ExtractValue[0m
[32m- ensure to preserve metadata[0m
[32m- StringToMap[0m
[32m- SPARK-22693: CreateNamedStruct should not use global variables[0m
[32mCollapseWindowSuite:[0m
[32m- collapse two adjacent windows with the same partition/order[0m
[32m- Don't collapse adjacent windows with different partitions or orders[0m
[32m- Don't collapse adjacent windows with dependent columns[0m
[32mCallMethodViaReflectionSuite:[0m
[32m- findMethod via reflection for static methods[0m
[32m- findMethod for a JDK library[0m
[32m- class not found[0m
[32m- method not found because name does not match[0m
[32m- method not found because there is no static method[0m
[32m- input type checking[0m
[32m- unsupported type checking[0m
[32m- invoking methods using acceptable types[0m
[32mPercentileSuite:[0m
[32m- serialize and de-serialize[0m
[32m- class Percentile, high level interface, update, merge, eval...[0m
[32m- class Percentile, low level interface, update, merge, eval...[0m
[32m- fail analysis if childExpression is invalid[0m
[32m- fails analysis if percentage(s) are invalid[0m
[32m- null handling[0m
[32m- negatives frequency column handling[0m
[32mExprIdSuite:[0m
[32m- hashcode independent of jvmId[0m
[32m- equality should depend on both id and jvmId[0m
[32mResolveGroupingAnalyticsSuite:[0m
[32m- rollupExprs[0m
[32m- cubeExprs[0m
[32m- grouping sets[0m
[32m- grouping sets with no explicit group by expressions[0m
[32m- cube[0m
[32m- rollup[0m
[32m- grouping function[0m
[32m- grouping_id[0m
[32m- filter with grouping function[0m
[32m- sort with grouping function[0m
[32mDecimalSuite:[0m
[32m- creating decimals[0m
[32m- creating decimals with negative scale[0m
[32m- double and long values[0m
[32m- small decimals represented as unscaled long[0m
[32m- hash code[0m
[32m- equals[0m
[32m- isZero[0m
[32m- arithmetic[0m
[32m- accurate precision after multiplication[0m
[32m- fix non-terminating decimal expansion problem[0m
[32m- fix loss of precision/scale when doing division operation[0m
[32m- set/setOrNull[0m
[32m- changePrecision/toPrecision on compact decimal should respect rounding mode[0m
[32m- SPARK-20341: support BigInt's value does not fit in long value range[0m
[32m- SPARK-26038: toScalaBigInt/toJavaBigInteger[0m
[32mArrayDataIndexedSeqSuite:[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(BooleanType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(BooleanType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(BooleanType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(BooleanType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(ByteType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(ByteType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(ByteType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(ByteType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(ShortType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(ShortType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(ShortType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(ShortType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(IntegerType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(IntegerType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(IntegerType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(IntegerType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(LongType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(LongType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(LongType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(LongType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(FloatType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(FloatType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(FloatType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(FloatType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DoubleType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DoubleType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DoubleType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DoubleType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DecimalType(10,0),false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DecimalType(10,0),false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DecimalType(10,0),true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DecimalType(10,0),true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(StringType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(StringType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(StringType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(StringType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(BinaryType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(BinaryType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(BinaryType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(BinaryType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DateType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DateType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DateType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DateType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(TimestampType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(TimestampType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(TimestampType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(TimestampType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(CalendarIntervalType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(CalendarIntervalType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(CalendarIntervalType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(CalendarIntervalType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(org.apache.spark.sql.catalyst.encoders.ExamplePointUDT@216abc2,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(org.apache.spark.sql.catalyst.encoders.ExamplePointUDT@216abc2,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(org.apache.spark.sql.catalyst.encoders.ExamplePointUDT@216abc2,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(org.apache.spark.sql.catalyst.encoders.ExamplePointUDT@216abc2,true)[0m
[32mNumberConverterSuite:[0m
[32m- convert[0m
[32mColumnPruningSuite:[0m
[32m- Column pruning for Generate when Generate.unrequiredChildIndex = child.output[0m
[32m- Fill Generate.unrequiredChildIndex if possible[0m
[32m- Another fill Generate.unrequiredChildIndex if possible[0m
[32m- Column pruning for Project on Sort[0m
[32m- Column pruning for Expand[0m
[32m- Column pruning for ScriptTransformation[0m
[32m- Column pruning on Filter[0m
[32m- Column pruning on except/intersect/distinct[0m
[32m- Column pruning on Project[0m
[32m- Eliminate the Project with an empty projectList[0m
[32m- column pruning for group[0m
[32m- column pruning for group with alias[0m
[32m- column pruning for Project(ne, Limit)[0m
[32m- push down project past sort[0m
[32m- Column pruning on Window with useless aggregate functions[0m
[32m- Column pruning on Window with selected agg expressions[0m
[32m- Column pruning on Window in select[0m
[32m- Column pruning on Union[0m
[32m- Remove redundant projects in column pruning rule[0m
[32m- Column pruning on MapPartitions[0m
[32m- push project down into sample[0m
[32m- SPARK-24696 ColumnPruning rule fails to remove extra Project[0m
[32mStringUtilsSuite:[0m
[32m- escapeLikeRegex[0m
[32m- filter pattern[0m
[32mPropagateEmptyRelationSuite:[0m
[32m- propagate empty relation through Union[0m
[32m- propagate empty relation through Join[0m
[32m- propagate empty relation through UnaryNode[0m
[32m- propagate empty streaming relation through multiple UnaryNode[0m
[32m- don't propagate empty streaming relation through agg[0m
[32m- don't propagate non-empty local relation[0m
[32m- propagate empty relation through Aggregate with grouping expressions[0m
[32m- don't propagate empty relation through Aggregate without grouping expressions[0m
[32m- propagate empty relation keeps the plan resolved[0m
[32mConstantPropagationSuite:[0m
[32m- basic test[0m
[32m- with combination of AND and OR predicates[0m
[32m- equality predicates outside a `NOT` can be propagated within a `NOT`[0m
[32m- equality predicates inside a `NOT` should not be picked for propagation[0m
[32m- equality predicates outside a `OR` can be propagated within a `OR`[0m
[32m- equality predicates inside a `OR` should not be picked for propagation[0m
[32m- equality operator not immediate child of root `AND` should not be used for propagation[0m
[32m- conflicting equality predicates[0m
[32mBinaryComparisonSimplificationSuite:[0m
[32m- Preserve nullable exprs in general[0m
[32m- Preserve non-deterministic exprs[0m
[32m- Nullable Simplification Primitive: <=>[0m
[32m- Non-Nullable Simplification Primitive[0m
[32m- Expression Normalization[0m
[32m- SPARK-26402: accessing nested fields with different cases in case insensitive mode[0m
[32mHyperLogLogPlusPlusSuite:[0m
[32m- test invalid parameter relativeSD[0m
[32m- add nulls[0m
[32m- deterministic cardinality estimation[0m
[32m- random cardinality estimation[0m
[32m- merging HLL instances[0m
[32mLastTestSuite:[0m
[32m- empty buffer[0m
[32m- update[0m
[32m- update - ignore nulls[0m
[32m- merge[0m
[32m- merge - ignore nulls[0m
[32m- eval[0m
[32m- eval - ignore nulls[0m
[32mQuantileSummariesSuite:[0m
[32m- Extremas with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=increasing, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Extremas with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=increasing, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Extremas with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=increasing, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Extremas with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=increasing, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Extremas with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=decreasing, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Extremas with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=decreasing, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Extremas with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=decreasing, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Extremas with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=decreasing, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Extremas with epsi=0.1 and seq=random, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=random, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=random, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=random, compression=1000[0m
[32m- Extremas with epsi=0.1 and seq=random, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=random, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=random, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=random, compression=10[0m
[32m- Extremas with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=random, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Extremas with epsi=1.0E-4 and seq=random, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=random, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=random, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=random, compression=10[0m
[32m- Merging ordered lists with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Merging ordered lists with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Merging ordered lists with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Merging ordered lists with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Merging ordered lists with epsi=0.1 and seq=random, compression=1000[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=random, compression=1000[0m
[32m- Merging ordered lists with epsi=0.1 and seq=random, compression=10[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=random, compression=10[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=random, compression=10[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=random, compression=10[0m
[32mDataSourceV2AnalysisSuite:[0m
[32m- Append.byName: basic behavior[0m
[32m- Append.byName: does not match by position[0m
[32m- Append.byName: case sensitive column resolution[0m
[32m- Append.byName: case insensitive column resolution[0m
[32m- Append.byName: data columns are reordered by name[0m
[32m- Append.byName: fail nullable data written to required columns[0m
[32m- Append.byName: allow required data written to nullable columns[0m
[32m- Append.byName: missing required columns cause failure and are identified by name[0m
[32m- Append.byName: missing optional columns cause failure and are identified by name[0m
[32m- Append.byName: fail canWrite check[0m
[32m- Append.byName: insert safe cast[0m
[32m- Append.byName: fail extra data fields[0m
[32m- Append.byName: multiple field errors are reported[0m
[32m- Append.byPosition: basic behavior[0m
[32m- Append.byPosition: data columns are not reordered[0m
[32m- Append.byPosition: fail nullable data written to required columns[0m
[32m- Append.byPosition: allow required data written to nullable columns[0m
[32m- Append.byPosition: missing required columns cause failure[0m
[32m- Append.byPosition: missing optional columns cause failure[0m
[32m- Append.byPosition: fail canWrite check[0m
[32m- Append.byPosition: insert safe cast[0m
[32m- Append.byPosition: fail extra data fields[0m
[32m- Append.byPosition: multiple field errors are reported[0m
[32mStringExpressionsSuite:[0m
[32m- concat[0m
[32m- SPARK-22498: Concat should not generate codes beyond 64KB[0m
[32m- SPARK-22771 Check Concat.checkInputDataTypes results[0m
[32m- concat_ws[0m
[32m- SPARK-22549: ConcatWs should not generate codes beyond 64KB[0m
[32m- elt[0m
[32m- SPARK-22550: Elt should not generate codes beyond 64KB[0m
[32m- StringComparison[0m
[32m- Substring[0m
[32m- string substring_index function[0m
[32m- ascii for string[0m
[32m- string for ascii[0m
[32m- base64/unbase64 for string[0m
[32m- encode/decode for string[0m
[32m- initcap unit test[0m
[32m- Levenshtein distance[0m
[32m- soundex unit test[0m
[32m- replace[0m
[32m- translate[0m
[32m- TRIM[0m
[32m- LTRIM[0m
[32m- RTRIM[0m
[32m- FORMAT[0m
[32m- SPARK-22603: FormatString should not generate codes beyond 64KB[0m
[32m- INSTR[0m
[32m- LOCATE[0m
[32m- LPAD/RPAD[0m
[32m- REPEAT[0m
[32m- REVERSE[0m
[32m- SPACE[0m
[32m- length for string / binary[0m
[32m- format_number / FormatNumber[0m
[32m- find in set[0m
[32m- ParseUrl[0m
[32m- Sentences[0m
[32mEncoderErrorMessageSuite:[0m
[32m- primitive types in encoders using Kryo serialization[0m
[32m- primitive types in encoders using Java serialization[0m
[32m- nice error message for missing encoder[0m
[32mOrderingSuite:[0m
[32m- compare two arrays: a = List(), b = List()[0m
[32m- compare two arrays: a = List(1), b = List(1)[0m
[32m- compare two arrays: a = List(1, 2), b = List(1, 2)[0m
[32m- compare two arrays: a = List(1, 2, 2), b = List(1, 2, 3)[0m
[32m- compare two arrays: a = List(), b = List(1)[0m
[32m- compare two arrays: a = List(1, 2, 3), b = List(1, 2, 3, 4)[0m
[32m- compare two arrays: a = List(1, 2, 3), b = List(1, 2, 3, 2)[0m
[32m- compare two arrays: a = List(1, 2, 3), b = List(1, 2, 2, 2)[0m
[32m- compare two arrays: a = List(1, 2, 3), b = List(1, 2, 3, null)[0m
[32m- compare two arrays: a = List(), b = List(null)[0m
[32m- compare two arrays: a = List(null), b = List(null)[0m
[32m- compare two arrays: a = List(null, null), b = List(null, null)[0m
[32m- compare two arrays: a = List(null), b = List(null, null)[0m
[32m- compare two arrays: a = List(null), b = List(1)[0m
[32m- compare two arrays: a = List(null), b = List(null, 1)[0m
[32m- compare two arrays: a = List(null, 1), b = List(1, 1)[0m
[32m- compare two arrays: a = List(1, null, 1), b = List(1, null, 1)[0m
[32m- compare two arrays: a = List(1, null, 1), b = List(1, null, 2)[0m
[32m- GenerateOrdering with StringType[0m
[32m- GenerateOrdering with NullType[0m
[32m- GenerateOrdering with ArrayType(IntegerType,true)[0m
[32m- GenerateOrdering with LongType[0m
[32m- GenerateOrdering with IntegerType[0m
[32m- GenerateOrdering with DecimalType(20,5)[0m
[32m- GenerateOrdering with TimestampType[0m
[32m- GenerateOrdering with DoubleType[0m
[32m- GenerateOrdering with DateType[0m
[32m- GenerateOrdering with StructType(StructField(f1,FloatType,true), StructField(f2,ArrayType(BooleanType,true),true))[0m
[32m- GenerateOrdering with ArrayType(StructType(StructField(f1,FloatType,true), StructField(f2,ArrayType(BooleanType,true),true)),true)[0m
[32m- GenerateOrdering with DecimalType(10,0)[0m
[32m- GenerateOrdering with BinaryType[0m
[32m- GenerateOrdering with BooleanType[0m
[32m- GenerateOrdering with DecimalType(38,18)[0m
[32m- GenerateOrdering with ByteType[0m
[32m- GenerateOrdering with FloatType[0m
[32m- GenerateOrdering with ShortType[0m
[32m- SPARK-16845: GeneratedClass$SpecificOrdering grows beyond 64 KB[0m
[32m- SPARK-21344: BinaryType comparison does signed byte array comparison[0m
[32m- SPARK-22591: GenerateOrdering shouldn't change ctx.INPUT_ROW[0m
[32mGenerateUnsafeProjectionSuite:[0m
[32m- Test unsafe projection string access pattern[0m
[32m- Test unsafe projection for array/map/struct[0m
[32mGeneratorExpressionSuite:[0m
[32m- explode[0m
[32m- posexplode[0m
[32m- inline[0m
[32m- stack[0m
[32mResolveNaturalJoinSuite:[0m
[32m- natural/using inner join[0m
[32m- natural/using left join[0m
[32m- natural/using right join[0m
[32m- natural/using full outer join[0m
[32m- natural/using inner join with no nullability[0m
[32m- natural/using left join with no nullability[0m
[32m- natural/using right join with no nullability[0m
[32m- natural/using full outer join with no nullability[0m
[32m- using unresolved attribute[0m
[32m- using join with a case sensitive analyzer[0m
[32m- using join on nested fields[0m
[32m- using join with a case insensitive analyzer[0m
[32mAnalysisErrorSuite:[0m
[32m- scalar subquery with 2 columns[0m
[32m- scalar subquery with no column[0m
[32m- single invalid type, single arg[0m
[32m- single invalid type, second arg[0m
[32m- multiple invalid type[0m
[32m- invalid window function[0m
[32m- distinct aggregate function in window[0m
[32m- distinct function[0m
[32m- distinct window function[0m
[32m- nested aggregate functions[0m
[32m- offset window function[0m
[32m- too many generators[0m
[32m- unresolved attributes[0m
[32m- unresolved attributes with a generated name[0m
[32m- unresolved star expansion in max[0m
[32m- bad casts[0m
[32m- sorting by unsupported column types[0m
[32m- sorting by attributes are not from grouping expressions[0m
[32m- non-boolean filters[0m
[32m- non-boolean join conditions[0m
[32m- missing group by[0m
[32m- ambiguous field[0m
[32m- ambiguous field due to case insensitivity[0m
[32m- missing field[0m
[32m- catch all unresolved plan[0m
[32m- union with unequal number of columns[0m
[32m- intersect with unequal number of columns[0m
[32m- except with unequal number of columns[0m
[32m- union with incompatible column types[0m
[32m- union with a incompatible column type and compatible column types[0m
[32m- intersect with incompatible column types[0m
[32m- intersect with a incompatible column type and compatible column types[0m
[32m- except with incompatible column types[0m
[32m- except with a incompatible column type and compatible column types[0m
[32m- SPARK-9955: correct error message for aggregate[0m
[32m- slide duration greater than window in time window[0m
[32m- start time greater than slide duration in time window[0m
[32m- start time equal to slide duration in time window[0m
[32m- SPARK-21590: absolute value of start time greater than slide duration in time window[0m
[32m- SPARK-21590: absolute value of start time equal to slide duration in time window[0m
[32m- negative window duration in time window[0m
[32m- zero window duration in time window[0m
[32m- negative slide duration in time window[0m
[32m- zero slide duration in time window[0m
[32m- generator nested in expressions[0m
[32m- generator appears in operator which is not Project[0m
[32m- an evaluated limit class must not be null[0m
[32m- num_rows in limit clause must be equal to or greater than 0[0m
[32m- more than one generators in SELECT[0m
[32m- SPARK-6452 regression test[0m
[32m- error test for self-join[0m
[32m- check grouping expression data types[0m
[32m- we should fail analysis when we find nested aggregate functions[0m
[32m- Join can work on binary types but can't work on map types[0m
[32m- PredicateSubQuery is used outside of a filter[0m
[32m- PredicateSubQuery is used is a nested condition[0m
[32m- PredicateSubQuery correlated predicate is nested in an illegal plan[0m
[32mTableIdentifierParserSuite:[0m
[32m- table identifier[0m
[32m- quoted identifiers[0m
[32m- table identifier - strict keywords[0m
[32m- table identifier - non reserved keywords[0m
[32m- SPARK-17364 table identifier - contains number[0m
[32m- SPARK-17832 table identifier - contains backtick[0m
[32mJoinEstimationSuite:[0m
[32m- equi-height histograms: a bin is contained by another one[0m
[32m- equi-height histograms: a bin has only one value after trimming[0m
[32m- equi-height histograms: skew distribution (some bins have only one value)[0m
[32m- equi-height histograms: skew distribution (histograms have different skewed values[0m
[32m- equi-height histograms: skew distribution (both histograms have the same skewed value[0m
[32m- cross join[0m
[32m- disjoint inner join[0m
[32m- disjoint left outer join[0m
[32m- disjoint right outer join[0m
[32m- disjoint full outer join[0m
[32m- inner join[0m
[32m- inner join with multiple equi-join keys[0m
[32m- left outer join[0m
[32m- right outer join[0m
[32m- full outer join[0m
[32m- left semi/anti join[0m
[32m- test join keys of different types[0m
[32m- join with null column[0m
[32mBooleanSimplificationSuite:[0m
[32m- a && a => a[0m
[32m- a || a => a[0m
[32m- (a && b && c && ...) || (a && b && d && ...) || (a && b && e && ...) ...[0m
[32m- (a || b || c || ...) && (a || b || d || ...) && (a || b || e || ...) ...[0m
[32m- e && (!e || f) - not nullable[0m
[32m- e && (!e || f) - nullable[0m
[32m- a < 1 && (!(a < 1) || f) - not nullable[0m
[32m- a < 1 && ((a >= 1) || f) - not nullable[0m
[32m- DeMorgan's law[0m
[32m- (a && b) || (a && c) => a && (b || c) when case insensitive[0m
[32m- (a || b) && (a || c) => a || (b && c) when case insensitive[0m
[32m- Complementation Laws[0m
[32m- Complementation Laws - null handling[0m
[32m- Complementation Laws - negative case[0m
[32m- filter reduction - positive cases[0m
[32mLikeSimplificationSuite:[0m
[32m- simplify Like into StartsWith[0m
[32m- simplify Like into EndsWith[0m
[32m- simplify Like into startsWith and EndsWith[0m
[32m- simplify Like into Contains[0m
[32m- simplify Like into EqualTo[0m
[32m- null pattern[0m
[32mRemoveRedundantAliasAndProjectSuite:[0m
[32m- all expressions in project list are aliased child output[0m
[32m- all expressions in project list are aliased child output but with different order[0m
[32m- some expressions in project list are aliased child output[0m
[32m- some expressions in project list are aliased child output but with different order[0m
[32m- some expressions in project list are not Alias or Attribute[0m
[32m- some expressions in project list are aliased child output but with metadata[0m
[32m- retain deduplicating alias in self-join[0m
[32m- alias removal should not break after push project through union[0m
[32m- remove redundant alias from aggregate[0m
[32m- remove redundant alias from window[0m
[32m- do not remove output attributes from a subquery[0m
[32mTypeUtilsSuite:[0m
[32m- checkForSameTypeInputExpr[0m
[32mScalaUDFSuite:[0m
[32m- basic[0m
[32m- better error message for NPE[0m
[32m- SPARK-22695: ScalaUDF should not use global variables[0m
[32mStreamingJoinHelperSuite:[0m
[32m- extract watermark from time condition[0m
[32mUnsafeRowWriterSuite:[0m
[32m- SPARK-25538: zero-out all bits for decimals[0m
[32mResolveLambdaVariablesSuite:[0m
[32m- resolution - no op[0m
[32m- resolution - simple[0m
[32m- resolution - nested[0m
[32m- resolution - hidden[0m
[32m- fail - name collisions[0m
[32m- fail - lambda arguments[0m
[32mOptimizeInSuite:[0m
[32m- OptimizedIn test: Remove deterministic repetitions[0m
[32m- OptimizedIn test: In clause not optimized to InSet when less than 10 items[0m
[32m- OptimizedIn test: In clause optimized to InSet when more than 10 items[0m
[32m- OptimizedIn test: In clause not optimized in case filter has attributes[0m
[32m- OptimizedIn test: NULL IN (expr1, ..., exprN) gets transformed to Filter(null)[0m
[32m- OptimizedIn test: NULL IN (subquery) gets transformed to Filter(null)[0m
[32m- OptimizedIn test: Inset optimization disabled as list expression contains attribute)[0m
[32m- OptimizedIn test: Inset optimization disabled as list expression contains attribute - select)[0m
[32m- OptimizedIn test: Setting the threshold for turning Set into InSet.[0m
[32m- OptimizedIn test: one element in list gets transformed to EqualTo.[0m
[32m- OptimizedIn test: In empty list gets transformed to FalseLiteral when value is not nullable[0m
[32m- OptimizedIn test: In empty list gets transformed to `If` expression when value is nullable[0m
[32mSimplifyStringCaseConversionSuite:[0m
[32m- simplify UPPER(UPPER(str))[0m
[32m- simplify UPPER(LOWER(str))[0m
[32m- simplify LOWER(UPPER(str))[0m
[32m- simplify LOWER(LOWER(str))[0m
[32mSimplifyCastsSuite:[0m
[32m- non-nullable element array to nullable element array cast[0m
[32m- nullable element to non-nullable element array cast[0m
[32m- non-nullable value map to nullable value map cast[0m
[32m- nullable value map to non-nullable value map cast[0m
[32mEliminateSerializationSuite:[0m
[32m- back to back serialization[0m
[32m- back to back serialization with object change[0m
[32m- back to back serialization in AppendColumns[0m
[32m- back to back serialization in AppendColumns with object change[0m
[32mComplexDataSuite:[0m
[32m- inequality tests for MapData[0m
[32m- GenericInternalRow.copy return a new instance that is independent from the old one[0m
[32m- SpecificMutableRow.copy return a new instance that is independent from the old one[0m
[32m- GenericArrayData.copy return a new instance that is independent from the old one[0m
[32m- copy on nested complex type[0m
[32m- SPARK-24659: GenericArrayData.equals should respect element type differences[0m
[32mRandomDataGeneratorSuite:[0m
[32m- StringType (nullable=true)[0m
[32m- StringType (nullable=false)[0m
[32m- LongType (nullable=true)[0m
[32m- LongType (nullable=false)[0m
[32m- IntegerType (nullable=true)[0m
[32m- IntegerType (nullable=false)[0m
[32m- TimestampType (nullable=true)[0m
[32m- TimestampType (nullable=false)[0m
[32m- DoubleType (nullable=true)[0m
[32m- DoubleType (nullable=false)[0m
[32m- DateType (nullable=true)[0m
[32m- DateType (nullable=false)[0m
[32m- BinaryType (nullable=true)[0m
[32m- BinaryType (nullable=false)[0m
[32m- BooleanType (nullable=true)[0m
[32m- BooleanType (nullable=false)[0m
[32m- ByteType (nullable=true)[0m
[32m- ByteType (nullable=false)[0m
[32m- FloatType (nullable=true)[0m
[32m- FloatType (nullable=false)[0m
[32m- ShortType (nullable=true)[0m
[32m- ShortType (nullable=false)[0m
[32m- ArrayType(FloatType,true)[0m
[32m- ArrayType(LongType,true)[0m
[32m- ArrayType(IntegerType,true)[0m
[32m- ArrayType(TimestampType,true)[0m
[32m- ArrayType(ByteType,true)[0m
[32m- ArrayType(ShortType,true)[0m
[32m- ArrayType(DecimalType(20,5),true)[0m
[32m- ArrayType(DateType,true)[0m
[32m- ArrayType(BooleanType,true)[0m
[32m- ArrayType(BinaryType,true)[0m
[32m- ArrayType(DecimalType(10,0),true)[0m
[32m- ArrayType(StringType,true)[0m
[32m- ArrayType(DoubleType,true)[0m
[32m- ArrayType(DecimalType(38,18),true)[0m
[32m- MapType(StringType,StringType,true)[0m
[32m- MapType(StringType,LongType,true)[0m
[32m- MapType(StringType,IntegerType,true)[0m
[32m- MapType(StringType,DecimalType(20,5),true)[0m
[32m- MapType(StringType,TimestampType,true)[0m
[32m- MapType(StringType,DoubleType,true)[0m
[32m- MapType(StringType,DateType,true)[0m
[32m- MapType(StringType,DecimalType(10,0),true)[0m
[32m- MapType(StringType,BinaryType,true)[0m
[32m- MapType(StringType,BooleanType,true)[0m
[32m- MapType(StringType,DecimalType(38,18),true)[0m
[32m- MapType(StringType,ByteType,true)[0m
[32m- MapType(StringType,FloatType,true)[0m
[32m- MapType(StringType,ShortType,true)[0m
[32m- MapType(LongType,StringType,true)[0m
[32m- MapType(LongType,LongType,true)[0m
[32m- MapType(LongType,IntegerType,true)[0m
[32m- MapType(LongType,DecimalType(20,5),true)[0m
[32m- MapType(LongType,TimestampType,true)[0m
[32m- MapType(LongType,DoubleType,true)[0m
[32m- MapType(LongType,DateType,true)[0m
[32m- MapType(LongType,DecimalType(10,0),true)[0m
[32m- MapType(LongType,BinaryType,true)[0m
[32m- MapType(LongType,BooleanType,true)[0m
[32m- MapType(LongType,DecimalType(38,18),true)[0m
[32m- MapType(LongType,ByteType,true)[0m
[32m- MapType(LongType,FloatType,true)[0m
[32m- MapType(LongType,ShortType,true)[0m
[32m- MapType(IntegerType,StringType,true)[0m
[32m- MapType(IntegerType,LongType,true)[0m
[32m- MapType(IntegerType,IntegerType,true)[0m
[32m- MapType(IntegerType,DecimalType(20,5),true)[0m
[32m- MapType(IntegerType,TimestampType,true)[0m
[32m- MapType(IntegerType,DoubleType,true)[0m
[32m- MapType(IntegerType,DateType,true)[0m
[32m- MapType(IntegerType,DecimalType(10,0),true)[0m
[32m- MapType(IntegerType,BinaryType,true)[0m
[32m- MapType(IntegerType,BooleanType,true)[0m
[32m- MapType(IntegerType,DecimalType(38,18),true)[0m
[32m- MapType(IntegerType,ByteType,true)[0m
[32m- MapType(IntegerType,FloatType,true)[0m
[32m- MapType(IntegerType,ShortType,true)[0m
[32m- MapType(TimestampType,StringType,true)[0m
[32m- MapType(TimestampType,LongType,true)[0m
[32m- MapType(TimestampType,IntegerType,true)[0m
[32m- MapType(TimestampType,DecimalType(20,5),true)[0m
[32m- MapType(TimestampType,TimestampType,true)[0m
[32m- MapType(TimestampType,DoubleType,true)[0m
[32m- MapType(TimestampType,DateType,true)[0m
[32m- MapType(TimestampType,DecimalType(10,0),true)[0m
[32m- MapType(TimestampType,BinaryType,true)[0m
[32m- MapType(TimestampType,BooleanType,true)[0m
[32m- MapType(TimestampType,DecimalType(38,18),true)[0m
[32m- MapType(TimestampType,ByteType,true)[0m
[32m- MapType(TimestampType,FloatType,true)[0m
[32m- MapType(TimestampType,ShortType,true)[0m
[32m- MapType(DoubleType,StringType,true)[0m
[32m- MapType(DoubleType,LongType,true)[0m
[32m- MapType(DoubleType,IntegerType,true)[0m
[32m- MapType(DoubleType,DecimalType(20,5),true)[0m
[32m- MapType(DoubleType,TimestampType,true)[0m
[32m- MapType(DoubleType,DoubleType,true)[0m
[32m- MapType(DoubleType,DateType,true)[0m
[32m- MapType(DoubleType,DecimalType(10,0),true)[0m
[32m- MapType(DoubleType,BinaryType,true)[0m
[32m- MapType(DoubleType,BooleanType,true)[0m
[32m- MapType(DoubleType,DecimalType(38,18),true)[0m
[32m- MapType(DoubleType,ByteType,true)[0m
[32m- MapType(DoubleType,FloatType,true)[0m
[32m- MapType(DoubleType,ShortType,true)[0m
[32m- MapType(DateType,StringType,true)[0m
[32m- MapType(DateType,LongType,true)[0m
[32m- MapType(DateType,IntegerType,true)[0m
[32m- MapType(DateType,DecimalType(20,5),true)[0m
[32m- MapType(DateType,TimestampType,true)[0m
[32m- MapType(DateType,DoubleType,true)[0m
[32m- MapType(DateType,DateType,true)[0m
[32m- MapType(DateType,DecimalType(10,0),true)[0m
[32m- MapType(DateType,BinaryType,true)[0m
[32m- MapType(DateType,BooleanType,true)[0m
[32m- MapType(DateType,DecimalType(38,18),true)[0m
[32m- MapType(DateType,ByteType,true)[0m
[32m- MapType(DateType,FloatType,true)[0m
[32m- MapType(DateType,ShortType,true)[0m
[32m- MapType(BinaryType,StringType,true)[0m
[32m- MapType(BinaryType,LongType,true)[0m
[32m- MapType(BinaryType,IntegerType,true)[0m
[32m- MapType(BinaryType,DecimalType(20,5),true)[0m
[32m- MapType(BinaryType,TimestampType,true)[0m
[32m- MapType(BinaryType,DoubleType,true)[0m
[32m- MapType(BinaryType,DateType,true)[0m
[32m- MapType(BinaryType,DecimalType(10,0),true)[0m
[32m- MapType(BinaryType,BinaryType,true)[0m
[32m- MapType(BinaryType,BooleanType,true)[0m
[32m- MapType(BinaryType,DecimalType(38,18),true)[0m
[32m- MapType(BinaryType,ByteType,true)[0m
[32m- MapType(BinaryType,FloatType,true)[0m
[32m- MapType(BinaryType,ShortType,true)[0m
[32m- MapType(BooleanType,StringType,true)[0m
[32m- MapType(BooleanType,LongType,true)[0m
[32m- MapType(BooleanType,IntegerType,true)[0m
[32m- MapType(BooleanType,DecimalType(20,5),true)[0m
[32m- MapType(BooleanType,TimestampType,true)[0m
[32m- MapType(BooleanType,DoubleType,true)[0m
[32m- MapType(BooleanType,DateType,true)[0m
[32m- MapType(BooleanType,DecimalType(10,0),true)[0m
[32m- MapType(BooleanType,BinaryType,true)[0m
[32m- MapType(BooleanType,BooleanType,true)[0m
[32m- MapType(BooleanType,DecimalType(38,18),true)[0m
[32m- MapType(BooleanType,ByteType,true)[0m
[32m- MapType(BooleanType,FloatType,true)[0m
[32m- MapType(BooleanType,ShortType,true)[0m
[32m- MapType(ByteType,StringType,true)[0m
[32m- MapType(ByteType,LongType,true)[0m
[32m- MapType(ByteType,IntegerType,true)[0m
[32m- MapType(ByteType,DecimalType(20,5),true)[0m
[32m- MapType(ByteType,TimestampType,true)[0m
[32m- MapType(ByteType,DoubleType,true)[0m
[32m- MapType(ByteType,DateType,true)[0m
[32m- MapType(ByteType,DecimalType(10,0),true)[0m
[32m- MapType(ByteType,BinaryType,true)[0m
[32m- MapType(ByteType,BooleanType,true)[0m
[32m- MapType(ByteType,DecimalType(38,18),true)[0m
[32m- MapType(ByteType,ByteType,true)[0m
[32m- MapType(ByteType,FloatType,true)[0m
[32m- MapType(ByteType,ShortType,true)[0m
[32m- MapType(FloatType,StringType,true)[0m
[32m- MapType(FloatType,LongType,true)[0m
[32m- MapType(FloatType,IntegerType,true)[0m
[32m- MapType(FloatType,DecimalType(20,5),true)[0m
[32m- MapType(FloatType,TimestampType,true)[0m
[32m- MapType(FloatType,DoubleType,true)[0m
[32m- MapType(FloatType,DateType,true)[0m
[32m- MapType(FloatType,DecimalType(10,0),true)[0m
[32m- MapType(FloatType,BinaryType,true)[0m
[32m- MapType(FloatType,BooleanType,true)[0m
[32m- MapType(FloatType,DecimalType(38,18),true)[0m
[32m- MapType(FloatType,ByteType,true)[0m
[32m- MapType(FloatType,FloatType,true)[0m
[32m- MapType(FloatType,ShortType,true)[0m
[32m- MapType(ShortType,StringType,true)[0m
[32m- MapType(ShortType,LongType,true)[0m
[32m- MapType(ShortType,IntegerType,true)[0m
[32m- MapType(ShortType,DecimalType(20,5),true)[0m
[32m- MapType(ShortType,TimestampType,true)[0m
[32m- MapType(ShortType,DoubleType,true)[0m
[32m- MapType(ShortType,DateType,true)[0m
[32m- MapType(ShortType,DecimalType(10,0),true)[0m
[32m- MapType(ShortType,BinaryType,true)[0m
[32m- MapType(ShortType,BooleanType,true)[0m
[32m- MapType(ShortType,DecimalType(38,18),true)[0m
[32m- MapType(ShortType,ByteType,true)[0m
[32m- MapType(ShortType,FloatType,true)[0m
[32m- MapType(ShortType,ShortType,true)[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,ShortType,true))[0m
[32m- check size of generated map[0m
[32m- Use Float.NaN for all NaN values[0m
[32m- Use Double.NaN for all NaN values[0m
[32mDecimalPrecisionSuite:[0m
[32m- basic operations[0m
[32m- Comparison operations[0m
[32m- decimal precision for union[0m
[32m- bringing in primitive types[0m
[32m- maximum decimals[0m
[32m- DecimalType.isWiderThan[0m
[32m- strength reduction for integer/decimal comparisons - basic test[0m
[32m- strength reduction for integer/decimal comparisons - overflow test[0m
[32m- SPARK-24468: operations on decimals with negative scale[0m
[32mUnsafeArraySuite:[0m
[32m- read array[0m
[32m- from primitive array[0m
[32m- to primitive array[0m
[32m- unsafe java serialization[0m
[32m- unsafe Kryo serialization[0m
[32mTimestampFormatterSuite:[0m
[32m- parsing timestamps using time zones[0m
[32m- format timestamps using time zones[0m
[32m- roundtrip micros -> timestamp -> micros using timezones[0m
[32m- roundtrip timestamp -> micros -> timestamp using timezones[0m
[32m-  case insensitive parsing of am and pm[0m
[32mExternalCatalogEventSuite:[0m
[32m- database[0m
[32m- table[0m
[32m- function[0m
[32mExpressionTypeCheckingSuite:[0m
[32m- check types for unary arithmetic[0m
[32m- check types for binary arithmetic[0m
[32m- check types for predicates[0m
[32m- check types for aggregates[0m
[32m- check types for others[0m
[32m- check types for CreateNamedStruct[0m
[32m- check types for CreateMap[0m
[32m- check types for ROUND/BROUND[0m
[32m- check types for Greatest/Least[0m
[32mInternalOutputModesSuite:[0m
[32m- supported strings[0m
[32m- unsupported strings[0m
[32mEliminateSubqueryAliasesSuite:[0m
[32m- eliminate top level subquery[0m
[32m- eliminate mid-tree subquery[0m
[32m- eliminate multiple subqueries[0m
[32mJoinTypesTest:[0m
[32m- construct an Inner type[0m
[32m- construct a FullOuter type[0m
[32m- construct a LeftOuter type[0m
[32m- construct a RightOuter type[0m
[32m- construct a LeftSemi type[0m
[32m- construct a LeftAnti type[0m
[32m- construct a Cross type[0m
[32mDateExpressionsSuite:[0m
[32m- datetime function current_date[0m
[32m- datetime function current_timestamp[0m
[32m- DayOfYear[0m
[32m- Year[0m
[32m- Quarter[0m
[32m- Month[0m
[32m- Day / DayOfMonth[0m
[32m- Seconds[0m
[32m- DayOfWeek[0m
[32m- WeekDay[0m
[32m- WeekOfYear[0m
[32m- DateFormat[0m
[32m- Hour[0m
[32m- Minute[0m
[32m- date_add[0m
[32m- date_sub[0m
[32m- time_add[0m
[32m- time_sub[0m
[32m- add_months[0m
[32m- months_between[0m
[32m- last_day[0m
[32m- next_day[0m
[32m- TruncDate[0m
[32m- TruncTimestamp[0m
[32m- from_unixtime[0m
[32m- unix_timestamp[0m
[32m- to_unix_timestamp[0m
[32m- datediff[0m
[32m- to_utc_timestamp[0m
[32m- from_utc_timestamp[0m
[32mDecimalAggregatesSuite:[0m
[32m- Decimal Sum Aggregation: Optimized[0m
[32m- Decimal Sum Aggregation: Not Optimized[0m
[32m- Decimal Average Aggregation: Optimized[0m
[32m- Decimal Average Aggregation: Not Optimized[0m
[32m- Decimal Sum Aggregation over Window: Optimized[0m
[32m- Decimal Sum Aggregation over Window: Not Optimized[0m
[32m- Decimal Average Aggregation over Window: Optimized[0m
[32m- Decimal Average Aggregation over Window: Not Optimized[0m
[32mDSLHintSuite:[0m
[32m- various hint parameters[0m
[32mPullOutPythonUDFInJoinConditionSuite:[0m
[32m- inner join condition with python udf[0m
[32m- unevaluable python udf and common condition[0m
[32m- unevaluable python udf or common condition[0m
[32m- pull out whole complex condition with multiple unevaluable python udf[0m
[32m- partial pull out complex condition with multiple unevaluable python udf[0m
[32m- pull out unevaluable python udf when it's mixed with evaluable one[0m
[32m- throw an exception for not supported join types[0m
[32mExpressionEncoderSuite:[0m
[32m- encode/decode for primitive boolean: false (codegen path)[0m
[32m- encode/decode for primitive boolean: false (interpreted path)[0m
[32m- encode/decode for primitive byte: -3 (codegen path)[0m
[32m- encode/decode for primitive byte: -3 (interpreted path)[0m
[32m- encode/decode for primitive short: -3 (codegen path)[0m
[32m- encode/decode for primitive short: -3 (interpreted path)[0m
[32m- encode/decode for primitive int: -3 (codegen path)[0m
[32m- encode/decode for primitive int: -3 (interpreted path)[0m
[32m- encode/decode for primitive long: -3 (codegen path)[0m
[32m- encode/decode for primitive long: -3 (interpreted path)[0m
[32m- encode/decode for primitive float: -3.7 (codegen path)[0m
[32m- encode/decode for primitive float: -3.7 (interpreted path)[0m
[32m- encode/decode for primitive double: -3.7 (codegen path)[0m
[32m- encode/decode for primitive double: -3.7 (interpreted path)[0m
[32m- encode/decode for boxed boolean: false (codegen path)[0m
[32m- encode/decode for boxed boolean: false (interpreted path)[0m
[32m- encode/decode for boxed byte: -3 (codegen path)[0m
[32m- encode/decode for boxed byte: -3 (interpreted path)[0m
[32m- encode/decode for boxed short: -3 (codegen path)[0m
[32m- encode/decode for boxed short: -3 (interpreted path)[0m
[32m- encode/decode for boxed int: -3 (codegen path)[0m
[32m- encode/decode for boxed int: -3 (interpreted path)[0m
[32m- encode/decode for boxed long: -3 (codegen path)[0m
[32m- encode/decode for boxed long: -3 (interpreted path)[0m
[32m- encode/decode for boxed float: -3.7 (codegen path)[0m
[32m- encode/decode for boxed float: -3.7 (interpreted path)[0m
[32m- encode/decode for boxed double: -3.7 (codegen path)[0m
[32m- encode/decode for boxed double: -3.7 (interpreted path)[0m
[32m- encode/decode for scala decimal: 32131413.211321313 (codegen path)[0m
[32m- encode/decode for scala decimal: 32131413.211321313 (interpreted path)[0m
[32m- encode/decode for java decimal: 231341.23123 (codegen path)[0m
[32m- encode/decode for java decimal: 231341.23123 (interpreted path)[0m
[32m- encode/decode for scala biginteger: 23134123123 (codegen path)[0m
[32m- encode/decode for scala biginteger: 23134123123 (interpreted path)[0m
[32m- encode/decode for java BigInteger: 23134123123 (codegen path)[0m
[32m- encode/decode for java BigInteger: 23134123123 (interpreted path)[0m
[32m- encode/decode for catalyst decimal: 32131413.211321313 (codegen path)[0m
[32m- encode/decode for catalyst decimal: 32131413.211321313 (interpreted path)[0m
[32m- encode/decode for string: hello (codegen path)[0m
[32m- encode/decode for string: hello (interpreted path)[0m
[32m- encode/decode for date: 2012-12-23 (codegen path)[0m
[32m- encode/decode for date: 2012-12-23 (interpreted path)[0m
[32m- encode/decode for timestamp: 2016-01-29 10:00:00.0 (codegen path)[0m
[32m- encode/decode for timestamp: 2016-01-29 10:00:00.0 (interpreted path)[0m
[32m- encode/decode for array of timestamp: [Ljava.sql.Timestamp;@463df207 (codegen path)[0m
[32m- encode/decode for array of timestamp: [Ljava.sql.Timestamp;@463df207 (interpreted path)[0m
[32m- encode/decode for binary: [B@52b9373c (codegen path)[0m
[32m- encode/decode for binary: [B@52b9373c (interpreted path)[0m
[32m- encode/decode for seq of int: List(31, -123, 4) (codegen path)[0m
[32m- encode/decode for seq of int: List(31, -123, 4) (interpreted path)[0m
[32m- encode/decode for seq of string: List(abc, xyz) (codegen path)[0m
[32m- encode/decode for seq of string: List(abc, xyz) (interpreted path)[0m
[32m- encode/decode for seq of string with null: List(abc, null, xyz) (codegen path)[0m
[32m- encode/decode for seq of string with null: List(abc, null, xyz) (interpreted path)[0m
[32m- encode/decode for empty seq of int: List() (codegen path)[0m
[32m- encode/decode for empty seq of int: List() (interpreted path)[0m
[32m- encode/decode for empty seq of string: List() (codegen path)[0m
[32m- encode/decode for empty seq of string: List() (interpreted path)[0m
[32m- encode/decode for seq of seq of int: List(List(31, -123), null, List(4, 67)) (codegen path)[0m
[32m- encode/decode for seq of seq of int: List(List(31, -123), null, List(4, 67)) (interpreted path)[0m
[32m- encode/decode for seq of seq of string: List(List(abc, xyz), List(null), null, List(1, null, 2)) (codegen path)[0m
[32m- encode/decode for seq of seq of string: List(List(abc, xyz), List(null), null, List(1, null, 2)) (interpreted path)[0m
[32m- encode/decode for array of int: [I@15955c13 (codegen path)[0m
[32m- encode/decode for array of int: [I@15955c13 (interpreted path)[0m
[32m- encode/decode for array of string: [Ljava.lang.String;@c2f2e6d2 (codegen path)[0m
[32m- encode/decode for array of string: [Ljava.lang.String;@c2f2e6d2 (interpreted path)[0m
[32m- encode/decode for array of string with null: [Ljava.lang.String;@e7945f09 (codegen path)[0m
[32m- encode/decode for array of string with null: [Ljava.lang.String;@e7945f09 (interpreted path)[0m
[32m- encode/decode for empty array of int: [I@8b1827b8 (codegen path)[0m
[32m- encode/decode for empty array of int: [I@8b1827b8 (interpreted path)[0m
[32m- encode/decode for empty array of string: [Ljava.lang.String;@fb5af117 (codegen path)[0m
[32m- encode/decode for empty array of string: [Ljava.lang.String;@fb5af117 (interpreted path)[0m
[32m- encode/decode for array of array of int: [[I@8c5660ba (codegen path)[0m
[32m- encode/decode for array of array of int: [[I@8c5660ba (interpreted path)[0m
[32m- encode/decode for array of array of string: [[Ljava.lang.String;@d17d145a (codegen path)[0m
[32m- encode/decode for array of array of string: [[Ljava.lang.String;@d17d145a (interpreted path)[0m
[32m- encode/decode for map: Map(1 -> a, 2 -> b) (codegen path)[0m
[32m- encode/decode for map: Map(1 -> a, 2 -> b) (interpreted path)[0m
[32m- encode/decode for map with null: Map(1 -> a, 2 -> null) (codegen path)[0m
[32m- encode/decode for map with null: Map(1 -> a, 2 -> null) (interpreted path)[0m
[32m- encode/decode for map of map: Map(1 -> Map(a -> 1), 2 -> Map(b -> 2)) (codegen path)[0m
[32m- encode/decode for map of map: Map(1 -> Map(a -> 1), 2 -> Map(b -> 2)) (interpreted path)[0m
[32m- encode/decode for null seq in tuple: (null) (codegen path)[0m
[32m- encode/decode for null seq in tuple: (null) (interpreted path)[0m
[32m- encode/decode for null map in tuple: (null) (codegen path)[0m
[32m- encode/decode for null map in tuple: (null) (interpreted path)[0m
[32m- encode/decode for list of int: List(1, 2) (codegen path)[0m
[32m- encode/decode for list of int: List(1, 2) (interpreted path)[0m
[32m- encode/decode for list with String and null: List(a, null) (codegen path)[0m
[32m- encode/decode for list with String and null: List(a, null) (interpreted path)[0m
[32m- encode/decode for udt with case class: UDTCaseClass(http://spark.apache.org/) (codegen path)[0m
[32m- encode/decode for udt with case class: UDTCaseClass(http://spark.apache.org/) (interpreted path)[0m
[32m- encode/decode for kryo string: hello (codegen path)[0m
[32m- encode/decode for kryo string: hello (interpreted path)[0m
[32m- encode/decode for kryo object: org.apache.spark.sql.catalyst.encoders.KryoSerializable@f (codegen path)[0m
[32m- encode/decode for kryo object: org.apache.spark.sql.catalyst.encoders.KryoSerializable@f (interpreted path)[0m
[32m- encode/decode for java string: hello (codegen path)[0m
[32m- encode/decode for java string: hello (interpreted path)[0m
[32m- encode/decode for java object: org.apache.spark.sql.catalyst.encoders.JavaSerializable@f (codegen path)[0m
[32m- encode/decode for java object: org.apache.spark.sql.catalyst.encoders.JavaSerializable@f (interpreted path)[0m
[32m- encode/decode for InnerClass: InnerClass(1) (codegen path)[0m
[32m- encode/decode for InnerClass: InnerClass(1) (interpreted path)[0m
[32m- encode/decode for array of inner class: [Lorg.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$InnerClass;@cdb4e693 (codegen path)[0m
[32m- encode/decode for array of inner class: [Lorg.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$InnerClass;@cdb4e693 (interpreted path)[0m
[32m- encode/decode for array of optional inner class: [Lscala.Option;@3ace2e99 (codegen path)[0m
[32m- encode/decode for array of optional inner class: [Lscala.Option;@3ace2e99 (interpreted path)[0m
[32m- encode/decode for PrimitiveData: PrimitiveData(1,1,1.0,1.0,1,1,true) (codegen path)[0m
[32m- encode/decode for PrimitiveData: PrimitiveData(1,1,1.0,1.0,1,1,true) (interpreted path)[0m
[32m- encode/decode for OptionalData: OptionalData(Some(2),Some(2),Some(2.0),Some(2.0),Some(2),Some(2),Some(true),Some(PrimitiveData(1,1,1.0,1.0,1,1,true))) (codegen path)[0m
[32m- encode/decode for OptionalData: OptionalData(Some(2),Some(2),Some(2.0),Some(2.0),Some(2),Some(2),Some(true),Some(PrimitiveData(1,1,1.0,1.0,1,1,true))) (interpreted path)[0m
[32m- encode/decode for OptionalData: OptionalData(None,None,None,None,None,None,None,None) (codegen path)[0m
[32m- encode/decode for OptionalData: OptionalData(None,None,None,None,None,None,None,None) (interpreted path)[0m
[32m- encode/decode for Option in array: List(Some(1), None) (codegen path)[0m
[32m- encode/decode for Option in array: List(Some(1), None) (interpreted path)[0m
[32m- encode/decode for Option in map: Map(1 -> Some(10), 2 -> Some(20), 3 -> None) (codegen path)[0m
[32m- encode/decode for Option in map: Map(1 -> Some(10), 2 -> Some(20), 3 -> None) (interpreted path)[0m
[32m- encode/decode for BoxedData: BoxedData(1,1,1.0,1.0,1,1,true) (codegen path)[0m
[32m- encode/decode for BoxedData: BoxedData(1,1,1.0,1.0,1,1,true) (interpreted path)[0m
[32m- encode/decode for BoxedData: BoxedData(null,null,null,null,null,null,null) (codegen path)[0m
[32m- encode/decode for BoxedData: BoxedData(null,null,null,null,null,null,null) (interpreted path)[0m
[32m- encode/decode for RepeatedStruct: RepeatedStruct(List(PrimitiveData(1,1,1.0,1.0,1,1,true))) (codegen path)[0m
[32m- encode/decode for RepeatedStruct: RepeatedStruct(List(PrimitiveData(1,1,1.0,1.0,1,1,true))) (interpreted path)[0m
[32m- encode/decode for Tuple3: (1,test,PrimitiveData(1,1,1.0,1.0,1,1,true)) (codegen path)[0m
[32m- encode/decode for Tuple3: (1,test,PrimitiveData(1,1,1.0,1.0,1,1,true)) (interpreted path)[0m
[32m- encode/decode for RepeatedData: RepeatedData(List(1, 2),List(1, null, 2),Map(1 -> 2),Map(1 -> null),PrimitiveData(1,1,1.0,1.0,1,1,true)) (codegen path)[0m
[32m- encode/decode for RepeatedData: RepeatedData(List(1, 2),List(1, null, 2),Map(1 -> 2),Map(1 -> null),PrimitiveData(1,1,1.0,1.0,1,1,true)) (interpreted path)[0m
[32m- encode/decode for NestedArray: NestedArray([[I@fbb8413c) (codegen path)[0m
[32m- encode/decode for NestedArray: NestedArray([[I@fbb8413c) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(String, String)],List((a,b))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(String, String)],List((a,b))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Int, Int)],List((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Int, Int)],List((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Long, Long)],List((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Long, Long)],List((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Float, Float)],List((1.0,2.0))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Float, Float)],List((1.0,2.0))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Double, Double)],List((1.0,2.0))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Double, Double)],List((1.0,2.0))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Short, Short)],List((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Short, Short)],List((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Byte, Byte)],List((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Byte, Byte)],List((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Boolean, Boolean)],List((true,false))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Boolean, Boolean)],List((true,false))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(String, String)],ArrayBuffer((a,b))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(String, String)],ArrayBuffer((a,b))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Int, Int)],ArrayBuffer((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Int, Int)],ArrayBuffer((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Long, Long)],ArrayBuffer((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Long, Long)],ArrayBuffer((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Float, Float)],ArrayBuffer((1.0,2.0))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Float, Float)],ArrayBuffer((1.0,2.0))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Double, Double)],ArrayBuffer((1.0,2.0))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Double, Double)],ArrayBuffer((1.0,2.0))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Short, Short)],ArrayBuffer((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Short, Short)],ArrayBuffer((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Byte, Byte)],ArrayBuffer((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Byte, Byte)],ArrayBuffer((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Boolean, Boolean)],ArrayBuffer((true,false))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Boolean, Boolean)],ArrayBuffer((true,false))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[Seq[(Int, Int)]],List(List((1,2)))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[Seq[(Int, Int)]],List(List((1,2)))) (interpreted path)[0m
[32m- encode/decode for tuple with 2 flat encoders: (1,10) (codegen path)[0m
[32m- encode/decode for tuple with 2 flat encoders: (1,10) (interpreted path)[0m
[32m- encode/decode for tuple with 2 product encoders: (PrimitiveData(1,1,1.0,1.0,1,1,true),(3,30)) (codegen path)[0m
[32m- encode/decode for tuple with 2 product encoders: (PrimitiveData(1,1,1.0,1.0,1,1,true),(3,30)) (interpreted path)[0m
[32m- encode/decode for tuple with flat encoder and product encoder: (PrimitiveData(1,1,1.0,1.0,1,1,true),3) (codegen path)[0m
[32m- encode/decode for tuple with flat encoder and product encoder: (PrimitiveData(1,1,1.0,1.0,1,1,true),3) (interpreted path)[0m
[32m- encode/decode for tuple with product encoder and flat encoder: (3,PrimitiveData(1,1,1.0,1.0,1,1,true)) (codegen path)[0m
[32m- encode/decode for tuple with product encoder and flat encoder: (3,PrimitiveData(1,1,1.0,1.0,1,1,true)) (interpreted path)[0m
[32m- encode/decode for nested tuple encoder: (1,(10,100)) (codegen path)[0m
[32m- encode/decode for nested tuple encoder: (1,(10,100)) (interpreted path)[0m
[32m- encode/decode for primitive value class: PrimitiveValueClass(42) (codegen path)[0m
[32m- encode/decode for primitive value class: PrimitiveValueClass(42) (interpreted path)[0m
[32m- encode/decode for reference value class: ReferenceValueClass(Container(1)) (codegen path)[0m
[32m- encode/decode for reference value class: ReferenceValueClass(Container(1)) (interpreted path)[0m
[32m- encode/decode for option of int: Some(31) (codegen path)[0m
[32m- encode/decode for option of int: Some(31) (interpreted path)[0m
[32m- encode/decode for empty option of int: None (codegen path)[0m
[32m- encode/decode for empty option of int: None (interpreted path)[0m
[32m- encode/decode for option of string: Some(abc) (codegen path)[0m
[32m- encode/decode for option of string: Some(abc) (interpreted path)[0m
[32m- encode/decode for empty option of string: None (codegen path)[0m
[32m- encode/decode for empty option of string: None (interpreted path)[0m
[32m- encode/decode for Tuple2: (UDT,org.apache.spark.sql.catalyst.encoders.ExamplePoint@691) (codegen path)[0m
[32m- encode/decode for Tuple2: (UDT,org.apache.spark.sql.catalyst.encoders.ExamplePoint@691) (interpreted path)[0m
[32m- nullable of encoder schema (codegen path)[0m
[32m- nullable of encoder schema (interpreted path)[0m
[32m- nullable of encoder serializer (codegen path)[0m
[32m- nullable of encoder serializer (interpreted path)[0m
[32m- null check for map key: String (codegen path)[0m
[32m- null check for map key: String (interpreted path)[0m
[32m- null check for map key: Integer (codegen path)[0m
[32m- null check for map key: Integer (interpreted path)[0m
[32mSubstituteUnresolvedOrdinalsSuite:[0m
[32m- unresolved ordinal should not be unresolved[0m
[32m- order by ordinal[0m
[32m- group by ordinal[0m
[32mCollapseProjectSuite:[0m
[32m- collapse two deterministic, independent projects into one[0m
[32m- collapse two deterministic, dependent projects into one[0m
[32m- do not collapse nondeterministic projects[0m
[32m- collapse two nondeterministic, independent projects into one[0m
[32m- collapse one nondeterministic, one deterministic, independent projects into one[0m
[32m- collapse project into aggregate[0m
[32m- do not collapse common nondeterministic project and aggregate[0m
[32m- preserve top-level alias metadata while collapsing projects[0m
[32mCodeGeneratorWithInterpretedFallbackSuite:[0m
[32m- UnsafeProjection with codegen factory mode[0m
[32m- fallback to the interpreter mode[0m
[32m- codegen failures in the CODEGEN_ONLY mode[0m
[32mPredicateSuite:[0m
[32m- 3VL Not[0m
[32m- AND, OR, EqualTo, EqualNullSafe consistency check[0m
[32m- 3VL AND[0m
[32m- 3VL OR[0m
[32m- 3VL =[0m
[32m- basic IN predicate test[0m
[32m- IN with different types[0m
[32m- SPARK-22501: In should not generate codes beyond 64KB[0m
[32m- SPARK-22705: In should use less global variables[0m
[32m- INSET[0m
[32m- INSET: binary[0m
[32m- INSET: struct[0m
[32m- INSET: array[0m
[32m- BinaryComparison consistency check[0m
[32m- BinaryComparison: lessThan[0m
[32m- BinaryComparison: LessThanOrEqual[0m
[32m- BinaryComparison: GreaterThan[0m
[32m- BinaryComparison: GreaterThanOrEqual[0m
[32m- BinaryComparison: EqualTo[0m
[32m- BinaryComparison: EqualNullSafe[0m
[32m- BinaryComparison: null test[0m
[32m- EqualTo on complex type[0m
[32m- EqualTo double/float infinity[0m
[32m- SPARK-22693: InSet should not use global variables[0m
[32m- SPARK-24007: EqualNullSafe for FloatType and DoubleType might generate a wrong result[0m
[32m- Interpreted Predicate should initialize nondeterministic expressions[0m
[32mCombineConcatsSuite:[0m
[32m- combine nested Concat exprs[0m
[32m- combine string and binary exprs[0m
[32mComplexTypesSuite:[0m
[32m- explicit get from namedStruct[0m
[32m- explicit get from named_struct- expression maintains original deduced alias[0m
[32m- collapsed getStructField ontop of namedStruct[0m
[32m- collapse multiple CreateNamedStruct/GetStructField pairs[0m
[32m- collapsed2 - deduced names[0m
[32m- simplified array ops[0m
[32m- SPARK-22570: CreateArray should not create a lot of global variables[0m
[32m- SPARK-23208: Test code splitting for create array related methods[0m
[32m- simplify map ops[0m
[32m- simplify map ops, constant lookup, dynamic keys[0m
[32m- simplify map ops, dynamic lookup, dynamic keys, lookup is equivalent to one of the keys[0m
[32m- simplify map ops, no positive match[0m
[32m- simplify map ops, constant lookup, mixed keys, eliminated constants[0m
[32m- simplify map ops, potential dynamic match with null value + an absolute constant match[0m
[32m- SPARK-23500: Simplify array ops that are not at the top node[0m
[32m- SPARK-23500: Simplify map ops that are not top nodes[0m
[32m- SPARK-23500: Simplify complex ops that aren't at the plan root[0m
[32m- SPARK-23500: Ensure that aggregation expressions are not simplified[0m
[32m- SPARK-23500: namedStruct and getField in the same Project #1[0m
[32m- SPARK-23500: namedStruct and getField in the same Project #2[0m
[32m- SPARK-24313: support binary type as map keys in GetMapValue[0m
[32mCatalogSuite:[0m
[32m- desc table when owner is set to null[0m
[32mOptimizerRuleExclusionSuite:[0m
[32m- Exclude a single rule from multiple batches[0m
[32m- Exclude multiple rules from single or multiple batches[0m
[32m- Exclude non-existent rule with other valid rules[0m
[32m- Try to exclude some non-excludable rules[0m
[32m- Custom optimizer[0m
[32m- Verify optimized plan after excluding CombineUnions rule[0m
[32mExpressionSetSuite:[0m
[32m- expect 1: (A#1 + 1), (a#1 + 1)[0m
[32m- expect 2: (A#1 + 1), (a#1 + 2)[0m
[32m- expect 2: (A#1 + 1), (a#3 + 1)[0m
[32m- expect 2: (A#1 + 1), (B#2 + 1)[0m
[32m- expect 1: (A#1 + a#1), (a#1 + A#1)[0m
[32m- expect 1: (A#1 + B#2), (B#2 + A#1)[0m
[32m- expect 1: ((A#1 + B#2) + 3), ((B#2 + 3) + A#1), ((B#2 + A#1) + 3), ((3 + A#1) + B#2)[0m
[32m- expect 1: ((A#1 * B#2) * 3), ((B#2 * 3) * A#1), ((B#2 * A#1) * 3), ((3 * A#1) * B#2)[0m
[32m- expect 1: (A#1 = B#2), (B#2 = A#1)[0m
[32m- expect 1: ((A#1 + 1) = B#2), (B#2 = (1 + A#1))[0m
[32m- expect 2: (A#1 - B#2), (B#2 - A#1)[0m
[32m- expect 1: (A#1 > B#2), (B#2 < A#1)[0m
[32m- expect 1: (A#1 >= B#2), (B#2 <= A#1)[0m
[32m- expect 1: NOT (none#4 > 1), (none#4 <= 1), NOT (1 < none#4), (1 >= none#4)[0m
[32m- expect 1: NOT (none#5 > 1), (none#5 <= 1), NOT (1 < none#5), (1 >= none#5)[0m
[32m- expect 1: NOT (none#4 < 1), (none#4 >= 1), NOT (1 > none#4), (1 <= none#4)[0m
[32m- expect 1: NOT (none#5 < 1), (none#5 >= 1), NOT (1 > none#5), (1 <= none#5)[0m
[32m- expect 1: NOT (none#4 >= 1), (none#4 < 1), NOT (1 <= none#4), (1 > none#4)[0m
[32m- expect 1: NOT (none#5 >= 1), (none#5 < 1), NOT (1 <= none#5), (1 > none#5)[0m
[32m- expect 1: NOT (none#4 <= 1), (none#4 > 1), NOT (1 >= none#4), (1 < none#4)[0m
[32m- expect 1: NOT (none#5 <= 1), (none#5 > 1), NOT (1 >= none#5), (1 < none#5)[0m
[32m- expect 1: ((A#1 > B#2) && (A#1 <= 10)), ((A#1 <= 10) && (A#1 > B#2))[0m
[32m- expect 1: (((A#1 > B#2) && (B#2 > 100)) && (A#1 <= 10)), (((B#2 > 100) && (A#1 <= 10)) && (A#1 > B#2))[0m
[32m- expect 1: ((A#1 > B#2) || (A#1 <= 10)), ((A#1 <= 10) || (A#1 > B#2))[0m
[32m- expect 1: (((A#1 > B#2) || (B#2 > 100)) || (A#1 <= 10)), (((B#2 > 100) || (A#1 <= 10)) || (A#1 > B#2))[0m
[32m- expect 1: (((A#1 <= 10) && (A#1 > B#2)) || (B#2 > 100)), ((B#2 > 100) || ((A#1 <= 10) && (A#1 > B#2)))[0m
[32m- expect 1: ((A#1 >= B#2) || ((A#1 > 10) && (B#2 < 10))), (((B#2 < 10) && (A#1 > 10)) || (A#1 >= B#2))[0m
[32m- expect 1: (((B#2 > 100) || ((A#1 < 100) && (B#2 <= A#1))) || ((A#1 >= 10) && (B#2 >= 50))), ((((A#1 >= 10) && (B#2 >= 50)) || (B#2 > 100)) || ((A#1 < 100) && (B#2 <= A#1))), ((((B#2 >= 50) && (A#1 >= 10)) || ((B#2 <= A#1) && (A#1 < 100))) || (B#2 > 100))[0m
[32m- expect 1: ((((B#2 > 100) && (A#1 < 100)) && (B#2 <= A#1)) || ((A#1 >= 10) && (B#2 >= 50))), (((A#1 >= 10) && (B#2 >= 50)) || (((A#1 < 100) && (B#2 > 100)) && (B#2 <= A#1))), (((B#2 >= 50) && (A#1 >= 10)) || (((B#2 <= A#1) && (A#1 < 100)) && (B#2 > 100)))[0m
[32m- expect 1: (((A#1 >= 10) || (((B#2 <= 10) && (A#1 = B#2)) && (A#1 < 100))) || (B#2 >= 100)), (((((A#1 = B#2) && (A#1 < 100)) && (B#2 <= 10)) || (B#2 >= 100)) || (A#1 >= 10)), (((((A#1 < 100) && (B#2 <= 10)) && (A#1 = B#2)) || (A#1 >= 10)) || (B#2 >= 100)), ((((B#2 <= 10) && (A#1 = B#2)) && (A#1 < 100)) || ((A#1 >= 10) || (B#2 >= 100)))[0m
[32m- expect 2: ((rand(1) > A#1) && (A#1 <= 10)), ((A#1 <= 10) && (rand(1) > A#1))[0m
[32m- expect 2: (((A#1 > B#2) && (B#2 > 100)) && (rand(1) > A#1)), (((B#2 > 100) && (rand(1) > A#1)) && (A#1 > B#2))[0m
[32m- expect 2: ((rand(1) > A#1) || (A#1 <= 10)), ((A#1 <= 10) || (rand(1) > A#1))[0m
[32m- expect 2: (((A#1 > B#2) || (A#1 <= rand(1))) || (A#1 <= 10)), (((A#1 <= rand(1)) || (A#1 <= 10)) || (A#1 > B#2))[0m
[32m- expect 2: rand(1), rand(1)[0m
[32m- expect 2: (((A#1 > B#2) || (B#2 > 100)) && (A#1 = rand(1))), (((B#2 > 100) || (A#1 > B#2)) && (A#1 = rand(1)))[0m
[32m- expect 2: (((rand(1) > A#1) || ((A#1 <= rand(1)) && (A#1 > B#2))) || ((A#1 > 10) && (B#2 > 10))), (((rand(1) > A#1) || ((A#1 <= rand(1)) && (A#1 > B#2))) || ((B#2 > 10) && (A#1 > 10)))[0m
[32m- expect 2: (((rand(1) > A#1) || ((A#1 <= rand(1)) && (A#1 > B#2))) || ((A#1 > 10) && (B#2 > 10))), (((rand(1) > A#1) || ((A#1 > B#2) && (A#1 <= rand(1)))) || ((A#1 > 10) && (B#2 > 10)))[0m
[32m- add to / remove from set[0m
[32m- add multiple elements to set[0m
[32m- add single element to set with non-deterministic expressions[0m
[32m- remove single element to set with non-deterministic expressions[0m
[32m- add multiple elements to set with non-deterministic expressions[0m
[32m- remove multiple elements to set with non-deterministic expressions[0m
[32mResolvedUuidExpressionsSuite:[0m
[32m- analyzed plan sets random seed for Uuid expression[0m
[32m- Uuid expressions should have different random seeds[0m
[32m- Different analyzed plans should have different random seeds in Uuids[0m
[32mMetadataSuite:[0m
[32m- String Metadata[0m
[32m- Long Metadata[0m
[32m- Double Metadata[0m
[32m- Boolean Metadata[0m
[32m- Null Metadata[0m
[32mParserUtilsSuite:[0m
[32m- unescapeSQLString[0m
[32m- command[0m
[32m- operationNotAllowed[0m
[32m- checkDuplicateKeys[0m
[32m- source[0m
[32m- remainder[0m
[32m- string[0m
[32m- position[0m
[32m- validate[0m
[32m- withOrigin[0m
[32mExpressionEvalHelperSuite:[0m
[32m- SPARK-16489 checkEvaluation should fail if expression reuses variable names[0m
[32mXPathExpressionSuite:[0m
[Fatal Error] :1:7: XML document structures must start and end within the same entity.
[32m- xpath_boolean[0m
[Fatal Error] :1:7: XML document structures must start and end within the same entity.
[32m- xpath_short[0m
[Fatal Error] :1:7: XML document structures must start and end within the same entity.
[32m- xpath_int[0m
[Fatal Error] :1:7: XML document structures must start and end within the same entity.
[32m- xpath_long[0m
[Fatal Error] :1:7: XML document structures must start and end within the same entity.
[32m- xpath_float[0m
[Fatal Error] :1:7: XML document structures must start and end within the same entity.
[32m- xpath_double[0m
[Fatal Error] :1:7: XML document structures must start and end within the same entity.
[32m- xpath_string[0m
[Fatal Error] :1:7: XML document structures must start and end within the same entity.
[32m- xpath[0m
[32m- accept only literal path[0m
[32mHashExpressionsSuite:[0m
[32m- md5[0m
[32m- sha1[0m
[32m- sha2[0m
[32m- crc32[0m
[32m- hive-hash for null[0m
[32m- hive-hash for boolean[0m
[32m- hive-hash for byte[0m
[32m- hive-hash for short[0m
[32m- hive-hash for int[0m
[32m- hive-hash for long[0m
[32m- hive-hash for float[0m
[32m- hive-hash for double[0m
[32m- hive-hash for string[0m
[32m- hive-hash for date type[0m
[32m- hive-hash for timestamp type[0m
[32m- hive-hash for CalendarInterval type[0m
[32m- hive-hash for array[0m
[32m- hive-hash for map[0m
[32m- hive-hash for struct[0m
[32m- murmur3/xxHash64/hive hash: struct<null:null,boolean:boolean,byte:tinyint,short:smallint,int:int,long:bigint,float:float,double:double,bigDecimal:decimal(38,18),smallDecimal:decimal(10,0),string:string,binary:binary,date:date,timestamp:timestamp,udt:examplepoint>[0m
[32m- murmur3/xxHash64/hive hash: struct<arrayOfNull:array<null>,arrayOfString:array<string>,arrayOfArrayOfString:array<array<string>>,arrayOfArrayOfInt:array<array<int>>,arrayOfMap:array<map<string,string>>,arrayOfStruct:array<struct<str:string>>,arrayOfUDT:array<examplepoint>>[0m
[32m- murmur3/xxHash64/hive hash: struct<mapOfIntAndString:map<int,string>,mapOfStringAndArray:map<string,array<string>>,mapOfArrayAndInt:map<array<string>,int>,mapOfArray:map<array<string>,array<string>>,mapOfStringAndStruct:map<string,struct<str:string>>,mapOfStructAndString:map<struct<str:string>,string>,mapOfStruct:map<struct<str:string>,struct<str:string>>>[0m
[32m- murmur3/xxHash64/hive hash: struct<structOfString:struct<str:string>,structOfStructOfString:struct<struct:struct<str:string>>,structOfArray:struct<array:array<string>>,structOfMap:struct<map:map<string,string>>,structOfArrayAndMap:struct<array:array<string>,map:map<string,string>>,structOfUDT:struct<udt:examplepoint>>[0m
[32m- hive-hash for decimal[0m
[32m- SPARK-18207: Compute hash for a lot of expressions[0m
[32m- SPARK-22284: Compute hash for nested structs[0m
[32mSimplifyConditionalSuite:[0m
[32m- simplify if[0m
[32m- remove unnecessary if when the outputs are semantic equivalence[0m
[32m- remove unreachable branches[0m
[32m- remove entire CaseWhen if only the else branch is reachable[0m
[32m- remove entire CaseWhen if the first branch is always true[0m
[32m- simplify CaseWhen, prune branches following a definite true[0m
[32m- simplify CaseWhen if all the outputs are semantic equivalence[0m
[32mRemoveRedundantSortsSuite:[0m
[32m- remove redundant order by[0m
[32m- do not remove sort if the order is different[0m
[32m- filters don't affect order[0m
[32m- limits don't affect order[0m
[32m- different sorts are not simplified if limit is in between[0m
[32m- range is already sorted[0m
[32m- sort should not be removed when there is a node which doesn't guarantee any order[0m
[32m- remove two consecutive sorts[0m
[32m- remove sorts separated by Filter/Project operators[0m
[32mLogicalPlanSuite:[0m
[32m- transformUp runs on operators[0m
[32m- transformUp runs on operators recursively[0m
[32m- isStreaming[0m
[32m- transformExpressions works with a Stream[0m
[32mHigherOrderFunctionsSuite:[0m
[32m- ArrayTransform[0m
[32m- ArrayFilter[0m
[32m- ArrayExists[0m
[32m- ArrayAggregate[0m
[32m- ZipWith[0m
[32mCastSuite:[0m
[32m- null cast[0m
[32m- cast string to date[0m
[32m- cast string to timestamp[0m
[32m- cast from int[0m
[32m- cast from long[0m
[32m- cast from boolean[0m
[32m- cast from int 2[0m
[32m- cast from float[0m
[32m- cast from double[0m
[32m- cast from string[0m
[32m- data type casting[0m
[32m- cast and add[0m
[32m- from decimal[0m
[32m- casting to fixed-precision decimals[0m
[32m- cast from date[0m
[32m- cast from timestamp[0m
[32m- cast from array[0m
[32m- cast from map[0m
[32m- cast from struct[0m
[32m- cast struct with a timestamp field[0m
[32m- complex casting[0m
[32m- cast between string and interval[0m
[32m- cast string to boolean[0m
[32m- SPARK-16729 type checking for casting to date type[0m
[32m- SPARK-20302 cast with same structure[0m
[32m- SPARK-22500: cast for struct should not generate codes beyond 64KB[0m
[32m- SPARK-22570: Cast should not create a lot of global variables[0m
[32m- SPARK-22825 Cast array to string[0m
[32m- SPARK-22973 Cast map to string[0m
[32m- SPARK-22981 Cast struct to string[0m
[32m- SPARK-26706: Fix Cast.mayTruncate for bytes[0m
[32m- canSafeCast and mayTruncate must be consistent for numeric types[0m
[32m- SPARK-27671: cast from nested null type in struct[0m
[32mInferFiltersFromConstraintsSuite:[0m
[32m- filter: filter out constraints in condition[0m
[32m- single inner join: filter out values on either side on equi-join keys[0m
[32m- single inner join: filter out nulls on either side on non equal keys[0m
[32m- single inner join with pre-existing filters: filter out values on either side[0m
[32m- single outer join: no null filters are generated[0m
[32m- multiple inner joins: filter out values on all sides on equi-join keys[0m
[32m- inner join with filter: filter out values on all sides on equi-join keys[0m
[32m- inner join with alias: alias contains multiple attributes[0m
[32m- inner join with alias: alias contains single attributes[0m
[32m- generate correct filters for alias that don't produce recursive constraints[0m
[32m- No inferred filter when constraint propagation is disabled[0m
[32m- constraints should be inferred from aliased literals[0m
[32m- SPARK-23405: left-semi equal-join should filter out null join keys on both sides[0m
[32m- SPARK-21479: Outer join after-join filters push down to null-supplying side[0m
[32m- SPARK-21479: Outer join pre-existing filters push down to null-supplying side[0m
[32m- SPARK-21479: Outer join no filter push down to preserved side[0m
[32m- SPARK-23564: left anti join should filter out null join keys on right side[0m
[32m- SPARK-23564: left outer join should filter out null join keys on right side[0m
[32m- SPARK-23564: right outer join should filter out null join keys on left side[0m
[32mBufferHolderSuite:[0m
[32m- SPARK-16071 Check the size limit to avoid integer overflow[0m
[32mRowTest:[0m
[32mRow (without schema)[0m
[32m- throws an exception when accessing by fieldName[0m
[32mRow (with schema)[0m
[32m- fieldIndex(name) returns field index[0m
[32m- getAs[T] retrieves a value by fieldname[0m
[32m- Accessing non existent field throws an exception[0m
[32m- getValuesMap() retrieves values of multiple fields as a Map(field -> value)[0m
[32m- getValuesMap() retrieves null value on non AnyVal Type[0m
[32m- getAs() on type extending AnyVal throws an exception when accessing field that is null[0m
[32m- getAs() on type extending AnyVal does not throw exception when value is null[0m
[32mrow equals[0m
[32m- equality check for external rows[0m
[32m- equality check for internal rows[0m
[32mrow immutability[0m
[32m- copy should return same ref for external rows[0m
[32m- toSeq should not expose internal state for external rows[0m
[32m- toSeq should not expose internal state for internal rows[0m
[32mPullupCorrelatedPredicatesSuite:[0m
[32m- PullupCorrelatedPredicates should not produce unresolved plan[0m
[32mSubexpressionEliminationSuite:[0m
[32m- Semantic equals and hash[0m
[32m- Expression Equivalence - basic[0m
[32m- Expression Equivalence - Trees[0m
[32m- Expression equivalence - non deterministic[0m
[32m- Children of CodegenFallback[0m
[32m- Children of conditional expressions[0m
[32mErrorParserSuite:[0m
[32m- no viable input[0m
[32m- extraneous input[0m
[32m- mismatched input[0m
[32m- semantic errors[0m
[32mCatalystTypeConvertersSuite:[0m
[32m- null handling in rows[0m
[32m- null handling for individual values[0m
[32m- option handling in convertToCatalyst[0m
[32m- option handling in createToCatalystConverter[0m
[32m- primitive array handling[0m
[32m- An array with null handling[0m
[32m- converting a wrong value to the struct type[0m
[32m- converting a wrong value to the map type[0m
[32m- converting a wrong value to the array type[0m
[32m- converting a wrong value to the decimal type[0m
[32m- converting a wrong value to the string type[0m
[32m- SPARK-24571: convert Char to String[0m
[32mScalaReflectionSuite:[0m
[32m- isSubtype[0m
[32m- SQLUserDefinedType annotation on Scala structure[0m
[32m- primitive data[0m
[32m- nullable data[0m
[32m- optional data[0m
[32m- complex data[0m
[32m- generic data[0m
[32m- tuple data[0m
[32m- type-aliased data[0m
[32m- convert PrimitiveData to catalyst[0m
[32m- convert Option[Product] to catalyst[0m
[32m- infer schema from case class with multiple constructors[0m
[32m- SPARK-15062: Get correct serializer for List[_][0m
[32m- SPARK 16792: Get correct deserializer for List[_][0m
[32m- serialize and deserialize arbitrary sequence types[0m
[32m- serialize and deserialize arbitrary map types[0m
[32m- SPARK-22442: Generate correct field names for special characters[0m
[32m- SPARK-22472: add null check for top-level primitive values[0m
[32m- SPARK-23025: schemaFor should support Null type[0m
[32m- SPARK-23835: add null check to non-nullable types in Tuples[0m
[32mApproximatePercentileSuite:[0m
[32m- serialize and de-serialize[0m
[32m- class PercentileDigest, basic operations[0m
[32m- class PercentileDigest, makes sure the memory foot print is bounded[0m
[32m- class ApproximatePercentile, high level interface, update, merge, eval...[0m
[32m- class ApproximatePercentile, low level interface, update, merge, eval...[0m
[32m- class ApproximatePercentile, sql string[0m
[32m- class ApproximatePercentile, fails analysis if percentage or accuracy is not a constant[0m
[32m- class ApproximatePercentile, fails analysis if parameters are invalid[0m
[32m- class ApproximatePercentile, automatically add type casting for parameters[0m
[32m- class ApproximatePercentile, null handling[0m
[32mCanonicalizeSuite:[0m
[32m- SPARK-24276: IN expression with different order are semantically equal[0m
[32m- SPARK-26402: accessing nested fields with different cases in case insensitive mode[0m
[32mDataTypeWriteCompatibilitySuite:[0m
[32m- Check NullType is incompatible with all other types[0m
[32m- Check each type with itself[0m
[32m- Check atomic types: write allowed only when casting is safe[0m
[32m- Check struct types: missing required field[0m
[32m- Check struct types: missing starting field, matched by position[0m
[32m- Check struct types: missing middle field, matched by position[0m
[32m- Check struct types: generic colN names are ignored[0m
[32m- Check struct types: required field is optional[0m
[32m- Check struct types: data field would be dropped[0m
[32m- Check struct types: unsafe casts are not allowed[0m
[32m- Check struct types: type promotion is allowed[0m
[33m- Check struct types: missing optional field is allowed !!! IGNORED !!![0m
[32m- Check array types: unsafe casts are not allowed[0m
[32m- Check array types: type promotion is allowed[0m
[32m- Check array types: cannot write optional to required elements[0m
[32m- Check array types: writing required to optional elements is allowed[0m
[32m- Check map value types: unsafe casts are not allowed[0m
[32m- Check map value types: type promotion is allowed[0m
[32m- Check map value types: cannot write optional to required values[0m
[32m- Check map value types: writing required to optional values is allowed[0m
[32m- Check map key types: unsafe casts are not allowed[0m
[32m- Check map key types: type promotion is allowed[0m
[32m- Check types with multiple errors[0m
[32mGenerateUnsafeRowJoinerBitsetSuite:[0m
[32m- bitset concat: boundary size 0, 0[0m
[32m  + num fields: 0 and 0 [0m
[32m  + num fields: 0 and 0 [0m
[32m  + num fields: 0 and 0 [0m
[32m  + num fields: 0 and 0 [0m
[32m  + num fields: 0 and 0 [0m
[32m- bitset concat: boundary size 0, 64[0m
[32m  + num fields: 0 and 64 [0m
[32m  + num fields: 0 and 64 [0m
[32m  + num fields: 0 and 64 [0m
[32m  + num fields: 0 and 64 [0m
[32m  + num fields: 0 and 64 [0m
[32m- bitset concat: boundary size 64, 0[0m
[32m  + num fields: 64 and 0 [0m
[32m  + num fields: 64 and 0 [0m
[32m  + num fields: 64 and 0 [0m
[32m  + num fields: 64 and 0 [0m
[32m  + num fields: 64 and 0 [0m
[32m- bitset concat: boundary size 64, 64[0m
[32m  + num fields: 64 and 64 [0m
[32m  + num fields: 64 and 64 [0m
[32m  + num fields: 64 and 64 [0m
[32m  + num fields: 64 and 64 [0m
[32m  + num fields: 64 and 64 [0m
[32m- bitset concat: boundary size 0, 128[0m
[32m  + num fields: 0 and 128 [0m
[32m  + num fields: 0 and 128 [0m
[32m  + num fields: 0 and 128 [0m
[32m  + num fields: 0 and 128 [0m
[32m  + num fields: 0 and 128 [0m
[32m- bitset concat: boundary size 128, 0[0m
[32m  + num fields: 128 and 0 [0m
[32m  + num fields: 128 and 0 [0m
[32m  + num fields: 128 and 0 [0m
[32m  + num fields: 128 and 0 [0m
[32m  + num fields: 128 and 0 [0m
[32m- bitset concat: boundary size 128, 128[0m
[32m  + num fields: 128 and 128 [0m
[32m  + num fields: 128 and 128 [0m
[32m  + num fields: 128 and 128 [0m
[32m  + num fields: 128 and 128 [0m
[32m  + num fields: 128 and 128 [0m
[32m- bitset concat: single word bitsets[0m
[32m  + num fields: 10 and 5 [0m
[32m  + num fields: 10 and 5 [0m
[32m  + num fields: 10 and 5 [0m
[32m  + num fields: 10 and 5 [0m
[32m  + num fields: 10 and 5 [0m
[32m- bitset concat: first bitset larger than a word[0m
[32m  + num fields: 67 and 5 [0m
[32m  + num fields: 67 and 5 [0m
[32m  + num fields: 67 and 5 [0m
[32m  + num fields: 67 and 5 [0m
[32m  + num fields: 67 and 5 [0m
[32m- bitset concat: second bitset larger than a word[0m
[32m  + num fields: 6 and 67 [0m
[32m  + num fields: 6 and 67 [0m
[32m  + num fields: 6 and 67 [0m
[32m  + num fields: 6 and 67 [0m
[32m  + num fields: 6 and 67 [0m
[32m- bitset concat: no reduction in bitset size[0m
[32m  + num fields: 33 and 34 [0m
[32m  + num fields: 33 and 34 [0m
[32m  + num fields: 33 and 34 [0m
[32m  + num fields: 33 and 34 [0m
[32m  + num fields: 33 and 34 [0m
[32m- bitset concat: two words[0m
[32m  + num fields: 120 and 95 [0m
[32m  + num fields: 120 and 95 [0m
[32m  + num fields: 120 and 95 [0m
[32m  + num fields: 120 and 95 [0m
[32m  + num fields: 120 and 95 [0m
[32m- bitset concat: bitset 65, 128[0m
[32m  + num fields: 65 and 128 [0m
[32m  + num fields: 65 and 128 [0m
[32m  + num fields: 65 and 128 [0m
[32m  + num fields: 65 and 128 [0m
[32m  + num fields: 65 and 128 [0m
[32m- bitset concat: randomized tests[0m
[32m  + num fields: 267 and 962 [0m
[32m  + num fields: 858 and 490 [0m
[32m  + num fields: 397 and 719 [0m
[32m  + num fields: 304 and 167 [0m
[32m  + num fields: 923 and 323 [0m
[32m  + num fields: 223 and 816 [0m
[32m  + num fields: 340 and 813 [0m
[32m  + num fields: 31 and 17 [0m
[32m  + num fields: 629 and 419 [0m
[32m  + num fields: 875 and 666 [0m
[32m  + num fields: 527 and 46 [0m
[32m  + num fields: 276 and 873 [0m
[32m  + num fields: 771 and 495 [0m
[32m  + num fields: 623 and 123 [0m
[32m  + num fields: 280 and 48 [0m
[32m  + num fields: 125 and 836 [0m
[32m  + num fields: 204 and 939 [0m
[32m  + num fields: 411 and 479 [0m
[32m  + num fields: 879 and 707 [0m
[32mNullExpressionsSuite:[0m
[32m- isnull and isnotnull[0m
[32m- AssertNotNUll[0m
[32m- IsNaN[0m
[32m- nanvl[0m
[32m- coalesce[0m
[32m- SPARK-16602 Nvl should support numeric-string cases[0m
[32m- AtLeastNNonNulls[0m
[32m- Coalesce should not throw 64kb exception[0m
[32m- SPARK-22705: Coalesce should use less global variables[0m
[32m- AtLeastNNonNulls should not throw 64kb exception[0m
[32mLookupFunctionsSuite:[0m
[32m- SPARK-23486: the functionExists for the Persistent function check[0m
[32m- SPARK-23486: the functionExists for the Registered function check[0m
[32mEliminateDistinctSuite:[0m
[32m- Eliminate Distinct in Max[0m
[32m- Eliminate Distinct in Min[0m
[32mDateTimeUtilsSuite:[0m
[32m- nanoseconds truncation[0m
[32m- timestamp and us[0m
[32m- us and julian day[0m
[32m- SPARK-6785: java date conversion before and after epoch[0m
[32m- string to date[0m
[32m- string to time[0m
[32m- string to timestamp[0m
[32m- SPARK-15379: special invalid date string[0m
[32m- hours[0m
[32m- minutes[0m
[32m- seconds[0m
[32m- hours / minutes / seconds[0m
[32m- get day in year[0m
[32m- get year[0m
[32m- get quarter[0m
[32m- get month[0m
[32m- get day of month[0m
[32m- date add months[0m
[32m- timestamp add months[0m
[32m- monthsBetween[0m
[32m- from UTC timestamp[0m
[32m- to UTC timestamp[0m
[32m- truncTimestamp[0m
[32m- daysToMillis and millisToDays[0m
[32mPlanParserSuite:[0m
[32m- case insensitive[0m
[32m- explain[0m
[32m- set operations[0m
[32m- common table expressions[0m
[32m- simple select query[0m
[32m- reverse select query[0m
[32m- multi select query[0m
[32m- query organization[0m
[32m- insert into[0m
[32m- insert with if not exists[0m
[32m- aggregation[0m
[32m- limit[0m
[32m- window spec[0m
[32m- lateral view[0m
[32m- joins[0m
[32m- sampled relations[0m
[32m- sub-query[0m
[32m- scalar sub-query[0m
[32m- table reference[0m
[32m- table valued function[0m
[32m- SPARK-20311 range(N) as alias[0m
[32m- SPARK-20841 Support table column aliases in FROM clause[0m
[32m- SPARK-20962 Support subquery column aliases in FROM clause[0m
[32m- SPARK-20963 Support aliases for join relations in FROM clause[0m
[32m- inline table[0m
[32m- simple select query with !> and !<[0m
[32m- select hint syntax[0m
[32m- SPARK-20854: select hint syntax with expressions[0m
[32m- SPARK-20854: multiple hints[0m
[32m- TRIM function[0m
[32m- precedence of set operations[0m
[32mGeneratedProjectionSuite:[0m
[32m- generated projections on wider table[0m
[32m- SPARK-18016: generated projections on wider table requiring class-splitting[0m
[32m- generated unsafe projection with array of binary[0m
[32m- padding bytes should be zeroed out[0m
[32m- MutableProjection should not cache content from the input row[0m
[32m- SafeProjection should not cache content from the input row[0m
[32m- SPARK-22699: GenerateSafeProjection should not use global variables for struct[0m
[32m- SPARK-18016: generated projections on wider table requiring state compaction[0m
[32mSetOperationSuite:[0m
[32m- union: combine unions into one unions[0m
[32m- union: filter to each side[0m
[32m- union: project to each side[0m
[32m- Remove unnecessary distincts in multiple unions[0m
[32m- Keep necessary distincts in multiple unions[0m
[32m- EXCEPT ALL rewrite[0m
[32m- INTERSECT ALL rewrite[0m
[32mRuleExecutorSuite:[0m
[32m- only once[0m
[32m- to fixed point[0m
[32m- to maxIterations[0m
[32m- structural integrity checker[0m
[32mDistributionSuite:[0m
[32m- UnspecifiedDistribution and AllTuples[0m
[32m- SinglePartition is the output partitioning[0m
[32m- HashPartitioning is the output partitioning[0m
[32m- RangePartitioning is the output partitioning[0m
[32m- Partitioning.numPartitions must match Distribution.requiredNumPartitions to satisfy it[0m
[32mRewriteSubquerySuite:[0m
[32m- Column pruning after rewriting predicate subquery[0m
[32mJoinReorderSuite:[0m
[32m- reorder 3 tables[0m
[32m- put unjoinable item at the end and reorder 3 joinable tables[0m
[32m- reorder 3 tables with pure-attribute project[0m
[32m- reorder 3 tables - one of the leaf items is a project[0m
[32m- don't reorder if project contains non-attribute[0m
[32m- reorder 4 tables (bushy tree)[0m
[32m- keep the order of attributes in the final output[0m
[32m- SPARK-26352: join reordering should not change the order of attributes[0m
[32m- reorder recursively[0m
[32mCodegenExpressionCachingSuite:[0m
[32m- GenerateUnsafeProjection should initialize expressions[0m
[32m- GenerateMutableProjection should initialize expressions[0m
[32m- GeneratePredicate should initialize expressions[0m
[32m- GenerateUnsafeProjection should not share expression instances[0m
[32m- GenerateMutableProjection should not share expression instances[0m
[32m- GeneratePredicate should not share expression instances[0m
[32mConditionalExpressionSuite:[0m
[32m- if[0m
[32m- case when[0m
[32m- if/case when - null flags of non-primitive types[0m
[32m- case key when[0m
[32m- case key when - internal pattern matching expects a List while apply takes a Seq[0m
[32m- SPARK-22705: case when should use less global variables[0m
[32m- SPARK-27917 test semantic equals of CaseWhen[0m
[32mBasicStatsEstimationSuite:[0m
[32m- BroadcastHint estimation[0m
[32m- range[0m
[32m- windows[0m
[32m- limit estimation: limit < child's rowCount[0m
[32m- limit estimation: limit > child's rowCount[0m
[32m- limit estimation: limit = 0[0m
[32m- sample estimation[0m
[32m- estimate statistics when the conf changes[0m
[32mResolveSubquerySuite:[0m
[32m- SPARK-17251 Improve `OuterReference` to be `NamedExpression`[0m
[32mComputeCurrentTimeSuite:[0m
[32m- analyzer should replace current_timestamp with literals[0m
[32m- analyzer should replace current_date with literals[0m
[32mFilterPushdownSuite:[0m
[32m- eliminate subqueries[0m
[32m- simple push down[0m
[32m- combine redundant filters[0m
[32m- do not combine non-deterministic filters even if they are identical[0m
[32m- SPARK-16164: Filter pushdown should keep the ordering in the logical plan[0m
[32m- SPARK-16994: filter should not be pushed through limit[0m
[32m- can't push without rewrite[0m
[32m- nondeterministic: can always push down filter through project with deterministic field[0m
[32m- nondeterministic: can't push down filter through project with nondeterministic field[0m
[32m- nondeterministic: can't push down filter through aggregate with nondeterministic field[0m
[32m- nondeterministic: push down part of filter through aggregate with deterministic field[0m
[32m- filters: combines filters[0m
[32m- joins: push to either side[0m
[32m- joins: push to one side[0m
[32m- joins: do not push down non-deterministic filters into join condition[0m
[32m- joins: push to one side after transformCondition[0m
[32m- joins: rewrite filter to push to either side[0m
[32m- joins: push down left semi join[0m
[32m- joins: push down left outer join #1[0m
[32m- joins: push down right outer join #1[0m
[32m- joins: push down left outer join #2[0m
[32m- joins: push down right outer join #2[0m
[32m- joins: push down left outer join #3[0m
[32m- joins: push down right outer join #3[0m
[32m- joins: push down left outer join #4[0m
[32m- joins: push down right outer join #4[0m
[32m- joins: push down left outer join #5[0m
[32m- joins: push down right outer join #5[0m
[32m- joins: can't push down[0m
[32m- joins: conjunctive predicates[0m
[32m- joins: conjunctive predicates #2[0m
[32m- joins: conjunctive predicates #3[0m
[32m- joins: push down where clause into left anti join[0m
[32m- joins: only push down join conditions to the right of a left anti join[0m
[32m- joins: only push down join conditions to the right of an existence join[0m
[32m- generate: predicate referenced no generated column[0m
[32m- generate: non-deterministic predicate referenced no generated column[0m
[32m- generate: part of conjuncts referenced generated column[0m
[32m- generate: all conjuncts referenced generated column[0m
[32m- aggregate: push down filter when filter on group by expression[0m
[32m- aggregate: don't push down filter when filter not on group by expression[0m
[32m- aggregate: push down filters partially which are subset of group by expressions[0m
[32m- aggregate: push down filters with alias[0m
[32m- aggregate: push down filters with literal[0m
[32m- aggregate: don't push down filters that are nondeterministic[0m
[32m- SPARK-17712: aggregate: don't push down filters that are data-independent[0m
[32m- aggregate: don't push filters if the aggregate has no grouping expressions[0m
[32m- broadcast hint[0m
[32m- union[0m
[32m- expand[0m
[32m- predicate subquery: push down simple[0m
[32m- predicate subquery: push down complex[0m
[32m- SPARK-20094: don't push predicate with IN subquery into join condition[0m
[32m- Window: predicate push down -- basic[0m
[32m- Window: predicate push down -- predicates with compound predicate using only one column[0m
[32m- Window: predicate push down -- multi window expressions with the same window spec[0m
[32m- Window: predicate push down -- multi window specification - 1[0m
[32m- Window: predicate push down -- multi window specification - 2[0m
[32m- Window: predicate push down -- predicates with multiple partitioning columns[0m
[33m- Window: predicate push down -- complex predicate with the same expressions !!! IGNORED !!![0m
[32m- Window: no predicate push down -- predicates are not from partitioning keys[0m
[32m- Window: no predicate push down -- partial compound partition key[0m
[32m- Window: no predicate push down -- complex predicates containing non partitioning columns[0m
[32m- Window: no predicate push down -- complex predicate with different expressions[0m
[32m- join condition pushdown: deterministic and non-deterministic[0m
[32m- watermark pushdown: no pushdown on watermark attribute #1[0m
[32m- watermark pushdown: no pushdown for nondeterministic filter[0m
[32m- watermark pushdown: full pushdown[0m
[32m- watermark pushdown: no pushdown on watermark attribute #2[0m
[32mExprValueSuite:[0m
[32m- TrueLiteral and FalseLiteral should be LiteralValue[0m
[32mBufferHolderSparkSubmitSuite:[0m
[32m- SPARK-22222: Buffer holder should be able to allocate memory larger than 1GB[0m
[32mAttributeSetSuite:[0m
[32m- sanity check[0m
[32m- checks by id not name[0m
[32m- ++ preserves AttributeSet[0m
[32m- extracts all references [0m
[32m- dedups attributes[0m
[32m- subset[0m
[32m- equality[0m
[32m- SPARK-18394 keep a deterministic output order along with attribute names and exprIds[0m
[32mTypeCoercionSuite:[0m
[32m- implicit type cast - ByteType[0m
[32m- implicit type cast - ShortType[0m
[32m- implicit type cast - IntegerType[0m
[32m- implicit type cast - LongType[0m
[32m- implicit type cast - FloatType[0m
[32m- implicit type cast - DoubleType[0m
[32m- implicit type cast - DecimalType(10, 2)[0m
[32m- implicit type cast - BinaryType[0m
[32m- implicit type cast - BooleanType[0m
[32m- implicit type cast - StringType[0m
[32m- implicit type cast - DateType[0m
[32m- implicit type cast - TimestampType[0m
[32m- implicit type cast - ArrayType(StringType)[0m
[32m- implicit type cast between two Map types[0m
[32m- implicit type cast - StructType().add("a1", StringType)[0m
[32m- implicit type cast - NullType[0m
[32m- implicit type cast - CalendarIntervalType[0m
[32m- eligible implicit type cast - TypeCollection[0m
[32m- ineligible implicit type cast - TypeCollection[0m
[32m- tightest common bound for types[0m
[32m- wider common type for decimal and array[0m
[32m- cast NullType for expressions that implement ExpectsInputTypes[0m
[32m- cast NullType for binary operators[0m
[32m- coalesce casts[0m
[32m- CreateArray casts[0m
[32m- CreateMap casts[0m
[32m- greatest/least cast[0m
[32m- nanvl casts[0m
[32m- type coercion for If[0m
[32m- type coercion for CaseKeyWhen[0m
[32m- type coercion for Stack[0m
[32m- type coercion for Concat[0m
[32m- type coercion for Elt[0m
[32m- BooleanEquality type cast[0m
[32m- BooleanEquality simplification[0m
[32m- WidenSetOperationTypes for except and intersect[0m
[32m- WidenSetOperationTypes for union[0m
[32m- Transform Decimal precision/scale for union except and intersect[0m
[32m- rule for date/timestamp operations[0m
[32m- make sure rules do not fire early[0m
[32m- SPARK-15776 Divide expression's dataType should be casted to Double or Decimal in aggregation function like sum[0m
[32m- SPARK-17117 null type coercion in divide[0m
[32m- binary comparison with string promotion[0m
[32m- cast WindowFrame boundaries to the type they operate upon[0m
[32mRandomUUIDGeneratorSuite:[0m
[32m- RandomUUIDGenerator should generate version 4, variant 2 UUIDs[0m
[32m- UUID from RandomUUIDGenerator should be deterministic[0m
[32m- Get UTF8String UUID[0m
[32mJoinOptimizationSuite:[0m
[32m- extract filters and joins[0m
[32m- reorder inner joins[0m
[32m- broadcasthint sets relation statistics to smallest value[0m
[32mAggregateOptimizeSuite:[0m
[32m- remove literals in grouping expression[0m
[32m- do not remove all grouping expressions if they are all literals[0m
[32m- Remove aliased literals[0m
[32m- remove repetition in grouping expression[0m
[32mDateFormatterSuite:[0m
[32m- parsing dates[0m
[32m- format dates[0m
[32m- roundtrip date -> days -> date[0m
[32m- roundtrip days -> date -> days[0m
[32m- parsing date without explicit day[0m
[32mAggregateExpressionSuite:[0m
[32m- test references from unresolved aggregate functions[0m
[32mGenerateUnsafeRowJoinerSuite:[0m
[32m- simple fixed width types[0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m- rows with all empty strings[0m
[32m- rows with all empty int arrays[0m
[32m- alternating empty and non-empty strings[0m
[32m- randomized fix width types[0m
[32m  + schema size 55, 79 [0m
[32m  + schema size 82, 44 [0m
[32m  + schema size 68, 98 [0m
[32m  + schema size 83, 27 [0m
[32m  + schema size 28, 73 [0m
[32m  + schema size 43, 23 [0m
[32m  + schema size 9, 12 [0m
[32m  + schema size 74, 78 [0m
[32m  + schema size 77, 90 [0m
[32m  + schema size 99, 38 [0m
[32m  + schema size 60, 65 [0m
[32m  + schema size 9, 87 [0m
[32m  + schema size 81, 3 [0m
[32m  + schema size 49, 76 [0m
[32m  + schema size 28, 29 [0m
[32m  + schema size 75, 28 [0m
[32m  + schema size 50, 25 [0m
[32m  + schema size 14, 81 [0m
[32m  + schema size 13, 89 [0m
[32m  + schema size 25, 59 [0m
[32m- simple variable width types[0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m- randomized variable width types[0m
[32m  + schema size 39, 84 [0m
[32m  + schema size 61, 12 [0m
[32m  + schema size 68, 34 [0m
[32m  + schema size 77, 24 [0m
[32m  + schema size 78, 71 [0m
[32m  + schema size 66, 53 [0m
[32m  + schema size 46, 89 [0m
[32m  + schema size 95, 29 [0m
[32m  + schema size 35, 33 [0m
[32m  + schema size 80, 86 [0m
[32m- SPARK-22508: GenerateUnsafeRowJoiner.create should not generate codes beyond 64KB[0m
[32m  + schema size 3000, 3000 [0m
[32mUnsafeMapSuite:[0m
[31m- unsafe java serialization *** FAILED ***[0m
[31m  0 did not equal 19285 (UnsafeMapSuite.scala:51)[0m
[31m- unsafe Kryo serialization *** FAILED ***[0m
[31m  0 did not equal 19285 (UnsafeMapSuite.scala:60)[0m
[32mEliminateSortsSuite:[0m
[32m- Empty order by clause[0m
[32m- All the SortOrder are no-op[0m
[32m- Partial order-by clauses contain no-op SortOrder[0m
[32m- Remove no-op alias[0m
[32mJsonExpressionsSuite:[0m
[32m- $.store.bicycle[0m
[32m- $['store'].bicycle[0m
[32m- $.store['bicycle'][0m
[32m- $['store']['bicycle'][0m
[32m- $['key with spaces'][0m
[32m- $.store.book[0m
[32m- $.store.book[0][0m
[32m- $.store.book[*][0m
[32m- $[0m
[32m- $.store.book[0].category[0m
[32m- $.store.book[*].category[0m
[32m- $.store.book[*].isbn[0m
[32m- $.store.book[*].reader[0m
[32m- $.store.basket[0][1][0m
[32m- $.store.basket[*][0m
[32m- $.store.basket[*][0][0m
[32m- $.store.basket[0][*][0m
[32m- $.store.basket[*][*][0m
[32m- $.store.basket[0][2].b[0m
[32m- $.store.basket[0][*].b[0m
[32m- $.zip code[0m
[32m- $.fb:testid[0m
[32m- preserve newlines[0m
[32m- escape[0m
[32m- $.non_exist_key[0m
[32m- $..no_recursive[0m
[32m- $.store.book[10][0m
[32m- $.store.book[0].non_exist_key[0m
[32m- $.store.basket[*].non_exist_key[0m
[32m- SPARK-16548: character conversion[0m
[32m- non foldable literal[0m
[32m- json_tuple - hive key 1[0m
[32m- json_tuple - hive key 2[0m
[32m- json_tuple - hive key 2 (mix of foldable fields)[0m
[32m- json_tuple - hive key 3[0m
[32m- json_tuple - hive key 3 (nonfoldable json)[0m
[32m- json_tuple - hive key 3 (nonfoldable fields)[0m
[32m- json_tuple - hive key 4 - null json[0m
[32m- json_tuple - hive key 5 - null and empty fields[0m
[32m- json_tuple - hive key 6 - invalid json (array)[0m
[32m- json_tuple - invalid json (object start only)[0m
[32m- json_tuple - invalid json (no object end)[0m
[32m- json_tuple - invalid json (invalid json)[0m
[32m- SPARK-16548: json_tuple - invalid json with leading nulls[0m
[32m- json_tuple - preserve newlines[0m
[32m- SPARK-21677: json_tuple throws NullPointException when column is null as string type[0m
[32m- SPARK-21804: json_tuple returns null values within repeated columns except the first one[0m
[32m- from_json[0m
[32m- from_json - invalid data[0m
[32m- from_json - input=array, schema=array, output=array[0m
[32m- from_json - input=object, schema=array, output=array of single row[0m
[32m- from_json - input=empty array, schema=array, output=empty array[0m
[32m- from_json - input=empty object, schema=array, output=array of single row with null[0m
[32m- from_json - input=array of single object, schema=struct, output=single row[0m
[32m- from_json - input=array, schema=struct, output=null[0m
[32m- from_json - input=empty array, schema=struct, output=null[0m
[32m- from_json - input=empty object, schema=struct, output=single row with null[0m
[32m- from_json null input column[0m
[32m- SPARK-20549: from_json bad UTF-8[0m
[32m- from_json with timestamp[0m
[32m- SPARK-19543: from_json empty input column[0m
[32m- to_json - struct[0m
[32m- to_json - array[0m
[32m- to_json - array with single empty row[0m
[32m- to_json - empty array[0m
[32m- to_json null input column[0m
[32m- to_json with timestamp[0m
[32m- SPARK-21513: to_json support map[string, struct] to json[0m
[32m- SPARK-21513: to_json support map[struct, struct] to json[0m
[32m- SPARK-21513: to_json support map[string, integer] to json[0m
[32m- to_json - array with maps[0m
[32m- to_json - array with single map[0m
[32m- to_json: verify MapType's value type instead of key type[0m
[32m- from_json missing fields[0m
[32m- SPARK-24709: infer schema of json strings[0m
[32mMetadataSuite:[0m
[32m- metadata builder and getters[0m
[32m- metadata json conversion[0m
[32mStructTypeSuite:[0m
[32m- lookup a single missing field should output existing fields[0m
[32m- lookup a set of missing fields should output existing fields[0m
[32m- lookup fieldIndex for missing field should output existing fields[0m
[32m- SPARK-24849: toDDL - simple struct[0m
[32m- SPARK-24849: round trip toDDL - fromDDL[0m
[32m- SPARK-24849: round trip fromDDL - toDDL[0m
[32m- SPARK-24849: toDDL must take into account case of fields.[0m
[32m- SPARK-24849: toDDL should output field's comment[0m
[32mSortOrderExpressionsSuite:[0m
[32m- SortPrefix[0m
[32mConstraintPropagationSuite:[0m
[32m- propagating constraints in filters[0m
[32m- propagating constraints in aggregate[0m
[32m- propagating constraints in expand[0m
[32m- propagating constraints in aliases[0m
[32m- propagating constraints in union[0m
[32m- propagating constraints in intersect[0m
[32m- propagating constraints in except[0m
[32m- propagating constraints in inner join[0m
[32m- propagating constraints in left-semi join[0m
[32m- propagating constraints in left-outer join[0m
[32m- propagating constraints in right-outer join[0m
[32m- propagating constraints in full-outer join[0m
[32m- infer additional constraints in filters[0m
[32m- infer constraints on cast[0m
[32m- infer isnotnull constraints from compound expressions[0m
[32m- infer IsNotNull constraints from non-nullable attributes[0m
[32m- not infer non-deterministic constraints[0m
[32m- enable/disable constraint propagation[0m
[32mConvertToLocalRelationSuite:[0m
[32m- Project on LocalRelation should be turned into a single LocalRelation[0m
[32m- Filter on LocalRelation should be turned into a single LocalRelation[0m
[32m- SPARK-27798: Expression reusing output shouldn't override values in local relation[0m
[32mCodeBlockSuite:[0m
[32m- Block interpolates string and ExprValue inputs[0m
[32m- Literals are folded into string code parts instead of block inputs[0m
[32m- Block.stripMargin[0m
[32m- Block can capture input expr values[0m
[32m- concatenate blocks[0m
[32m- Throws exception when interpolating unexcepted object in code block[0m
[32m- transform expr in code block[0m
[32m- transform expr in nested blocks[0m
[32mTreeNodeSuite:[0m
[32m- top node changed[0m
[32m- one child changed[0m
[32m- no change[0m
[32m- collect[0m
[32m- pre-order transform[0m
[32m- post-order transform[0m
[32m- transform works on nodes with Option children[0m
[32m- mapChildren should only works on children[0m
[32m- preserves origin[0m
[32m- foreach up[0m
[32m- find[0m
[32m- collectFirst[0m
[32m- transformExpressions on nested expression sequence[0m
[32m- expressions inside a map[0m
[32m- toJSON[0m
[32m- toJSON should not throws java.lang.StackOverflowError[0m
[32m- transform works on stream of children[0m
[32m- withNewChildren on stream of children[0m
[32mEliminateMapObjectsSuite:[0m
[32m- SPARK-20254: Remove unnecessary data conversion for primitive array[0m
[32mCountMinSketchAggSuite:[0m
[32m- test data type ByteType[0m
[32m- test data type ShortType[0m
[32m- test data type IntegerType[0m
[32m- test data type LongType[0m
[32m- test data type StringType[0m
[32m- test data type BinaryType[0m
[32m- serialize and de-serialize[0m
[32m- fails analysis if eps, confidence or seed provided is not foldable[0m
[32m- fails analysis if parameters are invalid[0m
[32m- null handling[0m
[32mArithmeticExpressionSuite:[0m
[32m- + (Add)[0m
[32m- - (UnaryMinus)[0m
[32m- - (Minus)[0m
[32m- * (Multiply)[0m
[32m- / (Divide) basic[0m
[33m- / (Divide) for integral type !!! IGNORED !!![0m
[32m- % (Remainder)[0m
[32m- SPARK-17617: % (Remainder) double % double on super big double[0m
[32m- Abs[0m
[32m- pmod[0m
[32m- function least[0m
[32m- function greatest[0m
[32m- SPARK-22499: Least and greatest should not generate codes beyond 64KB[0m
[32m- SPARK-22704: Least and greatest use less global variables[0m
[32mConstantFoldingSuite:[0m
[32m- eliminate subqueries[0m
[32m- Constant folding test: expressions only have literals[0m
[32m- Constant folding test: expressions have attribute references and literals in arithmetic operations[0m
[32m- Constant folding test: expressions have attribute references and literals in predicates[0m
[32m- Constant folding test: expressions have foldable functions[0m
[32m- Constant folding test: expressions have nonfoldable functions[0m
[32m- Constant folding test: expressions have null literals[0m
[32m- Constant folding test: Fold In(v, list) into true or false[0m
[32mCollapseRepartitionSuite:[0m
[32m- collapse two adjacent coalesces into one[0m
[32m- collapse two adjacent repartitions into one[0m
[32m- coalesce above repartition[0m
[32m- repartition above coalesce[0m
[32m- distribute above repartition[0m
[32m- distribute above coalesce[0m
[32m- repartition above distribute[0m
[32m- coalesce above distribute[0m
[32m- collapse two adjacent distributes into one[0m
[32mRewriteDistinctAggregatesSuite:[0m
[32m- single distinct group[0m
[32m- single distinct group with partial aggregates[0m
[32m- multiple distinct groups[0m
[32m- multiple distinct groups with partial aggregates[0m
[32m- multiple distinct groups with non-partial aggregates[0m
[32mUpdateNullabilityInAttributeReferencesSuite:[0m
[32m- update nullability in AttributeReference[0m
[32mJacksonGeneratorSuite:[0m
[32m- initial with StructType and write out a row[0m
[32m- initial with StructType and write out rows[0m
[32m- initial with StructType and write out an array with single empty row[0m
[32m- initial with StructType and write out an empty array[0m
[32m- initial with Map and write out a map data[0m
[32m- initial with Map and write out an array of maps[0m
[32m- error handling: initial with StructType but error calling write a map[0m
[32m- error handling: initial with MapType and write out a row[0m
[32mDataTypeParserSuite:[0m
[32m- parse int[0m
[32m- parse integer[0m
[32m- parse BooLean[0m
[32m- parse tinYint[0m
[32m- parse smallINT[0m
[32m- parse INT[0m
[32m- parse INTEGER[0m
[32m- parse bigint[0m
[32m- parse float[0m
[32m- parse dOUBle[0m
[32m- parse decimal(10, 5)[0m
[32m- parse decimal[0m
[32m- parse DATE[0m
[32m- parse timestamp[0m
[32m- parse string[0m
[32m- parse ChaR(5)[0m
[32m- parse varchAr(20)[0m
[32m- parse cHaR(27)[0m
[32m- parse BINARY[0m
[32m- parse array<doublE>[0m
[32m- parse Array<map<int, tinYint>>[0m
[32m- parse array<struct<tinYint:tinyint>>[0m
[32m- parse MAP<int, STRING>[0m
[32m- parse MAp<int, ARRAY<double>>[0m
[32m- parse MAP<int, struct<varchar:string>>[0m
[32m- parse struct<intType: int, ts:timestamp>[0m
[32m- parse Struct<int: int, timestamp:timestamp>[0m
[32m- parse struct<  struct:struct<deciMal:DECimal, anotherDecimal:decimAL(5,2)>,  MAP:Map<timestamp, varchar(10)>,  arrAy:Array<double>,  anotherArray:Array<char(9)>>    [0m
[32m- parse struct<`x+y`:int, `!@#$%^&*()`:string, `1_2.345<>:"`:varchar(20)>[0m
[32m- parse strUCt<>[0m
[32m- it is not a data type is not supported[0m
[32m- struct<x+y: int, 1.1:timestamp> is not supported[0m
[32m- struct<x: int is not supported[0m
[32m- struct<x int, y string> is not supported[0m
[32m- Do not print empty parentheses for no params[0m
[32m- parse Struct<TABLE: string, DATE:boolean>[0m
[32m- parse struct<end: long, select: int, from: string>[0m
[32m- parse Struct<x: INT, y: STRING COMMENT 'test'>[0m
[32mApproxCountDistinctForIntervalsSuite:[0m
[32m- fails analysis if parameters are invalid[0m
[32m- merging ApproxCountDistinctForIntervals instances[0m
[32m- test findHllppIndex(value) for values in the range[0m
[32m- round trip serialization[0m
[32m- basic operations: update, merge, eval...[0m
[32m- test for different input types: numeric/date/timestamp[0m
[32mRegexpExpressionsSuite:[0m
[32m- LIKE Pattern[0m
[32m- RLIKE Regular Expression[0m
[32m- RegexReplace[0m
[32m- SPARK-22570: RegExpReplace should not create a lot of global variables[0m
[32m- RegexExtract[0m
[32m- SPLIT[0m
[32mDecimalExpressionSuite:[0m
[32m- UnscaledValue[0m
[32m- MakeDecimal[0m
[32m- PromotePrecision[0m
[32m- CheckOverflow[0m
[32mRandomSuite:[0m
[32m- random[0m
[32m- SPARK-9127 codegen with long seed[0m
[32mCodeGenerationSuite:[0m
[32m- multithreaded eval[0m
[32m- metrics are recorded on compile[0m
[32m- SPARK-8443: split wide projections into blocks due to JVM code size limit[0m
[32m- SPARK-13242: case-when expression with large number of branches (or cases)[0m
[32m- SPARK-22543: split large if expressions into blocks due to JVM code size limit[0m
[32m- SPARK-14793: split wide array creation into blocks due to JVM code size limit[0m
[32m- SPARK-14793: split wide map creation into blocks due to JVM code size limit[0m
[32m- SPARK-14793: split wide struct creation into blocks due to JVM code size limit[0m
[32m- SPARK-14793: split wide named struct creation into blocks due to JVM code size limit[0m
[32m- SPARK-14224: split wide external row creation into blocks due to JVM code size limit[0m
[32m- SPARK-17702: split wide constructor into blocks due to JVM code size limit[0m
[32m- SPARK-22226: group splitted expressions into one method per nested class[0m
[32m- test generated safe and unsafe projection[0m
[32m- */ in the data[0m
[32m- \u in the data[0m
[32m- check compilation error doesn't occur caused by specific literal[0m
[32m- SPARK-17160: field names are properly escaped by GetExternalRowField[0m
[32m- SPARK-17160: field names are properly escaped by AssertTrue[0m
[32m- should not apply common subexpression elimination on conditional expressions[0m
[32m- SPARK-22543: split large predicates into blocks due to JVM code size limit[0m
[32m- SPARK-22696: CreateExternalRow should not use global variables[0m
[32m- SPARK-22696: InitializeJavaBean should not use global variables[0m
[32m- SPARK-22716: addReferenceObj should not add mutable states[0m
[32m- SPARK-18016: define mutable states by using an array[0m
[32m- SPARK-22750: addImmutableStateIfNotExists[0m
[32m- SPARK-23628: calculateParamLength should compute properly the param length[0m
[32m- SPARK-23760: CodegenContext.withSubExprEliminationExprs should save/restore correctly[0m
[32m- SPARK-23986: freshName can generate duplicated names[0m
[32m- SPARK-25113: should log when there exists generated methods above HugeMethodLimit[0m
[32mTableSchemaParserSuite:[0m
[32m- parse a int[0m
[32m- parse A int[0m
[32m- parse a INT[0m
[32m- parse `!@#$%.^&*()` string[0m
[32m- parse a int, b long[0m
[32m- parse a STRUCT<intType: int, ts:timestamp>[0m
[32m- parse a int comment 'test'[0m
[32m- complex hive type[0m
[32m- Negative cases[0m
[32mRowEncoderSuite:[0m
[32m- encode/decode: struct<null:null,boolean:boolean,byte:tinyint,short:smallint,int:int,long:bigint,float:float,double:double,decimal:decimal(38,18),string:string,binary:binary,date:date,timestamp:timestamp,udt:examplepoint> (codegen path)[0m
[32m- encode/decode: struct<null:null,boolean:boolean,byte:tinyint,short:smallint,int:int,long:bigint,float:float,double:double,decimal:decimal(38,18),string:string,binary:binary,date:date,timestamp:timestamp,udt:examplepoint> (interpreted path)[0m
[32m- encode/decode: struct<arrayOfNull:array<null>,arrayOfString:array<string>,arrayOfArrayOfString:array<array<string>>,arrayOfArrayOfInt:array<array<int>>,arrayOfMap:array<map<string,string>>,arrayOfStruct:array<struct<str:string>>,arrayOfUDT:array<examplepoint>> (codegen path)[0m
[32m- encode/decode: struct<arrayOfNull:array<null>,arrayOfString:array<string>,arrayOfArrayOfString:array<array<string>>,arrayOfArrayOfInt:array<array<int>>,arrayOfMap:array<map<string,string>>,arrayOfStruct:array<struct<str:string>>,arrayOfUDT:array<examplepoint>> (interpreted path)[0m
[32m- encode/decode: struct<mapOfIntAndString:map<int,string>,mapOfStringAndArray:map<string,array<string>>,mapOfArrayAndInt:map<array<string>,int>,mapOfArray:map<array<string>,array<string>>,mapOfStringAndStruct:map<string,struct<str:string>>,mapOfStructAndString:map<struct<str:string>,string>,mapOfStruct:map<struct<str:string>,struct<str:string>>> (codegen path)[0m
[32m- encode/decode: struct<mapOfIntAndString:map<int,string>,mapOfStringAndArray:map<string,array<string>>,mapOfArrayAndInt:map<array<string>,int>,mapOfArray:map<array<string>,array<string>>,mapOfStringAndStruct:map<string,struct<str:string>>,mapOfStructAndString:map<struct<str:string>,string>,mapOfStruct:map<struct<str:string>,struct<str:string>>> (interpreted path)[0m
[32m- encode/decode: struct<structOfString:struct<str:string>,structOfStructOfString:struct<struct:struct<str:string>>,structOfArray:struct<array:array<string>>,structOfMap:struct<map:map<string,string>>,structOfArrayAndMap:struct<array:array<string>,map:map<string,string>>,structOfUDT:struct<udt:examplepoint>> (codegen path)[0m
[32m- encode/decode: struct<structOfString:struct<str:string>,structOfStructOfString:struct<struct:struct<str:string>>,structOfArray:struct<array:array<string>>,structOfMap:struct<map:map<string,string>>,structOfArrayAndMap:struct<array:array<string>,map:map<string,string>>,structOfUDT:struct<udt:examplepoint>> (interpreted path)[0m
[32m- encode/decode decimal type (codegen path)[0m
[32m- encode/decode decimal type (interpreted path)[0m
[32m- RowEncoder should preserve decimal precision and scale (codegen path)[0m
[32m- RowEncoder should preserve decimal precision and scale (interpreted path)[0m
[32m- RowEncoder should preserve schema nullability (codegen path)[0m
[32m- RowEncoder should preserve schema nullability (interpreted path)[0m
[32m- RowEncoder should preserve nested column name (codegen path)[0m
[32m- RowEncoder should preserve nested column name (interpreted path)[0m
[32m- RowEncoder should support primitive arrays (codegen path)[0m
[32m- RowEncoder should support primitive arrays (interpreted path)[0m
[32m- RowEncoder should support array as the external type for ArrayType (codegen path)[0m
[32m- RowEncoder should support array as the external type for ArrayType (interpreted path)[0m
[32m- RowEncoder should throw RuntimeException if input row object is null (codegen path)[0m
[32m- RowEncoder should throw RuntimeException if input row object is null (interpreted path)[0m
[32m- RowEncoder should validate external type (codegen path)[0m
[32m- RowEncoder should validate external type (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = false), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = false), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = false), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = false), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = false), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = false), nullable = false (interpreted path)[0m
[32mSchemaUtilsSuite:[0m
[32m- Check column name duplication in case-sensitive cases[0m
[32m- Check column name duplication in case-insensitive cases[0m
[32m- Check no exception thrown for valid schemas[0m
[32mOptimizerExtendableSuite:[0m
[32m- Extending batches possible[0m
[32mResolveInlineTablesSuite:[0m
[32m- validate inputs are foldable[0m
[32m- validate input dimensions[0m
[32m- do not fire the rule if not all expressions are resolved[0m
[32m- convert[0m
[32m- convert TimeZoneAwareExpression[0m
[32m- nullability inference in convert[0m
[32mPushProjectThroughUnionSuite:[0m
[32m- SPARK-25450 PushProjectThroughUnion rule uses the same exprId for project expressions in each Union child, causing mistakes in constant propagation[0m
[32mReusableStringReaderSuite:[0m
[32m- empty reader[0m
[32m- mark reset[0m
[32m- skip[0m
[32mLimitPushdownSuite:[0m
[32m- Union: limit to each side[0m
[32m- Union: limit to each side with constant-foldable limit expressions[0m
[32m- Union: limit to each side with the new limit number[0m
[32m- Union: no limit to both sides if children having smaller limit values[0m
[32m- Union: limit to each sides if children having larger limit values[0m
[32m- left outer join[0m
[32m- left outer join and left sides are limited[0m
[32m- left outer join and right sides are limited[0m
[32m- right outer join[0m
[32m- right outer join and right sides are limited[0m
[32m- right outer join and left sides are limited[0m
[32m- larger limits are not pushed on top of smaller ones in right outer join[0m
[32m- full outer join where neither side is limited and both sides have same statistics[0m
[32m- full outer join where neither side is limited and left side has larger statistics[0m
[32m- full outer join where neither side is limited and right side has larger statistics[0m
[32m- full outer join where both sides are limited[0m
[32mTimeWindowSuite:[0m
[32m- time window is unevaluable[0m
[32m- blank intervals throw exception[0m
[32m- invalid intervals throw exception[0m
[32m- intervals greater than a month throws exception[0m
[32m- interval strings work with and without 'interval' prefix and return microseconds[0m
[32m- SPARK-21590: Start time works with negative values and return microseconds[0m
[32m- parse sql expression for duration in microseconds - string[0m
[32m- parse sql expression for duration in microseconds - integer[0m
[32m- parse sql expression for duration in microseconds - long[0m
[32m- parse sql expression for duration in microseconds - invalid interval[0m
[32m- parse sql expression for duration in microseconds - invalid expression[0m
[32m- SPARK-16837: TimeWindow.apply equivalent to TimeWindow constructor[0m
[32mBitwiseExpressionsSuite:[0m
[32m- BitwiseNOT[0m
[32m- BitwiseAnd[0m
[32m- BitwiseOr[0m
[32m- BitwiseXor[0m
[32mUnsafeRowConverterSuite:[0m
[32m- basic conversion with only primitive types with CODEGEN_ONLY[0m
[32m- basic conversion with only primitive types with NO_CODEGEN[0m
[32m- basic conversion with primitive, string and binary types with CODEGEN_ONLY[0m
[32m- basic conversion with primitive, string and binary types with NO_CODEGEN[0m
[32m- basic conversion with primitive, string, date and timestamp types with CODEGEN_ONLY[0m
[32m- basic conversion with primitive, string, date and timestamp types with NO_CODEGEN[0m
[32m- null handling with CODEGEN_ONLY[0m
[32m- null handling with NO_CODEGEN[0m
[32m- NaN canonicalization with CODEGEN_ONLY[0m
[32m- NaN canonicalization with NO_CODEGEN[0m
[32m- basic conversion with struct type with CODEGEN_ONLY[0m
[32m- basic conversion with struct type with NO_CODEGEN[0m
[32m- basic conversion with array type with CODEGEN_ONLY[0m
[32m- basic conversion with array type with NO_CODEGEN[0m
[32m- basic conversion with map type with CODEGEN_ONLY[0m
[32m- basic conversion with map type with NO_CODEGEN[0m
[32m- basic conversion with struct and array with CODEGEN_ONLY[0m
[32m- basic conversion with struct and array with NO_CODEGEN[0m
[32m- basic conversion with struct and map with CODEGEN_ONLY[0m
[32m- basic conversion with struct and map with NO_CODEGEN[0m
[32m- basic conversion with array and map with CODEGEN_ONLY[0m
[32m- basic conversion with array and map with NO_CODEGEN[0m
[36mRun completed in 14 minutes, 52 seconds.[0m
[36mTotal number of tests run: 3052[0m
[36mSuites: completed 183, aborted 0[0m
[36mTests: succeeded 3050, failed 2, canceled 0, ignored 3, pending 0[0m
[31m*** 2 TESTS FAILED ***[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------< [0;36morg.apache.spark:spark-sql_2.11[0;1m >-------------------[m
[[1;34mINFO[m] [1mBuilding Spark Project SQL 2.4.4                                 [14/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/sql/core/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/sql/core/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 4 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:42:20 AM [20.777s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mbuild-helper-maven-plugin:3.0.0:add-test-source[m [1m(add-scala-test-sources)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Test Source directory: /home/test/spark/sql/core/src/test/gen-java added.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 523 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/com/h2database/h2/1.4.195/h2-1.4.195.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/it/unimi/dsi/fastutil/7.0.13/fastutil-7.0.13.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/core/target/scala-2.11/test-classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/mysql/mysql-connector-java/5.1.38/mysql-connector-java-5.1.38.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/postgresql/postgresql/9.4.1207.jre7/postgresql-9.4.1207.jre7.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/parquet/parquet-avro/1.10.1/parquet-avro-1.10.1.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/spark/sql/catalyst/target/spark-catalyst_2.11-2.4.4-tests.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:42:41 AM [19.993s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.0.2:test-jar[m [1m(prepare-test-jar)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/test/spark/sql/core/target/spark-sql_2.11-2.4.4-tests.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-sql_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-sql_2.11[0;1m ---[m
[36mDiscovery starting.[0m
21:43:06.678 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[36mDiscovery completed in 29 seconds, 902 milliseconds.[0m
[36mRun starting. Expected test count is: 4775[0m
[32mSQLQuerySuite:[0m
21:43:20.897 WARN org.apache.spark.sql.SparkSession: An existing Spark session exists as the active or default session.
This probably means another suite leaked it. Attempting to stop it before continuing.
This existing Spark session was created at:

org.apache.spark.sql.execution.benchmark.FilterPushdownBenchmark.<init>(FilterPushdownBenchmark.scala:59)
java.lang.J9VMInternals.newInstanceImpl(Native Method)
java.lang.Class.newInstance(Class.java:1847)
org.scalatest.tools.DiscoverySuite$.getSuiteInstance(DiscoverySuite.scala:66)
org.scalatest.tools.DiscoverySuite$$anonfun$1.apply(DiscoverySuite.scala:38)
org.scalatest.tools.DiscoverySuite$$anonfun$1.apply(DiscoverySuite.scala:37)
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
scala.collection.Iterator$class.foreach(Iterator.scala:891)
scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
scala.collection.AbstractIterable.foreach(Iterable.scala:54)
scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
scala.collection.AbstractTraversable.map(Traversable.scala:104)
org.scalatest.tools.DiscoverySuite.<init>(DiscoverySuite.scala:37)
org.scalatest.tools.Runner$.genDiscoSuites$1(Runner.scala:1165)
org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1250)
org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)

         
21:43:27.161 WARN org.apache.spark.util.SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes
[32m- SPARK-8010: promote numeric to string[0m
[32m- show functions[0m
[32m- describe functions[0m
[32m- SPARK-14415: All functions should have own descriptions[0m
[32m- SPARK-6743: no columns from cache[0m
[32m- self join with aliases[0m
[32m- support table.star[0m
[32m- self join with alias in agg[0m
[32m- SPARK-8668 expr function[0m
[32m- SPARK-4625 support SORT BY in SimpleSQLParser & DSL[0m
[32m- SPARK-7158 collect and take return different results[0m
[32m- grouping on nested fields[0m
[32m- SPARK-6201 IN type conversion[0m
[32m- SPARK-11226 Skip empty line in json file[0m
[32m- SPARK-8828 sum should return null if all input values are null[0m
[32m- aggregation with codegen[0m
[32m- Add Parser of SQL COALESCE()[0m
[32m- SPARK-3176 Added Parser of SQL LAST()[0m
[32m- SPARK-2041 column name equals tablename[0m
[32m- SQRT[0m
[32m- SQRT with automatic string casts[0m
[32m- SPARK-2407 Added Parser of SQL SUBSTR()[0m
[32m- SPARK-3173 Timestamp support in the parser[0m
[32m- left semi greater than predicate[0m
[32m- left semi greater than predicate and equal operator[0m
[32m- select *[0m
[32m- simple select[0m
[32m- external sorting[0m
[32m- CTE feature[0m
[32m- Allow only a single WITH clause per query[0m
[32m- date row[0m
[32m- from follow multiple brackets[0m
[32m- average[0m
[32m- average overflow[0m
[32m- count[0m
[32m- count distinct[0m
21:44:03.110 WARN org.apache.spark.util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
[32m- approximate count distinct[0m
[32m- approximate count distinct with user provided standard deviation[0m
[32m- null count[0m
[32m- count of empty table[0m
[32m- inner join where, one match per row[0m
[32m- inner join ON, one match per row[0m
[32m- inner join, where, multiple matches[0m
[32m- inner join, no matches[0m
[32m- big inner join, 4 matches per row[0m
[32m- cartesian product join[0m
[32m- left outer join[0m
[32m- right outer join[0m
[32m- full outer join[0m
[32m- SPARK-11111 null-safe join should not use cartesian product[0m
[32m- SPARK-3349 partitioning after limit[0m
[32m- mixed-case keywords[0m
[32m- select with table name as qualifier[0m
[32m- inner join ON with table name as qualifier[0m
[32m- qualified select with inner join ON with table name as qualifier[0m
[32m- system function upper()[0m
[32m- system function lower()[0m
[32m- UNION[0m
[32m- UNION with column mismatches[0m
[32m- EXCEPT[0m
[32m- MINUS[0m
[32m- INTERSECT[0m
[32m- SET commands semantics using sql()[0m
[32m- SPARK-19218 SET command should show a result in a sorted order[0m
[32m- SPARK-19218 `SET -v` should not fail with null value configuration[0m
21:44:17.676 WARN org.apache.spark.sql.execution.command.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
21:44:17.687 WARN org.apache.spark.sql.execution.command.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
21:44:17.692 WARN org.apache.spark.sql.execution.command.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
[32m- SET commands with illegal or inappropriate argument[0m
21:44:17.703 WARN org.apache.spark.sql.execution.command.SetCommand: Property mapreduce.job.reduces is Hadoop's property, automatically converted to spark.sql.shuffle.partitions instead.
21:44:17.729 WARN org.apache.spark.sql.execution.command.SetCommand: Property mapreduce.job.reduces is Hadoop's property, automatically converted to spark.sql.shuffle.partitions instead.
[32m- SET mapreduce.job.reduces automatically converted to spark.sql.shuffle.partitions[0m
[32m- apply schema[0m
[32m- SPARK-3423 BETWEEN[0m
[32m- SPARK-17863: SELECT distinct does not work correctly if order by missing attribute[0m
[32m- cast boolean to string[0m
[32m- metadata is propagated correctly[0m
[32m- SPARK-3371 Renaming a function expression with group by gives error[0m
[32m- SPARK-3813 CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END[0m
[32m- SPARK-3813 CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END[0m
[32m- SPARK-16748: SparkExceptions during planning should not wrapped in TreeNodeException[0m
[32m- Multiple join[0m
[32m- SPARK-3483 Special chars in column names[0m
[32m- SPARK-3814 Support Bitwise & operator[0m
[32m- SPARK-3814 Support Bitwise | operator[0m
[32m- SPARK-3814 Support Bitwise ^ operator[0m
[32m- SPARK-3814 Support Bitwise ~ operator[0m
[32m- SPARK-4120 Join of multiple tables does not work in SparkSQL[0m
[32m- SPARK-4154 Query does not work if it has 'not between' in Spark SQL and HQL[0m
[32m- SPARK-4207 Query which has syntax like 'not like' is not working in Spark SQL[0m
[32m- SPARK-4322 Grouping field with struct field as sub expression[0m
[32m- SPARK-4432 Fix attribute reference resolution error when using ORDER BY[0m
[32m- oder by asc by default when not specify ascending and descending[0m
[32m- Supporting relational operator '<=>' in Spark SQL[0m
[32m- Multi-column COUNT(DISTINCT ...)[0m
[32m- SPARK-4699 case sensitivity SQL query[0m
[32m- SPARK-6145: ORDER BY test for nested fields[0m
[32m- SPARK-6145: special cases[0m
[32m- SPARK-6898: complete support for special chars in column names[0m
[32m- SPARK-6583 order by aggregated function[0m
[32m- SPARK-7952: fix the equality check between boolean and numeric types[0m
[32m- SPARK-7067: order by queries for complex ExtractValue chain[0m
[32m- SPARK-8782: ORDER BY NULL[0m
[32m- SPARK-8837: use keyword in column name[0m
[32m- SPARK-8753: add interval type[0m
[32m- SPARK-8945: add and subtract expressions for interval type[0m
[32m- aggregation with codegen updates peak execution memory[0m
[32m- SPARK-10215 Div of Decimal returns null[0m
[32m- precision smaller than scale[0m
[32m- external sorting updates peak execution memory[0m
[32m- SPARK-9511: error with table starting with number[0m
[32m- specifying database name for a temporary view is not allowed[0m
[32m- SPARK-10130 type coercion for IF should have children resolved first[0m
[32m- SPARK-10389: order by non-attribute grouping expression on Aggregate[0m
[32m- SPARK-23281: verify the correctness of sort direction on composite order by clause[0m
[32m- run sql directly on files[0m
[32m- SortMergeJoin returns wrong results when using UnsafeRows[0m
[32m- SPARK-11303: filter should not be pushed down into sample[0m
[32m- Struct Star Expansion[0m
[32m- Struct Star Expansion - Name conflict[0m
[32m- Star Expansion - group by[0m
[32m- Star Expansion - table with zero column[0m
[32m- Common subexpression elimination[0m
[32m- SPARK-10707: nullability should be correctly propagated through set operations (1)[0m
[32m- SPARK-10707: nullability should be correctly propagated through set operations (2)[0m
[32m- filter on a grouping column that is not presented in SELECT[0m
[32m- SPARK-13056: Null in map value causes NPE[0m
[32m- hash function[0m
[32m- join with using clause[0m
[32m- SPARK-15327: fail to compile generated code with complex data structure[0m
[32m- data source table created in InMemoryCatalog should be able to read/write[0m
[32m- Eliminate noop ordinal ORDER BY[0m
[32m- check code injection is prevented[0m
21:44:51.292 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:51.326 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:51.546 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:51.570 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:51.718 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:51.741 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:52.276 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:52.330 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:52.594 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:52.606 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:52.826 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:52.849 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:53.042 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:53.062 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:53.215 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:53.229 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:53.508 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:53.526 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:53.639 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:53.650 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:53.732 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:44:53.755 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
[32m- SPARK-15752 optimize metadata only query for datasource table[0m
[32m- SPARK-16975: Column-partition path starting '_' should be handled correctly[0m
[32m- SPARK-16644: Aggregate should not put aggregate expressions to constraints[0m
[32m- SPARK-16674: field names containing dots for both fields and partitioned fields[0m
[32m- SPARK-17515: CollectLimit.execute() should perform per-partition limits[0m
[32m- CREATE TABLE USING should not fail if a same-name temp view exists[0m
[32m- SPARK-18053: ARRAY equality is broken[0m
[32m- SPARK-19157: should be able to change spark.sql.runSQLOnFiles at runtime[0m
[32m- should be able to resolve a persistent view[0m
[32m- SPARK-19059: read file based table whose name starts with underscore[0m
[32m- SPARK-19334: check code injection is prevented[0m
[32m- SPARK-19650: An action on a Command should not trigger a Spark job[0m
[32m- SPARK-20164: AnalysisException should be tolerant to null query plan[0m
[32m- SPARK-12868: Allow adding jars from hdfs [0m
[32m- RuntimeReplaceable functions should not take extra parameters[0m
[32m- SPARK-21228: InSet incorrect handling of structs[0m
[32m- SPARK-21247: Allow case-insensitive type equality in Set operation[0m
[32m- SPARK-21335: support un-aliased subquery[0m
[32m- SPARK-21743: top-most limit should not cause memory leak[0m
[32m- SPARK-21652: rule confliction of InferFiltersFromConstraints and ConstantPropagation[0m
[32m- SPARK-23079: constraints should be inferred correctly with aliases[0m
[32m- SRARK-22266: the same aggregate function was calculated multiple times[0m
[32m- Non-deterministic aggregate functions should not be deduplicated[0m
21:45:00.340 WARN org.apache.spark.sql.execution.datasources.DataSource: Found duplicate column(s) in the data schema and the partition schema: `p`;
21:45:00.340 WARN org.apache.spark.sql.execution.command.CreateDataSourceTableCommand: It is not recommended to create a table with overlapped data and partition columns, as Spark cannot store a valid table schema and has to infer it at runtime, which hurts performance. Please check your data files and remove the partition columns in it.
21:45:00.412 WARN org.apache.spark.sql.execution.datasources.DataSource: Found duplicate column(s) in the data schema and the partition schema: `p`;
[32m- SPARK-22356: overlapped columns between data and partition schema in data source tables[0m
[32m- SPARK-24696 ColumnPruning rule fails to remove extra Project[0m
[32m- SPARK-24940: coalesce and repartition hint[0m
[32m- SPARK-25084: 'distribute by' on multiple columns may lead to codegen issue[0m
[32m- SPARK-25144 'distinct' causes memory leak[0m
[32m- SPARK-25454: decimal division with negative scale[0m
[32m- SPARK-25988: self join with aliases on partitioned tables #1[0m
[32m- SPARK-25988: self join with aliases on partitioned tables #2[0m
[32m- SPARK-26366: verify ReplaceExceptWithFilter[0m
[32m- SPARK-26402: accessing nested fields with different cases in case insensitive mode[0m
21:45:08.559 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:45:08.595 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:45:09.232 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
21:45:09.246 WARN org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery: Since configuration `spark.sql.optimizer.metadataOnly` is enabled, Spark will scan partition-level metadata without scanning data files. This could result in wrong results when the partition metadata exists but the inclusive data files are empty.
[32m- SPARK-26709: OptimizeMetadataOnlyQuery does not handle empty records correctly[0m
21:45:11.345 WARN org.apache.spark.storage.BlockManager: Putting block rdd_4983_1 failed due to exception io.airlift.compress.MalformedInputException: Malformed input: offset=47.
21:45:11.346 WARN org.apache.spark.storage.BlockManager: Block rdd_4983_1 could not be removed as it was not found on disk or in memory
21:45:11.346 ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 1278.0 (TID 3258)
io.airlift.compress.MalformedInputException: Malformed input: offset=47
	at io.airlift.compress.snappy.SnappyRawDecompressor.uncompressAll(SnappyRawDecompressor.java:153)
	at io.airlift.compress.snappy.SnappyRawDecompressor.decompress(SnappyRawDecompressor.java:53)
	at io.airlift.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:44)
	at org.apache.orc.impl.AircompressorCodec.decompress(AircompressorCodec.java:90)
	at org.apache.orc.impl.SnappyCodec.decompress(SnappyCodec.java:44)
	at org.apache.orc.impl.InStream$CompressedStream.readHeader(InStream.java:233)
	at org.apache.orc.impl.InStream$CompressedStream.ensureUncompressed(InStream.java:263)
	at org.apache.orc.impl.InStream$CompressedStream.read(InStream.java:250)
	at java.io.InputStream.read(InputStream.java:101)
	at com.google.protobuf25.CodedInputStream.refillBuffer(CodedInputStream.java:737)
	at com.google.protobuf25.CodedInputStream.isAtEnd(CodedInputStream.java:701)
	at com.google.protobuf25.CodedInputStream.readTag(CodedInputStream.java:99)
	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16289)
	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16253)
	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16400)
	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16395)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:89)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:95)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:49)
	at org.apache.orc.OrcProto$Footer.parseFrom(OrcProto.java:16801)
	at org.apache.orc.impl.ReaderImpl.extractFooter(ReaderImpl.java:424)
	at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:608)
	at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:367)
	at org.apache.orc.OrcFile.createReader(OrcFile.java:342)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:181)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:125)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
21:45:11.352 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 1278.0 (TID 3258, localhost, executor driver): io.airlift.compress.MalformedInputException: Malformed input: offset=47
	at io.airlift.compress.snappy.SnappyRawDecompressor.uncompressAll(SnappyRawDecompressor.java:153)
	at io.airlift.compress.snappy.SnappyRawDecompressor.decompress(SnappyRawDecompressor.java:53)
	at io.airlift.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:44)
	at org.apache.orc.impl.AircompressorCodec.decompress(AircompressorCodec.java:90)
	at org.apache.orc.impl.SnappyCodec.decompress(SnappyCodec.java:44)
	at org.apache.orc.impl.InStream$CompressedStream.readHeader(InStream.java:233)
	at org.apache.orc.impl.InStream$CompressedStream.ensureUncompressed(InStream.java:263)
	at org.apache.orc.impl.InStream$CompressedStream.read(InStream.java:250)
	at java.io.InputStream.read(InputStream.java:101)
	at com.google.protobuf25.CodedInputStream.refillBuffer(CodedInputStream.java:737)
	at com.google.protobuf25.CodedInputStream.isAtEnd(CodedInputStream.java:701)
	at com.google.protobuf25.CodedInputStream.readTag(CodedInputStream.java:99)
	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16289)
	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16253)
	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16400)
	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16395)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:89)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:95)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:49)
	at org.apache.orc.OrcProto$Footer.parseFrom(OrcProto.java:16801)
	at org.apache.orc.impl.ReaderImpl.extractFooter(ReaderImpl.java:424)
	at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:608)
	at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:367)
	at org.apache.orc.OrcFile.createReader(OrcFile.java:342)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:181)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:125)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)

21:45:11.352 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 1278.0 failed 1 times; aborting job
21:45:11.386 WARN org.apache.spark.storage.BlockManager: Putting block rdd_4983_0 failed due to exception io.airlift.compress.MalformedInputException: Malformed input: offset=47.
21:45:11.387 WARN org.apache.spark.storage.BlockManager: Block rdd_4983_0 could not be removed as it was not found on disk or in memory
21:45:11.406 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 1278.0 (TID 3257, localhost, executor driver): TaskKilled (Stage cancelled)
[31m- SPARK-28156: self-join should not miss cached view *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Exception thrown in awaitResult:[0m
[31m  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)[0m
[31m  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)[0m
[31m  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)[0m
[31m  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)[0m
[31m  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)[0m
[31m  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)[0m
[31m  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[31m  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)[0m
[31m  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)[0m
[31m  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)[0m
[31m  ...[0m
[31m  Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1278.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1278.0 (TID 3258, localhost, executor driver): io.airlift.compress.MalformedInputException: Malformed input: offset=47[0m
[31m	at io.airlift.compress.snappy.SnappyRawDecompressor.uncompressAll(SnappyRawDecompressor.java:153)[0m
[31m	at io.airlift.compress.snappy.SnappyRawDecompressor.decompress(SnappyRawDecompressor.java:53)[0m
[31m	at io.airlift.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:44)[0m
[31m	at org.apache.orc.impl.AircompressorCodec.decompress(AircompressorCodec.java:90)[0m
[31m	at org.apache.orc.impl.SnappyCodec.decompress(SnappyCodec.java:44)[0m
[31m	at org.apache.orc.impl.InStream$CompressedStream.readHeader(InStream.java:233)[0m
[31m	at org.apache.orc.impl.InStream$CompressedStream.ensureUncompressed(InStream.java:263)[0m
[31m	at org.apache.orc.impl.InStream$CompressedStream.read(InStream.java:250)[0m
[31m	at java.io.InputStream.read(InputStream.java:101)[0m
[31m	at com.google.protobuf25.CodedInputStream.refillBuffer(CodedInputStream.java:737)[0m
[31m	at com.google.protobuf25.CodedInputStream.isAtEnd(CodedInputStream.java:701)[0m
[31m	at com.google.protobuf25.CodedInputStream.readTag(CodedInputStream.java:99)[0m
[31m	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16289)[0m
[31m	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16253)[0m
[31m	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16400)[0m
[31m	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16395)[0m
[31m	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:89)[0m
[31m	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:95)[0m
[31m	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:49)[0m
[31m	at org.apache.orc.OrcProto$Footer.parseFrom(OrcProto.java:16801)[0m
[31m	at org.apache.orc.impl.ReaderImpl.extractFooter(ReaderImpl.java:424)[0m
[31m	at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:608)[0m
[31m	at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:367)[0m
[31m	at org.apache.orc.OrcFile.createReader(OrcFile.java:342)[0m
[31m	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:181)[0m
[31m	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)[0m
[31m	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)[0m
[31m	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)[0m
[31m	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)[0m
[31m	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)[0m
[31m	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[31m	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[31m	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)[0m
[31m	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:125)[0m
[31m	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)[0m
[31m	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)[0m
[31m	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)[0m
[31m	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)[0m
[31m	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)[0m
[31m	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)[0m
[31m	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)[0m
[31m	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: io.airlift.compress.MalformedInputException: Malformed input: offset=47[0m
[31m  at io.airlift.compress.snappy.SnappyRawDecompressor.uncompressAll(SnappyRawDecompressor.java:153)[0m
[31m  at io.airlift.compress.snappy.SnappyRawDecompressor.decompress(SnappyRawDecompressor.java:53)[0m
[31m  at io.airlift.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:44)[0m
[31m  at org.apache.orc.impl.AircompressorCodec.decompress(AircompressorCodec.java:90)[0m
[31m  at org.apache.orc.impl.SnappyCodec.decompress(SnappyCodec.java:44)[0m
[31m  at org.apache.orc.impl.InStream$CompressedStream.readHeader(InStream.java:233)[0m
[31m  at org.apache.orc.impl.InStream$CompressedStream.ensureUncompressed(InStream.java:263)[0m
[31m  at org.apache.orc.impl.InStream$CompressedStream.read(InStream.java:250)[0m
[31m  at java.io.InputStream.read(InputStream.java:101)[0m
[31m  at com.google.protobuf25.CodedInputStream.refillBuffer(CodedInputStream.java:737)[0m
[31m  ...[0m
21:45:11.586 WARN org.apache.spark.sql.SQLQuerySuite: 

===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.SQLQuerySuite, thread names: broadcast-exchange-19, broadcast-exchange-30, broadcast-exchange-37, broadcast-exchange-24, broadcast-exchange-35, broadcast-exchange-27, broadcast-exchange-20, broadcast-exchange-31, broadcast-exchange-16, broadcast-exchange-13, broadcast-exchange-26, broadcast-exchange-15, broadcast-exchange-33, broadcast-exchange-29, broadcast-exchange-17, broadcast-exchange-22, broadcast-exchange-25, broadcast-exchange-12, broadcast-exchange-36, broadcast-exchange-32, broadcast-exchange-21, broadcast-exchange-23, broadcast-exchange-38, broadcast-exchange-34, broadcast-exchange-14, broadcast-exchange-28, broadcast-exchange-18 =====

[32mMemorySourceStressSuite:[0m
[32m- memory stress test[0m
[32mSparkPlanSuite:[0m
[32m- SPARK-21619 execution of a canonicalized plan should fail[0m
[32m- SPARK-23731 plans should be canonicalizable after being (de)serialized[0m
[32m- SPARK-25357 SparkPlanInfo of FileScan contains nonEmpty metadata[0m
[32mStreamingAggregationSuite:[0m
21:45:18.223 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:18.227 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:18.278 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:18.304 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:18.448 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- simple count, update mode - state format version 1[0m
21:45:20.416 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:20.419 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:20.476 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:20.484 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:20.566 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- simple count, update mode - state format version 2[0m
[32m- count distinct - state format version 1[0m
[32m- count distinct - state format version 2[0m
21:45:23.783 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:23.817 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:23.875 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:23.916 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:23.951 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- simple count, complete mode - state format version 1[0m
21:45:25.695 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:25.699 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:25.739 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:25.743 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:25.810 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- simple count, complete mode - state format version 2[0m
[32m- simple count, append mode - state format version 1[0m
[32m- simple count, append mode - state format version 2[0m
21:45:27.916 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:27.919 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:27.969 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:27.995 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:28.032 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- sort after aggregate in complete mode - state format version 1[0m
21:45:30.463 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:30.490 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:30.531 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:30.543 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:30.589 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- sort after aggregate in complete mode - state format version 2[0m
[32m- state metrics - state format version 1[0m
[32m- state metrics - state format version 2[0m
[32m- multiple keys - state format version 1[0m
[32m- multiple keys - state format version 2[0m
[32m- midbatch failure - state format version 1[0m
[32m- midbatch failure - state format version 2[0m
[32m- typed aggregators - state format version 1[0m
[32m- typed aggregators - state format version 2[0m
21:45:43.422 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:43.425 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:43.450 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:43.456 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:43.509 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- prune results by current_time, complete mode - state format version 1[0m
21:45:45.566 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:45.570 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:45.640 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:45.643 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:45.714 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- prune results by current_time, complete mode - state format version 2[0m
21:45:48.042 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:48.045 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:48.134 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:48.139 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:48.218 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- prune results by current_date, complete mode - state format version 1[0m
21:45:50.457 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:50.492 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:50.536 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:50.571 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:50.631 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- prune results by current_date, complete mode - state format version 2[0m
[32m- SPARK-19690: do not convert batch aggregation in streaming query to streaming - state format version 1[0m
[32m- SPARK-19690: do not convert batch aggregation in streaming query to streaming - state format version 2[0m
[32m- SPARK-21977: coalesce(1) with 0 partition RDD should be repartitioned to 1 - state format version 1[0m
[32m- SPARK-21977: coalesce(1) with 0 partition RDD should be repartitioned to 1 - state format version 2[0m
21:45:57.534 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:57.545 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:57.614 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:57.618 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:57.715 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- SPARK-21977: coalesce(1) with aggregation should still be repartitioned when it has non-empty grouping keys - state format version 1[0m
21:45:59.317 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:59.328 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:59.450 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:59.460 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:45:59.538 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- SPARK-21977: coalesce(1) with aggregation should still be repartitioned when it has non-empty grouping keys - state format version 2[0m
[32m- SPARK-22230: last should change with new batches - state format version 1[0m
[32m- SPARK-22230: last should change with new batches - state format version 2[0m
[32m- SPARK-23004: Ensure that TypedImperativeAggregate functions do not throw errors - state format version 1[0m
[32m- SPARK-23004: Ensure that TypedImperativeAggregate functions do not throw errors - state format version 2[0m
21:46:04.420 WARN org.apache.spark.sql.execution.streaming.OffsetSeqMetadata: Conf 'spark.sql.streaming.multipleWatermarkPolicy' was not found in the offset log, using default value 'min'
21:46:04.421 WARN org.apache.spark.sql.execution.streaming.OffsetSeqMetadata: Conf 'spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion' was not found in the offset log, using default value '1'
21:46:04.421 WARN org.apache.spark.sql.execution.streaming.OffsetSeqMetadata: Conf 'spark.sql.streaming.aggregation.stateFormatVersion' was not found in the offset log, using default value '1'
21:46:04.710 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:04.714 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:04.819 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:04.865 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:04.924 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[31m- simple count, update mode - recovery from checkpoint uses state format version 1 *** FAILED ***[0m
[31m  == Results ==[0m
[31m  !== Correct Answer - 3 ==   == Spark Answer - 3 ==[0m
[31m  !struct<_1:int,_2:int>      struct<value:int,count(1):bigint>[0m
[31m   [1,1]                      [1,1][0m
[31m  ![2,2]                      [2,1][0m
[31m  ![3,3]                      [3,1][0m
      
  
[31m  == Progress ==[0m
[31m     StartStream(ProcessingTime(0),org.apache.spark.util.SystemClock@84a0990d,Map(spark.sql.streaming.aggregation.stateFormatVersion -> 2),/home/test/spark/sql/core/target/tmp/spark-7aa31070-888a-409c-af03-1a16c781c9a7)[0m
[31m     AddData to MemoryStream[value#13182]: 3,2,1[0m
[31m  => CheckLastBatch: [3,3],[2,2],[1,1][0m
[31m     AssertOnQuery(<condition>, name)[0m
[31m     AddData to MemoryStream[value#13182]: 4,4,4,4[0m
[31m     CheckLastBatch: [4,4][0m
  
[31m  == Stream ==[0m
[31m  Output Mode: Update[0m
[31m  Stream state: {MemoryStream[value#13182]: 2}[0m
[31m  Thread state: alive[0m
[31m  Thread stack trace: java.lang.Thread.sleep(Native Method)[0m
[31m  java.lang.Thread.sleep(Thread.java:943)[0m
[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:215)[0m
[31m  org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)[0m
[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)[0m
[31m  org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)[0m
[31m  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)[0m
  
  
[31m  == Sink ==[0m
[31m  2: [3,1] [1,1] [2,1][0m
  
  
[31m  == Plan ==[0m
[31m  == Parsed Logical Plan ==[0m
[31m  Aggregate [value#13182], [value#13182, count(1) AS count(1)#13187L][0m
[31m  +- Project [value#13209 AS value#13182][0m
[31m     +- Streaming RelationV2 MemoryStreamDataSource$[value#13209][0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  value: int, count(1): bigint[0m
[31m  Aggregate [value#13182], [value#13182, count(1) AS count(1)#13187L][0m
[31m  +- Project [value#13209 AS value#13182][0m
[31m     +- Streaming RelationV2 MemoryStreamDataSource$[value#13209][0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Aggregate [value#13209], [value#13209, count(1) AS count(1)#13187L][0m
[31m  +- Streaming RelationV2 MemoryStreamDataSource$[value#13209][0m
  
[31m  == Physical Plan ==[0m
[31m  *(3) HashAggregate(keys=[value#13209], functions=[count(1)], output=[value#13209, count(1)#13187L])[0m
[31m  +- StateStoreSave [value#13209], state info [ checkpoint = file:/home/test/spark/sql/core/target/tmp/spark-7aa31070-888a-409c-af03-1a16c781c9a7/state, runId = ef98f892-0b14-4ff7-b332-0ffc94776f18, opId = 0, ver = 2, numPartitions = 5], Update, 0, 1[0m
[31m     +- *(2) HashAggregate(keys=[value#13209], functions=[merge_count(1)], output=[value#13209, count#13211L])[0m
[31m        +- StateStoreRestore [value#13209], state info [ checkpoint = file:/home/test/spark/sql/core/target/tmp/spark-7aa31070-888a-409c-af03-1a16c781c9a7/state, runId = ef98f892-0b14-4ff7-b332-0ffc94776f18, opId = 0, ver = 2, numPartitions = 5], 1[0m
[31m           +- Exchange hashpartitioning(value#13209, 5)[0m
[31m              +- *(1) HashAggregate(keys=[value#13209], functions=[merge_count(1)], output=[value#13209, count#13211L])[0m
[31m                 +- *(1) HashAggregate(keys=[value#13209], functions=[partial_count(1)], output=[value#13209, count#13211L])[0m
[31m                    +- *(1) Project [value#13209][0m
[31m                       +- *(1) ScanV2 MemoryStreamDataSource$[value#13209] (StreamTest.scala:450)[0m
21:46:05.196 WARN org.apache.spark.sql.streaming.StreamingAggregationSuite: 

===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.streaming.StreamingAggregationSuite, thread names: state-store-maintenance-task, block-manager-ask-thread-pool-39, broadcast-exchange-41, block-manager-slave-async-thread-pool-68, broadcast-exchange-42, block-manager-ask-thread-pool-84, block-manager-slave-async-thread-pool-50, broadcast-exchange-39, block-manager-ask-thread-pool-81, block-manager-ask-thread-pool-64, block-manager-slave-async-thread-pool-97, broadcast-exchange-40 =====

[32mSortBenchmark:[0m
[33m- sort !!! IGNORED !!![0m
[32mDateFunctionsSuite:[0m
[32m- function current_date[0m
[32m- function current_timestamp and now[0m
[32m- timestamp comparison with date strings[0m
[32m- date comparison with date strings[0m
[32m- date format[0m
[32m- year[0m
[32m- quarter[0m
[32m- month[0m
[32m- dayofmonth[0m
[32m- dayofyear[0m
[32m- hour[0m
[32m- minute[0m
[32m- second[0m
[32m- weekofyear[0m
[32m- function date_add[0m
[32m- function date_sub[0m
[32m- time_add[0m
[32m- time_sub[0m
[32m- function add_months[0m
[32m- function months_between[0m
[32m- function last_day[0m
[32m- function next_day[0m
[32m- function to_date[0m
[32m- function trunc[0m
[32m- function date_trunc[0m
[32m- from_unixtime[0m
[32m- unix_timestamp[0m
[32m- to_unix_timestamp[0m
[32m- to_timestamp[0m
[32m- datediff[0m
[32m- from_utc_timestamp with literal zone[0m
[32m- from_utc_timestamp with column zone[0m
[32m- to_utc_timestamp with literal zone[0m
[32m- to_utc_timestamp with column zone[0m
[32mStateStoreSuite:[0m
21:46:20.413 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:20.414 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:20.432 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:20.457 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:20.501 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:20.524 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- get, put, remove, commit, and all data iterator[0m
[32m- removing while iterating[0m
[32m- abort[0m
21:46:20.696 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:20.716 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:20.745 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 3 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- getStore with invalid versions[0m
21:46:20.839 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:20.920 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- two concurrent StateStores - one for read-only and one for read-write[0m
21:46:20.998 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.040 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.121 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 3 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- retaining only two latest versions when MAX_BATCHES_TO_RETAIN_IN_MEMORY set to 2[0m
21:46:21.189 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.236 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.237 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.273 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- failure after committing with MAX_BATCHES_TO_RETAIN_IN_MEMORY set to 1[0m
21:46:21.344 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.349 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.427 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- no cache data with MAX_BATCHES_TO_RETAIN_IN_MEMORY set to 0[0m
21:46:21.521 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.547 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.718 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 6 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.779 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 6 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:21.824 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 6 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:22.204 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 20 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:22.270 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 20 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- snapshotting[0m
21:46:22.977 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 20 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:23.013 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 19 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- cleaning[0m
[32m- SPARK-19677: Committing a delta file atop an existing one should not fail on HDFS[0m
21:46:23.991 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 6 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:24.025 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 6 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:24.044 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 5 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:24.078 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 5 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:24.110 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 5 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- corrupted file handling[0m
[32m- reports memory usage[0m
[32m- reports memory usage on current version[0m
[32m- StateStore.get[0m
[32m- maintenance[0m
[32m- SPARK-18342: commit fails when rename fails[0m
[32m- SPARK-18416: do not create temp delta file until the store is updated[0m
21:46:26.886 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- SPARK-21145: Restarted queries create new provider instances[0m
[32m- error writing [version].delta cancels the output stream[0m
21:46:27.392 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
21:46:27.399 WARN org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
[32m- expose metrics with custom metrics to StateStoreMetrics[0m
21:46:27.404 WARN org.apache.spark.sql.SparkSession: An existing Spark session exists as the active or default session.
This probably means another suite leaked it. Attempting to stop it before continuing.
This existing Spark session was created at:

org.apache.spark.sql.execution.streaming.state.StateStoreSuite$$anonfun$32.apply(StateStoreSuite.scala:568)
org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
org.scalatest.Transformer.apply(Transformer.scala:22)
org.scalatest.Transformer.apply(Transformer.scala:20)
org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
org.apache.spark.sql.execution.streaming.state.StateStoreSuite.org$scalatest$BeforeAndAfter$$super$runTest(StateStoreSuite.scala:48)
org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:203)
org.apache.spark.sql.execution.streaming.state.StateStoreSuite.runTest(StateStoreSuite.scala:48)
org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
scala.collection.immutable.List.foreach(List.scala:392)

         
[32mMemorySinkV2Suite:[0m
[32m- data writer[0m
[32m- streaming writer[0m
[32mForeachBatchSinkSuite:[0m
[32m- foreachBatch with non-stateful query[0m
[32m- foreachBatch with stateful query in update mode[0m
[32m- foreachBatch with stateful query in complete mode[0m
[32m- foreachBatchSink does not affect metric generation[0m
[32m- throws errors in invalid situations[0m
21:46:31.342 WARN org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite: 

===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.execution.streaming.sources.ForeachBatchSinkSuite, thread names: state-store-maintenance-task, block-manager-ask-thread-pool-8, block-manager-ask-thread-pool-16, block-manager-ask-thread-pool-1, block-manager-ask-thread-pool-12, block-manager-ask-thread-pool-19, block-manager-ask-thread-pool-11, block-manager-ask-thread-pool-7, block-manager-ask-thread-pool-18, element-tracking-store-worker =====

[32mBucketedReadWithoutHiveSupportSuite:[0m
[32m- read bucketed data[0m
[32m- read partitioning bucketed tables with bucket pruning filters[0m
[32m- read non-partitioning bucketed tables with bucket pruning filters[0m
[32m- read partitioning bucketed tables having null in bucketing key[0m
[32m- read partitioning bucketed tables having composite filters[0m
[32m- read bucketed table without filters[0m
[32m- avoid shuffle when join 2 bucketed tables[0m
[33m- avoid shuffle when join keys are a super-set of bucket keys !!! IGNORED !!![0m
[32m- only shuffle one side when join bucketed table and non-bucketed table[0m
[32m- only shuffle one side when 2 bucketed tables have different bucket number[0m
[32m- only shuffle one side when 2 bucketed tables have different bucket keys[0m
[32m- shuffle when join keys are not equal to bucket keys[0m
[32m- shuffle when join 2 bucketed tables with bucketing disabled[0m
[32m- check sort and shuffle when bucket and sort columns are join keys[0m
[32m- avoid shuffle and sort when sort columns are a super set of join keys[0m
[32m- only sort one side when sort columns are different[0m
[32m- only sort one side when sort columns are same but their ordering is different[0m
[32m- avoid shuffle when grouping keys are equal to bucket keys[0m
[32m- avoid shuffle when grouping keys are a super-set of bucket keys[0m
[32m- SPARK-17698 Join predicates should not contain filter clauses[0m
[32m- SPARK-19122 Re-order join predicates if they match with the child's output partitioning[0m
[32m- SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns[0m
[32m- SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided[0m
[32m- error if there exists any malformed bucket files[0m
[32m- disable bucketing when the output doesn't contain all bucketing columns[0m
[33m- SPARK-27100 stack overflow: read data with large partitions !!! IGNORED !!![0m
21:48:25.529 WARN org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite: 

===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.sources.BucketedReadWithoutHiveSupportSuite, thread names: block-manager-slave-async-thread-pool-15, block-manager-slave-async-thread-pool-74, block-manager-slave-async-thread-pool-37, block-manager-slave-async-thread-pool-89, block-manager-slave-async-thread-pool-67, block-manager-slave-async-thread-pool-96, block-manager-slave-async-thread-pool-90, block-manager-slave-async-thread-pool-78, block-manager-slave-async-thread-pool-92, block-manager-slave-async-thread-pool-5, block-manager-slave-async-thread-pool-75, block-manager-slave-async-thread-pool-86, block-manager-slave-async-thread-pool-1, block-manager-slave-async-thread-pool-32, block-manager-slave-async-thread-pool-25, block-manager-slave-async-thread-pool-14, block-manager-slave-async-thread-pool-95, block-manager-slave-async-thread-pool-49, block-manager-slave-async-thread-pool-29, block-manager-slave-async-thread-pool-18, block-manager-slave-async-thread-pool-79, block-manager-slave-async-thread-pool-57, block-manager-slave-async-thread-pool-4, block-manager-slave-async-thread-pool-61, block-manager-slave-async-thread-pool-76, block-manager-slave-async-thread-pool-87, block-manager-slave-async-thread-pool-99, block-manager-slave-async-thread-pool-65, block-manager-slave-async-thread-pool-33, block-manager-slave-async-thread-pool-50, block-manager-slave-async-thread-pool-72, block-manager-slave-async-thread-pool-55, block-manager-slave-async-thread-pool-94, block-manager-slave-async-thread-pool-69, block-manager-slave-async-thread-pool-66, block-manager-slave-async-thread-pool-88, block-manager-slave-async-thread-pool-48, block-manager-slave-async-thread-pool-58, block-manager-slave-async-thread-pool-17, block-manager-slave-async-thread-pool-11, block-manager-slave-async-thread-pool-3, block-manager-slave-async-thread-pool-23, block-manager-slave-async-thread-pool-47, block-manager-slave-async-thread-pool-8, block-manager-slave-async-thread-pool-60, block-manager-slave-async-thread-pool-82, block-manager-slave-async-thread-pool-44, block-manager-slave-async-thread-pool-84, block-manager-slave-async-thread-pool-27, block-manager-slave-async-thread-pool-73, block-manager-slave-async-thread-pool-38, block-manager-slave-async-thread-pool-51, block-manager-slave-async-thread-pool-97, block-manager-slave-async-thread-pool-2, block-manager-slave-async-thread-pool-20, block-manager-slave-async-thread-pool-40, block-manager-slave-async-thread-pool-70, block-manager-slave-async-thread-pool-13, block-manager-slave-async-thread-pool-24, block-manager-slave-async-thread-pool-31, block-manager-slave-async-thread-pool-80 =====

[32mBasicWriteTaskStatsTrackerSuite:[0m
[32m- No files in run[0m
[32m- Missing File[0m
[32m- Empty filename is forwarded[0m
[32m- Null filename is only picked up in final status[0m
[32m- 0 byte file[0m
[32m- File with data[0m
[32m- Open file[0m
[32m- Two files[0m
[32m- Three files, last one empty[0m
[32m- Three files, one not found[0m
[32mArrowConvertersSuite:[0m
21:48:26.149 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ExceptionInInitializerError
	at java.lang.J9VMInternals.ensureError(J9VMInternals.java:146)
	at java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:135)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	... 22 more
21:48:26.155 ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:26.188 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ExceptionInInitializerError
	at java.lang.J9VMInternals.ensureError(J9VMInternals.java:146)
	at java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:135)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	... 22 more

21:48:26.190 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[31m- collect to arrow record batch *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ExceptionInInitializerError[0m
[31m	at java.lang.J9VMInternals.ensureError(J9VMInternals.java:146)[0m
[31m	at java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:135)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	... 22 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.ExceptionInInitializerError:[0m
[31m  at java.lang.J9VMInternals.ensureError(J9VMInternals.java:146)[0m
[31m  at java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:135)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:26.219 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:26.305 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 1.0 (TID 2)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:26.308 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 2, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:26.309 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
[31m- short conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:26.475 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 2.0 (TID 3)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:26.482 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:26.484 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
[31m- int conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:26.547 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 3.0 (TID 4)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:26.551 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 3.0 (TID 4, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:26.551 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
[31m- long conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 4, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:26.665 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 4.0 (TID 5)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:26.680 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 4.0 (TID 5, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:26.680 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
[31m- float conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 5, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:26.774 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 5.0 (TID 6)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:26.776 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 5.0 (TID 6, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:26.776 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
[31m- double conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 6, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:26.852 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 6.0 (TID 7)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:26.854 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 7, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:26.854 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job
[31m- decimal conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 7, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:26.902 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 7.0 (TID 8)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:26.907 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 7.0 (TID 8, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:26.908 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 7.0 failed 1 times; aborting job
[31m- index conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 8, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:26.992 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 8.0 (TID 9)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:26.996 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 8.0 (TID 9, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:26.996 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job
[31m- mixed numeric type conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 9, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.097 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 9.0 (TID 10)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.099 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 9.0 (TID 10, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.099 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job
[31m- string type conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 10, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.177 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 10.0 (TID 11)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.180 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 10.0 (TID 11, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.186 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 10.0 failed 1 times; aborting job
[31m- boolean type conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 11, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.231 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 11.0 (TID 12)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.233 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 11.0 (TID 12, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.233 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 11.0 failed 1 times; aborting job
[31m- byte type conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 12, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.268 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 12.0 (TID 13)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.279 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 12.0 (TID 13, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.280 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job
[31m- binary type conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 13, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.335 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 13.0 (TID 14)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.337 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 13.0 (TID 14, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.337 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 13.0 failed 1 times; aborting job
[31m- date type conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 14, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.410 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 14.0 (TID 15)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.412 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 14.0 (TID 15, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.412 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 14.0 failed 1 times; aborting job
[31m- timestamp type conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 15, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.527 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 15.0 (TID 16)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.529 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 15.0 (TID 16, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.529 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 15.0 failed 1 times; aborting job
[31m- floating-point NaN *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 16, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.701 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 16.0 (TID 17)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.702 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 16.0 (TID 17, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.702 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job
[31m- array type conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 17, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.762 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 17.0 (TID 18)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.764 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 17.0 (TID 18, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.764 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 17.0 failed 1 times; aborting job
[31m- struct type conversion *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 18, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.814 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 18.0 (TID 19)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.814 ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 18.0 (TID 20)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.816 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 18.0 (TID 19, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.816 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 18.0 failed 1 times; aborting job
[31m- partitioned DataFrame *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 19, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.916 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 19.0 (TID 21)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.916 ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 19.0 (TID 22)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.918 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 19.0 (TID 21, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:27.918 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 19.0 failed 1 times; aborting job
[31m- empty frame collect *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 21, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:27.995 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 20.0 (TID 23)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:27.996 ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 20.0 (TID 24)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:28.000 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 20.0 (TID 23, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:28.000 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 20.0 failed 1 times; aborting job
[31m- empty partition collect *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 23, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
21:48:28.025 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 21.0 (TID 25)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:28.025 ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 21.0 (TID 26)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:28.026 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 21.0 (TID 25, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:28.027 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job
[31m- max records in batch conf *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 25, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31mCaused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m	... 16 more[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
[31m- unsupported types *** FAILED ***[0m
[31m  "Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 27, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)[0m
[31m  	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)[0m
[31m  	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)[0m
[31m  	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[31m  	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)[0m
[31m  	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)[0m
[31m  	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[31m  	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m  	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m  	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m  	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m  	at java.lang.Thread.run(Thread.java:813)[0m
[31m  Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  	... 16 more[0m
  
[31m  Driver stacktrace:" did not contain "Unsupported data type" (ArrowConvertersSuite.scala:1216)[0m
21:48:28.249 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 23.0 (TID 29)
java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more
21:48:28.255 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 23.0 (TID 29, localhost, executor driver): java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)
	at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3437)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3435)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
Caused by: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)
	at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)
	at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)
	at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)
	at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)
	at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)
	at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)
	at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)
	... 16 more

21:48:28.255 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 23.0 failed 1 times; aborting job
[31m- test Arrow Validator *** FAILED ***[0m
[31m  Expected exception java.lang.IllegalArgumentException to be thrown, but org.apache.spark.SparkException was thrown (ArrowConvertersSuite.scala:1311)[0m
[31m*** RUN ABORTED ***[0m
[31m  java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.arrow.ArrowUtils$ (initialization failure)[0m
[31m  at java.lang.J9VMInternals.initializationAlreadyFailed(J9VMInternals.java:96)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConverters$.toBatchIterator(ArrowConverters.scala:84)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConvertersSuite$$anonfun$34.apply(ArrowConvertersSuite.scala:1329)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowConvertersSuite$$anonfun$34.apply(ArrowConvertersSuite.scala:1321)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  ...[0m
[31m  Cause: java.lang.IllegalStateException: Arrow only runs on LittleEndian systems.[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:66)[0m
[31m  at io.netty.buffer.UnsafeDirectLittleEndian.<init>(UnsafeDirectLittleEndian.java:51)[0m
[31m  at io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:50)[0m
[31m  at org.apache.arrow.memory.AllocationManager.<clinit>(AllocationManager.java:68)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.createEmpty(BaseAllocator.java:252)[0m
[31m  at org.apache.arrow.memory.BaseAllocator.<init>(BaseAllocator.java:83)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:34)[0m
[31m  at org.apache.arrow.memory.RootAllocator.<init>(RootAllocator.java:30)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<init>(ArrowUtils.scala:31)[0m
[31m  at org.apache.spark.sql.execution.arrow.ArrowUtils$.<clinit>(ArrowUtils.scala)[0m
[31m  ...[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------< [0;36morg.apache.spark:spark-mllib_2.11[0;1m >------------------[m
[[1;34mINFO[m] [1mBuilding Spark Project ML Library 2.4.4                          [15/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/mllib/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/mllib/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/spark/graphx/target/scala-2.11/classes:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/spark/mllib-local/target/scala-2.11/classes:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/spark/streaming/target/scala-2.11/classes:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 17 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:48:52 AM [21.312s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/spark/mllib-local/target/scala-2.11/test-classes:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/spark/graphx/target/scala-2.11/classes:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/spark/mllib-local/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/spark/streaming/target/scala-2.11/test-classes:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/spark/streaming/target/scala-2.11/classes:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/spark/sql/catalyst/target/spark-catalyst_2.11-2.4.4-tests.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/spark/sql/core/target/spark-sql_2.11-2.4.4-tests.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/core/target/scala-2.11/test-classes:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 4:49:21 AM [29.202s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-mllib_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-mllib_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 10 seconds, 609 milliseconds.[0m
[36mRun starting. Expected test count is: 1414[0m
[32mElementwiseProductSuite:[0m
[32m- streaming transform[0m
[32m- read/write[0m
[32mKMeansSuite:[0m
[32m- single cluster[0m
[32m- fewer distinct points than clusters[0m
[32m- unique cluster centers[0m
[32m- deterministic initialization[0m
[32m- single cluster with big dataset[0m
[32m- single cluster with sparse data[0m
[32m- k-means|| initialization[0m
[32m- two clusters[0m
[32m- model save/load[0m
[32m- Initialize using given cluster centers[0m
[32m- Kryo class register[0m
[32mMatrixFactorizationModelSuite:[0m
[32m- constructor[0m
[32m- save/load[0m
[32m- invalid user and product[0m
[32m- batch predict API recommendProductsForUsers[0m
[32m- batch predict API recommendUsersForProducts[0m
[32mDecisionTreeClassifierSuite:[0m
[32m- params[0m
[32m- Binary classification stump with ordered categorical features[0m
[32m- Binary classification stump with fixed labels 0,1 for Entropy,Gini[0m
[32m- Multiclass classification stump with 3-ary (unordered) categorical features[0m
[32m- Binary classification stump with 1 continuous feature, to check off-by-1 error[0m
[32m- Binary classification stump with 2 continuous features[0m
[32m- Multiclass classification stump with unordered categorical features, with just enough bins[0m
[32m- Multiclass classification stump with continuous features[0m
[32m- Multiclass classification stump with continuous + unordered categorical features[0m
[32m- Multiclass classification stump with 10-ary (ordered) categorical features[0m
[32m- Multiclass classification tree with 10-ary (ordered) categorical features, with just enough bins[0m
[32m- split must satisfy min instances per node requirements[0m
[32m- do not choose split that does not satisfy min instance per node requirements[0m
[32m- split must satisfy min info gain requirements[0m
[32m- predictRaw and predictProbability[0m
[32m- prediction on single instance[0m
[32m- training with 1-category categorical feature[0m
[32m- Feature importance with toy data[0m
[32m- should support all NumericType labels and not support other types[0m
[32m- Fitting without numClasses in metadata[0m
[32m- read/write[0m
[32m- SPARK-20043: ImpurityCalculator builder fails for uppercase impurity type Gini in model read/write[0m
[32mPCASuite:[0m
[32m- params[0m
[32m- pca[0m
[32m- PCA read/write[0m
[32m- PCAModel read/write[0m
[32mKMeansClusterSuite:[0m
[32m- task size should be small in both training and prediction[0m
[32mMultilabelMetricsSuite:[0m
[32m- Multilabel evaluation metrics[0m
[32mLinearSVCSuite:[0m
[33m- export test data into CSV format !!! IGNORED !!![0m
[32m- Linear SVC binary classification[0m
[32m- Linear SVC binary classification with regularization[0m
[32m- params[0m
[32m- linear svc: default params[0m
[32m- LinearSVC threshold acts on rawPrediction[0m
[32m- linear svc doesn't fit intercept when fitIntercept is off[0m
[32m- sparse coefficients in HingeAggregator[0m
[32m- linearSVC with sample weights[0m
[32m- prediction on single instance[0m
[32m- linearSVC comparison with R e1071 and scikit-learn[0m
[32m- read/write: SVM[0m
[32mCorrelationSuite:[0m
[32m- corr(X) default, pearson[0m
[32m- corr(X) spearman[0m
[32mStandardScalerSuite:[0m
[32m- params[0m
[32m- Standardization with default parameter[0m
[32m- Standardization with setter[0m
[32m- sparse data and withMean[0m
[32m- StandardScaler read/write[0m
[32m- StandardScalerModel read/write[0m
[32mMaxAbsScalerSuite:[0m
[32m- MaxAbsScaler fit basic case[0m
[32m- MaxAbsScaler read/write[0m
[32m- MaxAbsScalerModel read/write[0m
[32mFPTreeSuite:[0m
[32m- add transaction[0m
[32m- merge tree[0m
[32m- extract freq itemsets[0m
[32mPipelineSuite:[0m
[32m- pipeline[0m
[32m- pipeline with duplicate stages[0m
[32m- Pipeline.copy[0m
[32m- PipelineModel.copy[0m
[32m- pipeline model constructors[0m
[32m- Pipeline read/write[0m
[32m- Pipeline read/write with non-Writable stage[0m
[32m- PipelineModel read/write[0m
[32m- PipelineModel read/write: getStagePath[0m
[32m- PipelineModel read/write with non-Writable stage[0m
[32m- pipeline validateParams[0m
[32m- Pipeline.setStages should handle Java Arrays being non-covariant[0m
[32mLDASuite:[0m
[32m- default parameters[0m
[32m- set parameters[0m
[32m- parameters validation[0m
[32m- fit & transform with Online LDA[0m
[32m- fit & transform with EM LDA[0m
[32m- read/write LocalLDAModel[0m
[32m- read/write DistributedLDAModel[0m
[32m- EM LDA checkpointing: save last checkpoint[0m
[32m- EM LDA checkpointing: remove last checkpoint[0m
[32m- EM LDA disable checkpointing[0m
[32m- string params should be case-insensitive[0m
[32m- LDA with Array input[0m
[32mNumericParserSuite:[0m
[32m- parser[0m
[32m- parser with whitespaces[0m
[32mMatricesSuite:[0m
[32m- kryo class register[0m
[32m- dense matrix construction[0m
[32m- dense matrix construction with wrong dimension[0m
[32m- sparse matrix construction[0m
[32m- sparse matrix construction with wrong number of elements[0m
[32m- index in matrices incorrect input[0m
[32m- equals[0m
[32m- matrix copies are deep copies[0m
[32m- matrix indexing and updating[0m
[32m- toSparse, toDense[0m
[32m- map, update[0m
[32m- transpose[0m
[32m- foreachActive[0m
[32m- horzcat, vertcat, eye, speye[0m
[32m- zeros[0m
[32m- ones[0m
[32m- eye[0m
[32m- rand[0m
[32m- randn[0m
[32m- diag[0m
[32m- sprand[0m
[32m- sprandn[0m
[32m- MatrixUDT[0m
[32m- toString[0m
[32m- numNonzeros and numActives[0m
[32m- fromBreeze with sparse matrix[0m
[32m- Test FromBreeze when Breeze.CSCMatrix.rowIndices has trailing zeros. - SPARK-20687[0m
[32m- row/col iterator[0m
[32m- conversions between new local linalg and mllib linalg[0m
[32m- implicit conversions between new local linalg and mllib linalg[0m
[32mMultivariateGaussianSuite:[0m
[32m- univariate[0m
[32m- multivariate[0m
[32m- multivariate degenerate[0m
[32m- SPARK-11302[0m
[32mStreamingLogisticRegressionSuite:[0m
[32m- parameter accuracy[0m
[32m- parameter convergence[0m
[32m- predictions[0m
[32m- training and prediction[0m
[32m- handling empty RDDs in a stream[0m
[32mRandomDataGeneratorSuite:[0m
[32m- UniformGenerator[0m
[32m- StandardNormalGenerator[0m
[32m- LogNormalGenerator[0m
[32m- PoissonGenerator[0m
[32m- ExponentialGenerator[0m
[32m- GammaGenerator[0m
[32m- WeibullGenerator[0m
[32mJsonVectorConverterSuite:[0m
[32m- toJson/fromJson[0m
[32mLogisticRegressionClusterSuite:[0m
[32m- task size should be small in both training and prediction using SGD optimizer[0m
[32m- task size should be small in both training and prediction using LBFGS optimizer[0m
[32mRegressionEvaluatorSuite:[0m
[32m- params[0m
[32m- Regression Evaluator: default params[0m
[32m- read/write[0m
[32m- should support all NumericType labels and not support other types[0m
[32mRankingMetricsSuite:[0m
[32m- Ranking metrics: MAP, NDCG[0m
[32m- MAP, NDCG with few predictions (SPARK-14886)[0m
[32mStreamingKMeansSuite:[0m
[32m- accuracy for single center and equivalence to grand average[0m
[32m- accuracy for two centers[0m
[32m- detecting dying clusters[0m
[32m- SPARK-7946 setDecayFactor[0m
[32mMLUtilsSuite:[0m
[32m- epsilon computation[0m
[32m- fast squared distance[0m
[32m- loadLibSVMFile[0m
[32m- loadLibSVMFile throws IllegalArgumentException when indices is zero-based[0m
[32m- loadLibSVMFile throws IllegalArgumentException when indices is not in ascending order[0m
[32m- saveAsLibSVMFile[0m
[32m- appendBias[0m
[32m- kFold[0m
[32m- loadVectors[0m
[32m- loadLabeledPoints[0m
[32m- log1pExp[0m
[32m- convertVectorColumnsToML[0m
[32m- convertVectorColumnsFromML[0m
[32m- convertMatrixColumnsToML[0m
[32m- convertMatrixColumnsFromML[0m
[32mSQLDataTypesSuite:[0m
[32m- sqlDataTypes[0m
[32mGradientBoostedTreesSuite:[0m
[32m- runWithValidation stops early and performs better on a validation dataset[0m
[32mRDDFunctionsSuite:[0m
[32m- sliding[0m
[32m- sliding with empty partitions[0m
[32mGradientDescentClusterSuite:[0m
[32m- task size should be small[0m
[32mMulticlassClassificationEvaluatorSuite:[0m
[32m- params[0m
[32m- read/write[0m
[32m- should support all NumericType labels and not support other types[0m
[32mLogisticRegressionSuite:[0m
[33m- export test data into CSV format !!! IGNORED !!![0m
[32m- params[0m
[32m- logistic regression: default params[0m
[32m- logistic regression: illegal params[0m
[32m- empty probabilityCol or predictionCol[0m
[32m- check summary types for binary and multiclass[0m
[32m- setThreshold, getThreshold[0m
[32m- thresholds prediction[0m
[32m- logistic regression doesn't fit intercept when fitIntercept is off[0m
[32m- logistic regression with setters[0m
[32m- multinomial logistic regression: Predictor, Classifier methods[0m
[32m- binary logistic regression: Predictor, Classifier methods[0m
[32m- prediction on single instance[0m
[32m- coefficients and intercept methods[0m
[32m- sparse coefficients in LogisticAggregator[0m
[32m- overflow prediction for multiclass[0m
[32m- MultiClassSummarizer[0m
[32m- MultiClassSummarizer with weighted samples[0m
[32m- binary logistic regression with intercept without regularization[0m
[32m- binary logistic regression with intercept without regularization with bound[0m
[32m- binary logistic regression without intercept without regularization[0m
[32m- binary logistic regression without intercept without regularization with bound[0m
[32m- binary logistic regression with intercept with L1 regularization[0m
[32m- binary logistic regression without intercept with L1 regularization[0m
[32m- binary logistic regression with intercept with L2 regularization[0m
[32m- binary logistic regression with intercept with L2 regularization with bound[0m
[32m- binary logistic regression without intercept with L2 regularization[0m
[32m- binary logistic regression without intercept with L2 regularization with bound[0m
[32m- binary logistic regression with intercept with ElasticNet regularization[0m
[32m- binary logistic regression without intercept with ElasticNet regularization[0m
[32m- binary logistic regression with intercept with strong L1 regularization[0m
[32m- multinomial logistic regression with intercept with strong L1 regularization[0m
[32m- multinomial logistic regression with intercept without regularization[0m
[32m- multinomial logistic regression with zero variance (SPARK-21681)[0m
[32m- multinomial logistic regression with intercept without regularization with bound[0m
[32m- multinomial logistic regression without intercept without regularization[0m
[32m- multinomial logistic regression without intercept without regularization with bound[0m
[32m- multinomial logistic regression with intercept with L1 regularization[0m
[32m- multinomial logistic regression without intercept with L1 regularization[0m
[32m- multinomial logistic regression with intercept with L2 regularization[0m
[32m- multinomial logistic regression with intercept with L2 regularization with bound[0m
[32m- multinomial logistic regression without intercept with L2 regularization[0m
[32m- multinomial logistic regression without intercept with L2 regularization with bound[0m
[32m- multinomial logistic regression with intercept with elasticnet regularization[0m
[32m- multinomial logistic regression without intercept with elasticnet regularization[0m
[32m- evaluate on test set[0m
[32m- evaluate with labels that are not doubles[0m
[32m- statistics on training data[0m
[32m- logistic regression with sample weights[0m
[32m- set family[0m
[32m- set initial model[0m
[32m- binary logistic regression with all labels the same[0m
[32m- multiclass logistic regression with all labels the same[0m
[32m- compressed storage for constant label[0m
[32m- compressed coefficients[0m
[32m- numClasses specified in metadata/inferred[0m
[32m- read/write[0m
[32m- should support all NumericType labels and weights, and not support other types[0m
[32m- string params should be case-insensitive[0m
[32m- toString[0m
[32mInstanceSuite:[0m
[32m- Kryo class register[0m
[32mDCTSuite:[0m
[32m- forward transform of discrete cosine matches jTransforms result[0m
[32m- inverse transform of discrete cosine matches jTransforms result[0m
[32m- read/write[0m
[32mKernelDensitySuite:[0m
[32m- kernel density single sample[0m
[32m- kernel density multiple samples[0m
[32mBreezeVectorConversionSuite:[0m
[32m- dense to breeze[0m
[32m- sparse to breeze[0m
[32m- dense breeze to vector[0m
[32m- sparse breeze to vector[0m
[32m- sparse breeze with partially-used arrays to vector[0m
[32mLibSVMRelationSuite:[0m
[32m- select as sparse vector[0m
[32m- select as dense vector[0m
[32m- illegal vector types[0m
[32m- select a vector with specifying the longer dimension[0m
[32m- case insensitive option[0m
[32m- write libsvm data and read it again[0m
[32m- write libsvm data failed due to invalid schema[0m
[32m- write libsvm data from scratch and read it again[0m
[32m- select features from libsvm relation[0m
[32m- create libsvmTable table without schema[0m
[32m- create libsvmTable table without schema and path[0m
[32mIsotonicRegressionSuite:[0m
[32m- isotonic regression predictions[0m
[32m- antitonic regression predictions[0m
[32m- params validation[0m
[32m- default params[0m
[32m- set parameters[0m
[32m- missing column[0m
[32m- vector features column with feature index[0m
[32m- read/write[0m
[32m- should support all NumericType labels and weights, and not support other types[0m
[32mBinaryClassificationEvaluatorSuite:[0m
[32m- params[0m
[32m- read/write[0m
[32m- should accept both vector and double raw prediction col[0m
[32m- should support all NumericType labels and not support other types[0m
[32mOneHotEncoderEstimatorSuite:[0m
[32m- params[0m
[32m- OneHotEncoderEstimator dropLast = false[0m
[32m- OneHotEncoderEstimator dropLast = true[0m
[32m- input column with ML attribute[0m
[32m- input column without ML attribute[0m
[32m- read/write[0m
[32m- OneHotEncoderModel read/write[0m
[32m- OneHotEncoderEstimator with varying types[0m
[32m- OneHotEncoderEstimator: encoding multiple columns and dropLast = false[0m
[32m- OneHotEncoderEstimator: encoding multiple columns and dropLast = true[0m
[32m- Throw error on invalid values[0m
[32m- Can't transform on negative input[0m
[32m- Keep on invalid values: dropLast = false[0m
[32m- Keep on invalid values: dropLast = true[0m
[32m- OneHotEncoderModel changes dropLast[0m
[32m- OneHotEncoderModel changes handleInvalid[0m
[32m- Transforming on mismatched attributes[0m
[32mAttributeGroupSuite:[0m
[32m- attribute group[0m
[32m- attribute group without attributes[0m
[32mCorrelationSuite:[0m
[32m- corr(x, y) pearson, 1 value in data[0m
[32m- corr(x, y) default, pearson[0m
[32m- corr(x, y) spearman[0m
[32m- corr(X) default, pearson[0m
[32m- corr(X) spearman[0m
[32m- method identification[0m
[33m- Pearson correlation of very large uncorrelated values (SPARK-14533) !!! IGNORED !!![0m
[32mStopwatchSuite:[0m
[32m- LocalStopwatch[0m
[32m- DistributedStopwatch on driver[0m
[32m- DistributedStopwatch on executors[0m
[32m- MultiStopwatch[0m
[32mSVMClusterSuite:[0m
[32m- task size should be small in both training and prediction[0m
[32mRowMatrixClusterSuite:[0m
[32m- task size should be small in svd[0m
[32m- task size should be small in summarize[0m
[32mBucketizerSuite:[0m
[32m- params[0m
[32m- Bucket continuous features, without -inf,inf[0m
[32m- Bucket continuous features, with -inf,inf[0m
[32m- Bucket continuous features, with NaN data but non-NaN splits[0m
[32m- Bucketizer should only drop NaN in input columns, with handleInvalid=skip[0m
[32m- Bucket continuous features, with NaN splits[0m
[32m- Binary search correctness on hand-picked examples[0m
[32m- Binary search correctness in contrast with linear search, on random data[0m
[32m- read/write[0m
[32m- Bucket numeric features[0m
[32m- multiple columns: Bucket continuous features, without -inf,inf[0m
[32m- multiple columns: Bucket continuous features, with -inf,inf[0m
[32m- multiple columns: Bucket continuous features, with NaN data but non-NaN splits[0m
[32m- multiple columns: Bucket continuous features, with NaN splits[0m
[32m- multiple columns: read/write[0m
[32m- Bucketizer in a pipeline[0m
[32m- Compare single/multiple column(s) Bucketizer in pipeline[0m
[32m- assert exception is thrown if both multi-column and single-column params are set[0m
[32mSVMSuite:[0m
[32m- SVM with threshold[0m
[32m- SVM using local random SGD[0m
[32m- SVM local random SGD with initial weights[0m
[32m- SVM with invalid labels[0m
[32m- model save/load[0m
[32mBisectingKMeansSuite:[0m
[32m- default parameters[0m
[32m- SPARK-16473: Verify Bisecting K-Means does not fail in edge case whereone cluster is empty after split[0m
[32m- setter/getter[0m
[32m- fit, transform and summary[0m
[32m- read/write[0m
[32m- BisectingKMeans with cosine distance is not supported for 0-length vectors[0m
[32m- BisectingKMeans with cosine distance[0m
[32m- BisectingKMeans with Array input[0m
[32mLeastSquaresAggregatorSuite:[0m
[32m- aggregator add method input size[0m
[32m- negative weight[0m
[32m- check sizes[0m
[32m- check correctness[0m
[32m- check with zero standard deviation[0m
[32mParamGridBuilderSuite:[0m
[32m- param grid builder[0m
[32mMLTestSuite:[0m
[32m- test transformer on stream data[0m
[32mRegressionMetricsSuite:[0m
[32m- regression metrics for unbiased (includes intercept term) predictor[0m
[32m- regression metrics for biased (no intercept term) predictor[0m
[32m- regression metrics with complete fitting[0m
[32mPowerIterationClusteringSuite:[0m
[32m- power iteration clustering[0m
[32m- power iteration clustering on graph[0m
[32m- normalize and powerIter[0m
[32m- model save/load[0m
[32mRandomForestSuite:[0m
[32m- Binary classification with continuous features: split calculation[0m
[32m- Binary classification with binary (ordered) categorical features: split calculation[0m
[32m- Binary classification with 3-ary (ordered) categorical features, with no samples for one category: split calculation[0m
[32m- find splits for a continuous feature[0m
[32m- train with empty arrays[0m
[32m- train with constant features[0m
[32m- Multiclass classification with unordered categorical features: split calculations[0m
[32m- Multiclass classification with ordered categorical features: split calculations[0m
[32m- extract categories from a number for multiclass classification[0m
[32m- Avoid aggregation on the last level[0m
[32m- Avoid aggregation if impurity is 0.0[0m
[32m- Use soft prediction for binary classification with ordered categorical features[0m
[32m- Second level node building with vs. without groups[0m
[32m- Binary classification with continuous features: subsampling features[0m
[32m- Binary classification with continuous features and node Id cache: subsampling features[0m
[32m- computeFeatureImportance, featureImportances[0m
[32m- normalizeMapValues[0m
[32m- SPARK-3159 tree model redundancy - classification[0m
[32m- SPARK-3159 tree model redundancy - regression[0m
[32mPrefixSpanSuite:[0m
[32m- PrefixSpan internal (integer seq, 0 delim) run, singleton itemsets[0m
[32m- PrefixSpan internal (integer seq, -1 delim) run, variable-size itemsets[0m
[32m- PrefixSpan projections with multiple partial starts[0m
[32m- PrefixSpan Integer type, variable-size itemsets[0m
[32m- PrefixSpan String type, variable-size itemsets[0m
[32m- PrefixSpan pre-processing's cleaning test[0m
[32m- model save/load[0m
[32mParamsSuite:[0m
[32m- json encode/decode[0m
[32m- param[0m
[32m- param pair[0m
[32m- param map[0m
[32m- params[0m
[32m- ParamValidate[0m
[32m- Params.copyValues[0m
[32m- Filtering ParamMap[0m
[32mAFTSurvivalRegressionSuite:[0m
[33m- export test data into CSV format !!! IGNORED !!![0m
[32m- params[0m
[32m- aft survival regression: default params[0m
[32m- aft survival regression with univariate[0m
[32m- aft survival regression with multivariate[0m
[32m- aft survival regression w/o intercept[0m
[32m- aft survival regression w/o quantiles column[0m
[32m- should support all NumericType labels, and not support other types[0m
[32m- should support all NumericType censors, and not support other types[0m
[32m- numerical stability of standardization[0m
[32m- read/write[0m
[32m- SPARK-15892: Incorrectly merged AFTAggregator with zero total count[0m
[32mCrossValidatorSuite:[0m
[32m- cross validation with logistic regression[0m
[32m- cross validation with linear regression[0m
[32m- transformSchema should check estimatorParamMaps[0m
[32m- cross validation with parallel evaluation[0m
[32m- read/write: CrossValidator with simple estimator[0m
[32m- CrossValidator expose sub models[0m
[32m- read/write: CrossValidator with nested estimator[0m
[32m- read/write: Persistence of nested estimator works if parent directory changes[0m
[32m- read/write: CrossValidator with complex estimator[0m
[32m- read/write: CrossValidator fails for extraneous Param[0m
[32m- read/write: CrossValidatorModel[0m
[32mChiSquareTestSuite:[0m
[32m- test DataFrame of labeled points[0m
[32m- large number of features (SPARK-3087)[0m
[32m- fail on continuous features or labels[0m
[32mLogisticRegressionSuite:[0m
[32m- logistic regression with SGD[0m
[32m- logistic regression with LBFGS[0m
[32m- logistic regression with initial weights with SGD[0m
[32m- logistic regression with initial weights and non-default regularization parameter[0m
[32m- logistic regression with initial weights with LBFGS[0m
[32m- numerical stability of scaling features using logistic regression with LBFGS[0m
[32m- multinomial logistic regression with LBFGS[0m
[32m- model save/load: binary classification[0m
[32m- model save/load: multiclass classification[0m
[32m- binary logistic regression with intercept without regularization[0m
[32m- binary logistic regression without intercept without regularization[0m
[32m- binary logistic regression with intercept with L1 regularization[0m
[32m- binary logistic regression without intercept with L1 regularization[0m
[32m- binary logistic regression with intercept with L2 regularization[0m
[32m- binary logistic regression without intercept with L2 regularization[0m
[32mOneHotEncoderSuite:[0m
[32m- params[0m
[32m- OneHotEncoder dropLast = false[0m
[32m- OneHotEncoder dropLast = true[0m
[32m- input column with ML attribute[0m
[32m- input column without ML attribute[0m
[32m- read/write[0m
[32m- OneHotEncoder with varying types[0m
[32mVectorIndexerSuite:[0m
[32m- params[0m
[32m- Cannot fit an empty DataFrame[0m
[32m- Throws error when given RDDs with different size vectors[0m
[32m- Same result with dense and sparse vectors[0m
[32m- Builds valid categorical feature value index, transform correctly, check metadata[0m
[32m- handle invalid[0m
[32m- Maintain sparsity for sparse vectors[0m
[32m- Preserve metadata[0m
[32m- VectorIndexer read/write[0m
[32m- VectorIndexerModel read/write[0m
[32mBucketedRandomProjectionLSHSuite:[0m
[32m- params[0m
[32m- setters[0m
[32m- BucketedRandomProjectionLSH: default params[0m
[32m- read/write[0m
[32m- hashFunction[0m
[32m- keyDistance[0m
[32m- BucketedRandomProjectionLSH: randUnitVectors[0m
[32m- BucketedRandomProjectionLSH: streaming transform[0m
[32m- BucketedRandomProjectionLSH: test of LSH property[0m
[32m- BucketedRandomProjectionLSH with high dimension data: test of LSH property[0m
[32m- approxNearestNeighbors for bucketed random projection[0m
[32m- approxNearestNeighbors with multiple probing[0m
[32m- approxNearestNeighbors for numNeighbors <= 0[0m
[32m- approxSimilarityJoin for bucketed random projection on different dataset[0m
[32m- approxSimilarityJoin for self join[0m
[32mHashingTFSuite:[0m
[32m- hashing tf on a single doc[0m
[32m- hashing tf on an RDD[0m
[32m- applying binary term freqs[0m
[32mLinearRegressionClusterSuite:[0m
[32m- task size should be small in both training and prediction[0m
[32mBreezeMatrixConversionSuite:[0m
[32m- dense matrix to breeze[0m
[32m- dense breeze matrix to matrix[0m
[32m- sparse matrix to breeze[0m
[32m- sparse breeze matrix to sparse matrix[0m
[32mGBTClassifierSuite:[0m
[32m- params[0m
[32m- GBTClassifier: default params[0m
[32m- setThreshold, getThreshold[0m
[32m- thresholds prediction[0m
[32m- GBTClassifier: Predictor, Classifier methods[0m
[32m- prediction on single instance[0m
[32m- GBT parameter stepSize should be in interval (0, 1][0m
[32m- Binary classification with continuous features: Log Loss[0m
[32m- Checkpointing[0m
[32m- should support all NumericType labels and not support other types[0m
[32m- Fitting without numClasses in metadata[0m
[32m- extractLabeledPoints with bad data[0m
[32m- Feature importance with toy data[0m
[32m- Tests of feature subset strategy[0m
[32m- model evaluateEachIteration[0m
[32m- runWithValidation stops early and performs better on a validation dataset[0m
[32m- model save/load[0m
[32mNaiveBayesClusterSuite:[0m
[32m- task size should be small in both training and prediction[0m
[32mCoordinateMatrixSuite:[0m
[32m- size[0m
[32m- empty entries[0m
[32m- toBreeze[0m
[32m- transpose[0m
[32m- toIndexedRowMatrix[0m
[32m- toRowMatrix[0m
[32m- toBlockMatrix[0m
[32mTrainValidationSplitSuite:[0m
[32m- train validation with logistic regression[0m
[32m- train validation with linear regression[0m
[32m- transformSchema should check estimatorParamMaps[0m
[32m- train validation with parallel evaluation[0m
[32m- read/write: TrainValidationSplit[0m
[32m- TrainValidationSplit expose sub models[0m
[32m- read/write: TrainValidationSplit with nested estimator[0m
[32m- read/write: Persistence of nested estimator works if parent directory changes[0m
[32m- read/write: TrainValidationSplitModel[0m
[32mIsotonicRegressionSuite:[0m
[32m- increasing isotonic regression[0m
[32m- model save/load[0m
[32m- isotonic regression with size 0[0m
[32m- isotonic regression with size 1[0m
[32m- isotonic regression strictly increasing sequence[0m
[32m- isotonic regression strictly decreasing sequence[0m
[32m- isotonic regression with last element violating monotonicity[0m
[32m- isotonic regression with first element violating monotonicity[0m
[32m- isotonic regression with negative labels[0m
[32m- isotonic regression with unordered input[0m
[32m- weighted isotonic regression[0m
[32m- weighted isotonic regression with weights lower than 1[0m
[32m- weighted isotonic regression with negative weights[0m
[32m- weighted isotonic regression with zero weights[0m
[32m- SPARK-16426 isotonic regression with duplicate features that produce NaNs[0m
[32m- isotonic regression prediction[0m
[32m- isotonic regression prediction with duplicate features[0m
[32m- antitonic regression prediction with duplicate features[0m
[32m- isotonic regression RDD prediction[0m
[32m- antitonic regression prediction[0m
[32m- model construction[0m
[32mSharedParamsSuite:[0m
[32m- outputCol[0m
[32mALSSuite:[0m
[32m- LocalIndexEncoder[0m
[32m- normal equation construction[0m
[32m- CholeskySolver[0m
[32m- RatingBlockBuilder[0m
[32m- UncompressedInBlock[0m
[32m- CheckedCast[0m
[32m- exact rank-1 matrix[0m
[32m- approximate rank-1 matrix[0m
[32m- approximate rank-2 matrix[0m
[32m- different block settings[0m
[32m- more blocks than ratings[0m
[32m- implicit feedback[0m
[32m- implicit feedback regression[0m
[32m- using generic ID types[0m
[32m- nonnegative constraint[0m
[32m- als partitioner is a projection[0m
[32m- partitioner in returned factors[0m
[32m- als with large number of iterations[0m
[32m- read/write[0m
[32m- input type validation[0m
[32m- SPARK-18268: ALS with empty RDD should fail with better message[0m
[32m- ALS cold start user/item prediction strategy[0m
[32m- case insensitive cold start param value[0m
[32m- recommendForAllUsers with k <, = and > num_items[0m
[32m- recommendForAllItems with k <, = and > num_users[0m
[32m- recommendForUserSubset with k <, = and > num_items[0m
[32m- recommendForItemSubset with k <, = and > num_users[0m
[32m- subset recommendations eliminate duplicate ids, returns same results as unique ids[0m
[32m- subset recommendations on full input dataset equivalent to recommendForAll[0m
[32mWord2VecSuite:[0m
[32m- Word2Vec[0m
[32m- Word2Vec throws exception when vocabulary is empty[0m
[32m- Word2VecModel[0m
[32m- findSynonyms doesn't reject similar word vectors when called with a vector[0m
[32m- model load / save[0m
[32m- big model load / save[0m
[32m- test similarity for word vectors with large values is not Infinity or NaN[0m
[32mMulticlassMetricsSuite:[0m
[32m- Multiclass evaluation metrics[0m
[32mGaussianMixtureSuite:[0m
[32m- gmm fails on high dimensional data[0m
[32m- default parameters[0m
[32m- set parameters[0m
[32m- parameters validation[0m
[32m- fit, transform and summary[0m
[32m- read/write[0m
[32m- univariate dense/sparse data with two clusters[0m
[32m- check distributed decomposition[0m
[32m- multivariate data and check againt R mvnormalmixEM[0m
[32m- upper triangular matrix unpacking[0m
[32m- GaussianMixture with Array input[0m
[32mMultilayerPerceptronClassifierSuite:[0m
[32m- Input Validation[0m
[32m- XOR function learning as binary classification problem with two outputs.[0m
[32m- prediction on single instance[0m
[32m- Predicted class probabilities: calibration on toy dataset[0m
[32m- test model probability[0m
[32m- Test setWeights by training restart[0m
[32m- 3 class classification with 2 hidden layers[0m
[32m- read/write: MultilayerPerceptronClassifier[0m
[32m- read/write: MultilayerPerceptronClassificationModel[0m
[32m- should support all NumericType labels and not support other types[0m
[32mJsonMatrixConverterSuite:[0m
[32m- toJson/fromJson[0m
[32mDefaultReadWriteSuite:[0m
[32m- default read/write[0m
[32m- default param shouldn't become user-supplied param after persistence[0m
[32m- User-supplied value for default param should be kept after persistence[0m
[32m- Read metadata without default field prior to 2.4[0m
[32m- Should raise error when read metadata without default field after Spark 2.4[0m
[32mLassoClusterSuite:[0m
[32m- task size should be small in both training and prediction[0m
[32mAttributeSuite:[0m
[32m- default numeric attribute[0m
[32m- customized numeric attribute[0m
[32m- bad numeric attributes[0m
[32m- default nominal attribute[0m
[32m- customized nominal attribute[0m
[32m- bad nominal attributes[0m
[32m- default binary attribute[0m
[32m- customized binary attribute[0m
[32m- bad binary attributes[0m
[32m- attribute from struct field[0m
[32mStreamingTestSuite:[0m
[32m- accuracy for null hypothesis using welch t-test[0m
[32m- accuracy for alternative hypothesis using welch t-test[0m
[32m- accuracy for null hypothesis using student t-test[0m
[32m- accuracy for alternative hypothesis using student t-test[0m
[32m- batches within same test window are grouped[0m
[32m- entries in peace period are dropped[0m
[32m- null hypothesis when only data from one group is present[0m
[32mGeneralizedLinearPMMLModelExportSuite:[0m
[32m- linear regression PMML export[0m
[32m- ridge regression PMML export[0m
[32m- lasso PMML export[0m
[32mKolmogorovSmirnovTestSuite:[0m
[32m- 1 sample Kolmogorov-Smirnov test: apache commons math3 implementation equivalence[0m
[32m- 1 sample Kolmogorov-Smirnov test: R implementation equivalence[0m
[32mLinearRegressionSuite:[0m
[33m- export test data into CSV format !!! IGNORED !!![0m
[32m- params[0m
[32m- linear regression: default params[0m
[32m- linear regression: can transform data with LinearRegressionModel[0m
[32m- linear regression: illegal params[0m
[32m- linear regression handles singular matrices[0m
[32m- linear regression with intercept without regularization[0m
[32m- linear regression without intercept without regularization[0m
[32m- linear regression with intercept with L1 regularization[0m
[32m- linear regression without intercept with L1 regularization[0m
[32m- linear regression with intercept with L2 regularization[0m
[32m- linear regression without intercept with L2 regularization[0m
[32m- linear regression with intercept with ElasticNet regularization[0m
[32m- linear regression without intercept with ElasticNet regularization[0m
[32m- prediction on single instance[0m
[32m- linear regression model with constant label[0m
[32m- regularized linear regression through origin with constant label[0m
[32m- linear regression with l-bfgs when training is not needed[0m
[32m- linear regression model training summary[0m
[32m- linear regression model testset evaluation summary[0m
[32m- linear regression with weighted samples[0m
[32m- linear regression model with l-bfgs with big feature datasets[0m
[32m- linear regression summary with weighted samples and intercept by normal solver[0m
[32m- linear regression summary with weighted samples and w/o intercept by normal solver[0m
[32m- read/write[0m
[32m- pmml export[0m
[32m- should support all NumericType labels and weights, and not support other types[0m
[32m- linear regression (huber loss) with intercept without regularization[0m
[32m- linear regression (huber loss) without intercept without regularization[0m
[32m- linear regression (huber loss) with intercept with L2 regularization[0m
[32m- linear regression (huber loss) without intercept with L2 regularization[0m
[32m- huber loss model match squared error for large epsilon[0m
[32mALSStorageSuite:[0m
[32m- invalid storage params[0m
[32m- default and non-default storage params set correct RDD StorageLevels[0m
[32mPredictorSuite:[0m
[32m- should support all NumericType labels and weights, and not support other types[0m
[32mHypothesisTestSuite:[0m
[32m- chi squared pearson goodness of fit[0m
[32m- chi squared pearson matrix independence[0m
[32m- chi squared pearson RDD[LabeledPoint][0m
[32m- 1 sample Kolmogorov-Smirnov test: apache commons math3 implementation equivalence[0m
[32m- 1 sample Kolmogorov-Smirnov test: R implementation equivalence[0m
[32mRWrapperUtilsSuite:[0m
[32m- avoid libsvm data column name conflicting[0m
[32mProbabilisticClassifierSuite:[0m
[32m- test thresholding[0m
[32m- test thresholding not required[0m
[32m- test tiebreak[0m
[32m- test one zero threshold[0m
[32m- bad thresholds[0m
[32m- normalizeToProbabilitiesInPlace[0m
[32mVectorSizeHintSuite:[0m
[32m- Test Param Validators[0m
[32m- Required params must be set before transform.[0m
[32m- Adding size to column of vectors.[0m
[32m- Size hint preserves attributes.[0m
[32m- Size mismatch between current and target size raises an error.[0m
[32m- Handle invalid does the right thing.[0m
[32m- read/write[0m
[32mVectorsSuite:[0m
[32m- kryo class register[0m
[32m- dense vector construction with varargs[0m
[32m- dense vector construction from a double array[0m
[32m- sparse vector construction[0m
[32m- sparse vector construction with unordered elements[0m
[32m- sparse vector construction with mismatched indices/values array[0m
[32m- sparse vector construction with too many indices vs size[0m
[32m- dense to array[0m
[32m- dense argmax[0m
[32m- sparse to array[0m
[32m- sparse argmax[0m
[32m- vector equals[0m
[32m- vectors equals with explicit 0[0m
[32m- indexing dense vectors[0m
[32m- indexing sparse vectors[0m
[32m- parse vectors[0m
[32m- zeros[0m
[32m- Vector.copy[0m
[32m- VectorUDT[0m
[32m- fromBreeze[0m
[32m- sqdist[0m
[32m- foreachActive[0m
[32m- vector p-norm[0m
[32m- Vector numActive and numNonzeros[0m
[32m- Vector toSparse and toDense[0m
[32m- Vector.compressed[0m
[32m- SparseVector.slice[0m
[32m- toJson/fromJson[0m
[32m- conversions between new local linalg and mllib linalg[0m
[32m- implicit conversions between new local linalg and mllib linalg[0m
[32m- sparse vector only support non-negative length[0m
[32mAssociationRulesSuite:[0m
[32m- association rules using String type[0m
[32mHashingTFSuite:[0m
[32m- params[0m
[32m- hashingTF[0m
[32m- applying binary term freqs[0m
[32m- read/write[0m
[32mLabeledPointSuite:[0m
[32m- Kryo class register[0m
[32mNaiveBayesSuite:[0m
[32m- model types[0m
[32m- params[0m
[32m- naive bayes: default params[0m
[32m- Naive Bayes Multinomial[0m
[32m- prediction on single instance[0m
[32m- Naive Bayes with weighted samples[0m
[32m- Naive Bayes Bernoulli[0m
[32m- detect negative values[0m
[32m- detect non zero or one values in Bernoulli[0m
[32m- read/write[0m
[32m- should support all NumericType labels and weights, and not support other types[0m
[32mDifferentiableLossAggregatorSuite:[0m
[32m- empty aggregator[0m
[32m- aggregator initialization[0m
[32m- merge aggregators[0m
[32m- loss, gradient, weight[0m
[32mIndexedRowMatrixSuite:[0m
[32m- size[0m
[32m- empty rows[0m
[32m- toBreeze[0m
[32m- toRowMatrix[0m
[32m- toCoordinateMatrix[0m
[32m- toBlockMatrix dense backing[0m
[32m- toBlockMatrix sparse backing[0m
[32m- toBlockMatrix mixed backing[0m
[32m- multiply a local matrix[0m
[32m- gram[0m
[32m- svd[0m
[32m- validate matrix sizes of svd[0m
[32m- validate k in svd[0m
[32m- similar columns[0m
[32mDecisionTreeSuite:[0m
[32m- Binary classification stump with ordered categorical features[0m
[32m- Regression stump with 3-ary (ordered) categorical features[0m
[32m- Regression stump with binary (ordered) categorical features[0m
[32m- Binary classification stump with fixed label 0 for Gini[0m
[32m- Binary classification stump with fixed label 1 for Gini[0m
[32m- Binary classification stump with fixed label 0 for Entropy[0m
[32m- Binary classification stump with fixed label 1 for Entropy[0m
[32m- Multiclass classification stump with 3-ary (unordered) categorical features[0m
[32m- Binary classification stump with 1 continuous feature, to check off-by-1 error[0m
[32m- Binary classification stump with 2 continuous features[0m
[32m- Multiclass classification stump with unordered categorical features, with just enough bins[0m
[32m- Multiclass classification stump with continuous features[0m
[32m- Multiclass classification stump with continuous + unordered categorical features[0m
[32m- Multiclass classification stump with 10-ary (ordered) categorical features[0m
[32m- Multiclass classification tree with 10-ary (ordered) categorical features, with just enough bins[0m
[32m- split must satisfy min instances per node requirements[0m
[32m- do not choose split that does not satisfy min instance per node requirements[0m
[32m- split must satisfy min info gain requirements[0m
[32m- Node.subtreeIterator[0m
[32m- model save/load[0m
[32mWord2VecSuite:[0m
[32m- params[0m
[32m- Word2Vec[0m
[32m- getVectors[0m
[32m- findSynonyms[0m
[32m- window size[0m
[32m- Word2Vec read/write numPartitions calculation[0m
[32m- Word2Vec read/write[0m
[32m- Word2VecModel read/write[0m
[32m- Word2Vec works with input that is non-nullable (NGram)[0m
[32mMinHashLSHSuite:[0m
[32m- params[0m
[32m- setters[0m
[32m- MinHashLSH: default params[0m
[32m- read/write[0m
[32m- Model copy and uid checks[0m
[32m- hashFunction[0m
[32m- hashFunction: empty vector[0m
[32m- keyDistance[0m
[32m- MinHashLSH: test of LSH property[0m
[32m- MinHashLSH: test of inputDim > prime[0m
[32m- approxNearestNeighbors for min hash[0m
[32m- approxNearestNeighbors for numNeighbors <= 0[0m
[32m- approxSimilarityJoin for min hash on different dataset[0m
[32m- MinHashLSHModel.transform should work with Structured Streaming[0m
[32mPythonMLLibAPISuite:[0m
[32m- pickle vector[0m
[32m- pickle labeled point[0m
[32m- pickle double[0m
[32m- pickle matrix[0m
[32m- pickle rating[0m
[32mLabeledPointSuite:[0m
[32m- parse labeled points[0m
[32m- parse labeled points with whitespaces[0m
[32m- parse labeled points with v0.9 format[0m
[32m- conversions between new ml LabeledPoint and mllib LabeledPoint[0m
[32m- Kryo class register[0m
[32mFPGrowthSuite:[0m
[32m- FPGrowth fit and transform with different data types[0m
[32m- FPGrowth getFreqItems[0m
[32m- FPGrowth getFreqItems with Null[0m
[32m- FPGrowth prediction should not contain duplicates[0m
[32m- FPGrowthModel setMinConfidence should affect rules generation and transform[0m
[32m- FPGrowth parameter check[0m
[32m- read/write[0m
[32mRandomRDDsSuite:[0m
[32m- RandomRDD sizes[0m
[32m- randomRDD for different distributions[0m
[32m- randomVectorRDD for different distributions[0m
[32mAreaUnderCurveSuite:[0m
[32m- auc computation[0m
[32m- auc of an empty curve[0m
[32m- auc of a curve with a single point[0m
[32mRDDLossFunctionSuite:[0m
[32m- regularization[0m
[32m- empty RDD[0m
[32m- versus aggregating on an iterable[0m
[32mRFormulaParserSuite:[0m
[32m- parse simple formulas[0m
[32m- parse dot[0m
[32m- parse deletion[0m
[32m- parse additions and deletions in order[0m
[32m- dot ignores complex column types[0m
[32m- parse intercept[0m
[32m- parse interactions[0m
[32m- parse basic interactions with dot[0m
[32m- parse all to all iris interactions[0m
[32m- parse interaction negation with iris[0m
[32mImputerSuite:[0m
[32m- Imputer for Double with default missing Value NaN[0m
[32m- Imputer should handle NaNs when computing surrogate value, if missingValue is not NaN[0m
[32m- Imputer for Float with missing Value -1.0[0m
[32m- Imputer should impute null as well as 'missingValue'[0m
[32m- Imputer should work with Structured Streaming[0m
[32m- Imputer throws exception when surrogate cannot be computed[0m
[32m- Imputer input & output column validation[0m
[32m- Imputer read/write[0m
[32m- ImputerModel read/write[0m
[32mSQLTransformerSuite:[0m
[32m- params[0m
[32m- transform numeric data[0m
[32m- read/write[0m
[32m- transformSchema[0m
[32m- SPARK-22538: SQLTransformer should not unpersist given dataset[0m
[32mMinMaxScalerSuite:[0m
[32m- MinMaxScaler fit basic case[0m
[32m- MinMaxScaler arguments max must be larger than min[0m
[32m- MinMaxScaler read/write[0m
[32m- MinMaxScalerModel read/write[0m
[32m- MinMaxScaler should remain NaN value[0m
[32mBlockMatrixSuite:[0m
[32m- size[0m
[32m- grid partitioner[0m
[32m- toCoordinateMatrix[0m
[32m- toIndexedRowMatrix[0m
[32m- toBreeze and toLocalMatrix[0m
[32m- add[0m
[32m- subtract[0m
[32m- multiply[0m
[32m- simulate multiply[0m
[32m- validate[0m
[32m- transpose[0m
[32mImageFileFormatSuite:[0m
[32m- image datasource count test[0m
[32m- image datasource test: read jpg image[0m
[32m- image datasource test: read png image[0m
[32m- image datasource test: read non image[0m
[32m- image datasource partition test[0m
[32m- readImages pixel values test[0m
[32mIterativelyReweightedLeastSquaresSuite:[0m
[32m- IRLS against GLM with Binomial errors[0m
[32m- IRLS against GLM with Poisson errors[0m
[32m- IRLS against L1Regression[0m
[32mTestingUtilsSuite:[0m
[32m- Comparing doubles using relative error.[0m
[32m- Comparing doubles using absolute error.[0m
[32m- Comparing vectors using relative error.[0m
[32m- Comparing vectors using absolute error.[0m
[32m- Comparing Matrices using absolute error.[0m
[32m- Comparing Matrices using relative error.[0m
[32mTreePointSuite:[0m
[32m- Kryo class register[0m
[32mTopByKeyAggregatorSuite:[0m
[32m- topByKey with k < #items[0m
[32m- topByKey with k > #items[0m
[32mStandardScalerSuite:[0m
[32m- Standardization with dense input when means and stds are provided[0m
[32m- Standardization with dense input[0m
[32m- Standardization with sparse input when means and stds are provided[0m
[32m- Standardization with sparse input[0m
[32m- Standardization with constant input when means and stds are provided[0m
[32m- Standardization with constant input[0m
[32m- StandardScalerModel argument nulls are properly handled[0m
[32mDecisionTreeRegressorSuite:[0m
[32m- Regression stump with 3-ary (ordered) categorical features[0m
[32m- Regression stump with binary (ordered) categorical features[0m
[32m- copied model must have the same parent[0m
[32m- predictVariance[0m
[32m- Feature importance with toy data[0m
[32m- prediction on single instance[0m
[32m- should support all NumericType labels and not support other types[0m
[32m- read/write[0m
[32mRowMatrixSuite:[0m
[32m- size[0m
[32m- empty rows[0m
[32m- toBreeze[0m
[32m- gram[0m
[32m- similar columns[0m
[32m- svd of a full-rank matrix[0m
[32m- svd of a low-rank matrix[0m
[32m- validate k in svd[0m
[32m- pca[0m
[32m- multiply a local matrix[0m
[32m- compute column summary statistics[0m
[32m- QR Decomposition[0m
[32m- compute covariance[0m
[32m- covariance matrix is symmetric (SPARK-10875)[0m
[32m- QR decomposition should aware of empty partition (SPARK-16369)[0m
[32mNaiveBayesSuite:[0m
[32m- model types[0m
[32m- get, set params[0m
[32m- Naive Bayes Multinomial[0m
[32m- Naive Bayes Bernoulli[0m
[32m- detect negative values[0m
[32m- detect non zero or one values in Bernoulli[0m
[32m- model save/load: 2.0 to 2.0[0m
[32m- model save/load: 1.0 to 2.0[0m
[32mNNLSSuite:[0m
[32m- NNLS: exact solution cases[0m
[32m- NNLS: nonnegativity constraint active[0m
[32m- NNLS: objective value test[0m
[32mGradientDescentSuite:[0m
[32m- Assert the loss is decreasing.[0m
[32m- Test the loss and gradient of first iteration with regularization.[0m
[32m- iteration should end with convergence tolerance[0m
[32mVectorSizeHintStreamingSuite:[0m
[32m- Test assemble vectors with size hint in streaming.[0m
[32mPCASuite:[0m
[32m- Correct computing use a PCA wrapper[0m
[32m- memory cost computation[0m
[32mIdentifiableSuite:[0m
[32m- Identifiable[0m
[32mNGramSuite:[0m
[32m- default behavior yields bigram features[0m
[32m- NGramLength=4 yields length 4 n-grams[0m
[32m- empty input yields empty output[0m
[32m- input array < n yields empty output[0m
[32m- read/write[0m
[32mGeneralizedLinearRegressionSuite:[0m
[33m- export test data into CSV format !!! IGNORED !!![0m
[32m- params[0m
[32m- generalized linear regression: default params[0m
[32m- prediction on single instance[0m
[32m- generalized linear regression: gaussian family against glm[0m
[32m- generalized linear regression: gaussian family against glmnet[0m
[32m- generalized linear regression: binomial family against glm[0m
[32m- generalized linear regression: poisson family against glm[0m
[32m- generalized linear regression: poisson family against glm (with zero values)[0m
[32m- generalized linear regression: gamma family against glm[0m
[32m- generalized linear regression: tweedie family against glm[0m
[32m- generalized linear regression: tweedie family against glm (default power link)[0m
[32m- generalized linear regression: intercept only[0m
[32m- generalized linear regression with weight and offset[0m
[32m- glm summary: gaussian family with weight and offset[0m
[32m- glm summary: binomial family with weight and offset[0m
[32m- glm summary: poisson family with weight and offset[0m
[32m- glm summary: gamma family with weight and offset[0m
[32m- glm summary: tweedie family with weight and offset[0m
[32m- glm handle collinear features[0m
[32m- read/write[0m
[32m- should support all NumericType labels and weights, and not support other types[0m
[32m- glm accepts Dataset[LabeledPoint][0m
[32m- glm summary: feature name[0m
[32m- glm summary: coefficient with statistics[0m
[32m- generalized linear regression: regularization parameter[0m
[32m- evaluate with labels that are not doubles[0m
[32m- SPARK-23131 Kryo raises StackOverflow during serializing GLR model[0m
[32mPMMLModelExportFactorySuite:[0m
[32m- PMMLModelExportFactory create KMeansPMMLModelExport when passing a KMeansModel[0m
[32m- PMMLModelExportFactory create GeneralizedLinearPMMLModelExport when passing a LinearRegressionModel, RidgeRegressionModel or LassoModel[0m
[32m- PMMLModelExportFactory create BinaryClassificationPMMLModelExport when passing a LogisticRegressionModel or SVMModel[0m
[32m- PMMLModelExportFactory throw IllegalArgumentException when passing a Multinomial Logistic Regression[0m
[32m- PMMLModelExportFactory throw IllegalArgumentException when passing an unsupported model[0m
[32mTokenizerSuite:[0m
[32m- params[0m
[32m- read/write[0m
[32mRandomForestRegressorSuite:[0m
[32m- Regression with continuous features: comparing DecisionTree vs. RandomForest(numTrees = 1)[0m
[32m- Regression with continuous features and node Id cache : comparing DecisionTree vs. RandomForest(numTrees = 1)[0m
[32m- prediction on single instance[0m
[32m- Feature importance with toy data[0m
[32m- should support all NumericType labels and not support other types[0m
[32m- read/write[0m
[32mBinaryClassificationMetricsSuite:[0m
[32m- binary evaluation metrics[0m
[32m- binary evaluation metrics for RDD where all examples have positive label[0m
[32m- binary evaluation metrics for RDD where all examples have negative label[0m
[32m- binary evaluation metrics with downsampling[0m
[32mCountVectorizerSuite:[0m
[32m- params[0m
[32m- CountVectorizerModel common cases[0m
[32m- CountVectorizer common cases[0m
[32m- CountVectorizer vocabSize and minDF[0m
[32m- CountVectorizer maxDF[0m
[32m- CountVectorizer using both minDF and maxDF[0m
[32m- CountVectorizer throws exception when vocab is empty[0m
[32m- CountVectorizerModel with minTF count[0m
[32m- CountVectorizerModel with minTF freq[0m
[32m- CountVectorizerModel and CountVectorizer with binary[0m
[32m- CountVectorizer read/write[0m
[32m- CountVectorizerModel read/write[0m
[32m- SPARK-22974: CountVectorModel should attach proper attribute to output column[0m
[32mNormalizerSuite:[0m
[32m- Normalization using L1 distance[0m
[32m- Normalization using L2 distance[0m
[32m- Normalization using L^Inf distance.[0m
[32mRidgeRegressionClusterSuite:[0m
[32m- task size should be small in both training and prediction[0m
[32mALSSuite:[0m
[32m- rank-1 matrices[0m
[32m- rank-1 matrices bulk[0m
[32m- rank-2 matrices[0m
[32m- rank-2 matrices bulk[0m
[32m- rank-1 matrices implicit[0m
[32m- rank-1 matrices implicit bulk[0m
[32m- rank-2 matrices implicit[0m
[32m- rank-2 matrices implicit bulk[0m
[32m- rank-2 matrices implicit negative[0m
[32m- rank-2 matrices with different user and product blocks[0m
[32m- pseudorandomness[0m
[32m- Storage Level for RDDs in model[0m
[32m- negative ids[0m
[32m- NNALS, rank 2[0m
[32m- SPARK-18268: ALS with empty RDD should fail with better message[0m
[32mALSCleanerSuite:[0m
[32m- ALS shuffle cleanup standalone[0m
[32m- ALS shuffle cleanup in algorithm[0m
[32mRFormulaSuite:[0m
[32m- params[0m
[32m- transform numeric data[0m
[32m- features column already exists[0m
[32m- label column already exists and forceIndexLabel was set with false[0m
[32m- label column already exists but forceIndexLabel was set with true[0m
[32m- label column already exists but is not numeric type[0m
[32m- allow missing label column for test datasets[0m
[32m- allow empty label[0m
[32m- encodes string terms[0m
[32m- encodes string terms with string indexer order type[0m
[32m- test consistency with R when encoding string terms[0m
[32m- formula w/o intercept, we should output reference category when encoding string terms[0m
[32m- index string label[0m
[32m- force to index label even it is numeric type[0m
[32m- attribute generation[0m
[32m- vector attribute generation[0m
[32m- vector attribute generation with unnamed input attrs[0m
[32m- numeric interaction[0m
[32m- factor numeric interaction[0m
[32m- factor factor interaction[0m
[32m- read/write: RFormula[0m
[32m- read/write: RFormulaModel[0m
[32m- should support all NumericType labels[0m
[32m- handle unseen features or labels[0m
[32m- Use Vectors as inputs to formula.[0m
[32m- SPARK-23562 RFormula handleInvalid should handle invalid values in non-string columns.[0m
[32mQuantileDiscretizerSuite:[0m
[32m- Test observed number of buckets and their sizes match expected values[0m
[32m- Test on data with high proportion of duplicated values[0m
[32m- Test transform on data with NaN value[0m
[32m- Test transform method on unseen data[0m
[32m- read/write[0m
[32m- Verify resulting model has parent[0m
[32m- Multiple Columns: Test observed number of buckets and their sizes match expected values[0m
[32m- Multiple Columns: Test on data with high proportion of duplicated values[0m
[32m- Multiple Columns: Test transform on data with NaN value[0m
[32m- Multiple Columns: Test numBucketsArray[0m
[32m- Multiple Columns: Compare single/multiple column(s) QuantileDiscretizer in pipeline[0m
[32m- Multiple Columns: Comparing setting numBuckets with setting numBucketsArray explicitly with identical values[0m
[32m- Multiple Columns: read/write[0m
[32m- Multiple Columns: Both inputCol and inputCols are set[0m
[32m- Multiple Columns: Mismatched sizes of inputCols / outputCols[0m
[32mNormalizerSuite:[0m
[32m- Normalization with default parameter[0m
[32m- Normalization with setter[0m
[32m- read/write[0m
[32mChiSqSelectorSuite:[0m
[32m- ChiSqSelector transform by numTopFeatures test (sparse & dense vector)[0m
[32m- ChiSqSelector transform by Percentile test (sparse & dense vector)[0m
[32m- ChiSqSelector transform by FPR test (sparse & dense vector)[0m
[32m- ChiSqSelector transform by FDR test (sparse & dense vector)[0m
[32m- ChiSqSelector transform by FWE test (sparse & dense vector)[0m
[32m- model load / save[0m
[32mGBTRegressorSuite:[0m
[32m- Regression with continuous features[0m
[32m- GBTRegressor behaves reasonably on toy data[0m
[32m- prediction on single instance[0m
[32m- Checkpointing[0m
[32m- should support all NumericType labels and not support other types[0m
[32m- Feature importance with toy data[0m
[32m- Tests of feature subset strategy[0m
[32m- model evaluateEachIteration[0m
[32m- runWithValidation stops early and performs better on a validation dataset[0m
[32m- model save/load[0m
[32mInteractionSuite:[0m
[32m- params[0m
[32m- feature encoder[0m
[32m- numeric interaction[0m
[32m- nominal interaction[0m
[32m- default attr names[0m
[32m- read/write[0m
[32mImpuritySuite:[0m
[32m- Gini impurity does not support negative labels[0m
[32m- Entropy does not support negative labels[0m
[32mBisectingKMeansSuite:[0m
[32m- default values[0m
[32m- setter/getter[0m
[32m- 1D data[0m
[32m- points are the same[0m
[32m- more desired clusters than points[0m
[32m- min divisible cluster[0m
[32m- larger clusters get selected first[0m
[32m- 2D data[0m
[32m- BisectingKMeans model save/load[0m
[32mKMeansPMMLModelExportSuite:[0m
[32m- KMeansPMMLModelExport generate PMML format[0m
[32mGradientBoostedTreesSuite:[0m
[32m- Regression with continuous features: SquaredError[0m
[32m- Regression with continuous features: Absolute Error[0m
[32m- Binary classification with continuous features: Log Loss[0m
[32m- SPARK-5496: BoostingStrategy.defaultParams should recognize Classification[0m
[32m- model save/load[0m
[32m- Checkpointing[0m
[32mIDFSuite:[0m
[32m- idf[0m
[32m- idf minimum document frequency filtering[0m
[32mVectorUDTSuite:[0m
[32m- preloaded VectorUDT[0m
[32m- JavaTypeInference with VectorUDT[0m
[32mHuberAggregatorSuite:[0m
[32m- aggregator add method should check input size[0m
[32m- negative weight[0m
[32m- check sizes[0m
[32m- check correctness[0m
[32m- check with zero standard deviation[0m
[32mRegexTokenizerSuite:[0m
[32m- params[0m
[32m- RegexTokenizer[0m
[32m- RegexTokenizer with toLowercase false[0m
[32m- read/write[0m
[32mClassifierSuite:[0m
[32m- extractLabeledPoints[0m
[32m- getNumClasses[0m
[32mBinaryClassificationPMMLModelExportSuite:[0m
[32m- logistic regression PMML export[0m
[32m- linear SVM PMML export[0m
[32mBLASSuite:[0m
[32m- copy[0m
[32m- scal[0m
[32m- axpy[0m
[32m- dot[0m
[32m- spr[0m
[32m- syr[0m
[32m- gemm[0m
[32m- gemv[0m
[32mChiSqSelectorSuite:[0m
[32m- params[0m
[32m- Test Chi-Square selector: numTopFeatures[0m
[32m- Test Chi-Square selector: percentile[0m
[32m- Test Chi-Square selector: fpr[0m
[32m- Test Chi-Square selector: fdr[0m
[32m- Test Chi-Square selector: fwe[0m
[32m- read/write[0m
[32m- should support all NumericType labels and not support other types[0m
[32m- SPARK-25289: ChiSqSelector should not fail when selecting no features with FDR[0m
[32mDifferentiableRegularizationSuite:[0m
[32m- L2 regularization[0m
[32mRidgeRegressionSuite:[0m
[32m- ridge regression can help avoid overfitting[0m
[32m- model save/load[0m
[32mLBFGSSuite:[0m
[32m- LBFGS loss should be decreasing and match the result of Gradient Descent.[0m
[32m- LBFGS and Gradient Descent with L2 regularization should get the same result.[0m
[32m- The convergence criteria should work as we expect.[0m
[32m- Optimize via class LBFGS.[0m
[32m- SPARK-18471: LBFGS aggregator on empty partitions[0m
[32mGradientSuite:[0m
[32m- Gradient computation against numerical differentiation[0m
[32mKMeansSuite:[0m
[32m- default parameters[0m
[32m- set parameters[0m
[32m- parameters validation[0m
[32m- fit, transform and summary[0m
[32m- KMeansModel transform with non-default feature and prediction cols[0m
[32m- KMeans using cosine distance[0m
[32m- KMeans with cosine distance is not supported for 0-length vectors[0m
[32m- KMean with Array input[0m
[32m- read/write[0m
[32m- pmml export[0m
[32mClusteringEvaluatorSuite:[0m
[32m- params[0m
[32m- read/write[0m
[32m- squared euclidean Silhouette[0m
[32m- cosine Silhouette[0m
[32m- number of clusters must be greater than one[0m
[32m- SPARK-23568: we should use metadata to determine features number[0m
[32mPolynomialExpansionSuite:[0m
[32m- params[0m
[32m- Polynomial expansion with default parameter[0m
[32m- Polynomial expansion with setter[0m
[32m- Polynomial expansion with degree 1 is identity on vectors[0m
[32m- read/write[0m
[32m- SPARK-17027. Integer overflow in PolynomialExpansion.getPolySize[0m
[32mHingeAggregatorSuite:[0m
[32m- aggregator add method input size[0m
[32m- negative weight[0m
[32m- check sizes[0m
[32m- check correctness[0m
[32m- check with zero standard deviation[0m
[32mStopWordsRemoverSuite:[0m
[32m- StopWordsRemover default[0m
[32m- StopWordsRemover with particular stop words list[0m
[32m- StopWordsRemover with localed input (case insensitive)[0m
[32m- StopWordsRemover with localed input (case sensitive)[0m
[32m- StopWordsRemover with invalid locale[0m
[32m- StopWordsRemover case sensitive[0m
[32m- default stop words of supported languages are not empty[0m
[32m- StopWordsRemover with language selection[0m
[32m- StopWordsRemover with ignored words[0m
[32m- StopWordsRemover with additional words[0m
[32m- read/write[0m
[32m- StopWordsRemover output column already exists[0m
[32mLDASuite:[0m
[32m- LocalLDAModel[0m
[32m- running and DistributedLDAModel with default Optimizer (EM)[0m
[32m- vertex indexing[0m
[32m- setter alias[0m
[32m- initializing with alpha length != k or 1 fails[0m
[32m- initializing with elements in alpha < 0 fails[0m
[32m- OnlineLDAOptimizer initialization[0m
[32m- OnlineLDAOptimizer one iteration[0m
[32m- OnlineLDAOptimizer with toy data[0m
[32m- LocalLDAModel logLikelihood[0m
[32m- LocalLDAModel logPerplexity[0m
[32m- LocalLDAModel predict[0m
[32m- OnlineLDAOptimizer with asymmetric prior[0m
[32m- OnlineLDAOptimizer alpha hyperparameter optimization[0m
[32m- model save/load[0m
[32m- EMLDAOptimizer with empty docs[0m
[32m- OnlineLDAOptimizer with empty docs[0m
[32mLinearRegressionSuite:[0m
[32m- linear regression[0m
[32m- linear regression without intercept[0m
[32m- sparse linear regression without intercept[0m
[32m- model save/load[0m
[32mPowerIterationClusteringSuite:[0m
[32m- default parameters[0m
[32m- parameter validation[0m
[32m- power iteration clustering[0m
[32m- supported input types[0m
[32m- invalid input: negative similarity[0m
[32m- test default weight[0m
[32m- read/write[0m
[32mOneVsRestSuite:[0m
[32m- params[0m
[32m- one-vs-rest: default params[0m
[32m- one-vs-rest: tuning parallelism does not change output[0m
[32m- one-vs-rest: pass label metadata correctly during train[0m
[32m- SPARK-8092: ensure label features and prediction cols are configurable[0m
[32m- SPARK-18625 : OneVsRestModel should support setFeaturesCol and setPredictionCol[0m
[32m- SPARK-8049: OneVsRest shouldn't output temp columns[0m
[32m- SPARK-21306: OneVsRest should support setWeightCol[0m
[32m- OneVsRest.copy and OneVsRestModel.copy[0m
[32m- read/write: OneVsRest[0m
[32m- read/write: OneVsRestModel[0m
[32m- should support all NumericType labels and not support other types[0m
[32mImageSchemaSuite:[0m
[32m- Smoke test: create basic ImageSchema dataframe[0m
[32m- readImages count test[0m
[32m- readImages test: recursive = false[0m
[32m- readImages test: read jpg image[0m
[32m- readImages test: read png image[0m
[32m- readImages test: read non image[0m
[32m- readImages test: read non image and dropImageFailures is false[0m
[32m- readImages test: sampleRatio > 1[0m
[32m- readImages test: sampleRatio < 0[0m
[32m- readImages test: sampleRatio = 0[0m
[32m- readImages test: with sparkSession[0m
[32m- readImages partition test[0m
[32m- readImages partition test: < 0[0m
[32m- readImages partition test: = 0[0m
[32m- readImages pixel values test[0m
[32mLBFGSClusterSuite:[0m
[32m- task size should be small[0m
[32mVectorSlicerSuite:[0m
[32m- params[0m
[32m- feature validity checks[0m
[32m- Test vector slicer[0m
[32m- read/write[0m
[32mRandomForestSuite:[0m
[32m- Binary classification with continuous features: comparing DecisionTree vs. RandomForest(numTrees = 1)[0m
[32m- Binary classification with continuous features and node Id cache : comparing DecisionTree vs. RandomForest(numTrees = 1)[0m
[32m- Regression with continuous features: comparing DecisionTree vs. RandomForest(numTrees = 1)[0m
[32m- Regression with continuous features and node Id cache : comparing DecisionTree vs. RandomForest(numTrees = 1)[0m
[32m- alternating categorical and continuous features with multiclass labels to test indexing[0m
[32m- subsampling rate in RandomForest[0m
[32m- model save/load[0m
[32mMLSerDeSuite:[0m
[32m- pickle vector[0m
[32m- pickle double[0m
[32m- pickle matrix[0m
[32mStreamingLinearRegressionSuite:[0m
[32m- parameter accuracy[0m
[32m- parameter convergence[0m
[32m- predictions[0m
[32m- training and prediction[0m
[32m- handling empty RDDs in a stream[0m
[32mRandomForestClassifierSuite:[0m
[32m- params[0m
[32m- Binary classification with continuous features: comparing DecisionTree vs. RandomForest(numTrees = 1)[0m
[32m- Binary classification with continuous features and node Id cache: comparing DecisionTree vs. RandomForest(numTrees = 1)[0m
[32m- alternating categorical and continuous features with multiclass labels to test indexing[0m
[32m- subsampling rate in RandomForest[0m
[32m- predictRaw and predictProbability[0m
[32m- prediction on single instance[0m
[32m- Fitting without numClasses in metadata[0m
[32m- Feature importance with toy data[0m
[32m- should support all NumericType labels and not support other types[0m
[32m- read/write[0m
[32mPrefixSpanSuite:[0m
[32m- PrefixSpan projections with multiple partial starts[0m
[32m- PrefixSpan Integer type, variable-size itemsets[0m
[32m- PrefixSpan input row with nulls[0m
[32m- PrefixSpan String type, variable-size itemsets[0m
[32mStringIndexerSuite:[0m
[32m- params[0m
[32m- StringIndexer[0m
[32m- StringIndexerUnseen[0m
[32m- StringIndexer with a numeric input column[0m
[32m- StringIndexer with NULLs[0m
[32m- StringIndexerModel should keep silent if the input column does not exist.[0m
[32m- StringIndexerModel can't overwrite output column[0m
[32m- StringIndexer read/write[0m
[32m- StringIndexerModel read/write[0m
[32m- IndexToString params[0m
[32m- IndexToString.transform[0m
[32m- StringIndexer, IndexToString are inverses[0m
[32m- IndexToString.transformSchema (SPARK-10573)[0m
[32m- IndexToString read/write[0m
[32m- SPARK 18698: construct IndexToString with custom uid[0m
[32m- StringIndexer metadata[0m
[32m- StringIndexer order types[0m
[32m- SPARK-22446: StringIndexerModel's indexer UDF should not apply on filtered data[0m
[32mSummarizerSuite:[0m
[32m- no element[0m
[32m- single element - mean only[0m
[32m- single element - mean only w/o weight[0m
[32m- single element - variance only[0m
[32m- single element - variance only w/o weight[0m
[32m- single element - count only[0m
[32m- single element - count only w/o weight[0m
[32m- single element - numNonZeros only[0m
[32m- single element - numNonZeros only w/o weight[0m
[32m- single element - min only[0m
[32m- single element - min only w/o weight[0m
[32m- single element - max only[0m
[32m- single element - max only w/o weight[0m
[32m- single element - normL1 only[0m
[32m- single element - normL1 only w/o weight[0m
[32m- single element - normL2 only[0m
[32m- single element - normL2 only w/o weight[0m
[32m- single element - multiple metrics at once[0m
[32m- single element - multiple metrics at once w/o weight[0m
[32m- multiple elements (dense) - mean only[0m
[32m- multiple elements (dense) - mean only w/o weight[0m
[32m- multiple elements (dense) - variance only[0m
[32m- multiple elements (dense) - variance only w/o weight[0m
[32m- multiple elements (dense) - count only[0m
[32m- multiple elements (dense) - count only w/o weight[0m
[32m- multiple elements (dense) - numNonZeros only[0m
[32m- multiple elements (dense) - numNonZeros only w/o weight[0m
[32m- multiple elements (dense) - min only[0m
[32m- multiple elements (dense) - min only w/o weight[0m
[32m- multiple elements (dense) - max only[0m
[32m- multiple elements (dense) - max only w/o weight[0m
[32m- multiple elements (dense) - normL1 only[0m
[32m- multiple elements (dense) - normL1 only w/o weight[0m
[32m- multiple elements (dense) - normL2 only[0m
[32m- multiple elements (dense) - normL2 only w/o weight[0m
[32m- multiple elements (dense) - multiple metrics at once[0m
[32m- multiple elements (dense) - multiple metrics at once w/o weight[0m
[32m- multiple elements (sparse) - mean only[0m
[32m- multiple elements (sparse) - mean only w/o weight[0m
[32m- multiple elements (sparse) - variance only[0m
[32m- multiple elements (sparse) - variance only w/o weight[0m
[32m- multiple elements (sparse) - count only[0m
[32m- multiple elements (sparse) - count only w/o weight[0m
[32m- multiple elements (sparse) - numNonZeros only[0m
[32m- multiple elements (sparse) - numNonZeros only w/o weight[0m
[32m- multiple elements (sparse) - min only[0m
[32m- multiple elements (sparse) - min only w/o weight[0m
[32m- multiple elements (sparse) - max only[0m
[32m- multiple elements (sparse) - max only w/o weight[0m
[32m- multiple elements (sparse) - normL1 only[0m
[32m- multiple elements (sparse) - normL1 only w/o weight[0m
[32m- multiple elements (sparse) - normL2 only[0m
[32m- multiple elements (sparse) - normL2 only w/o weight[0m
[32m- multiple elements (sparse) - multiple metrics at once[0m
[32m- multiple elements (sparse) - multiple metrics at once w/o weight[0m
[32m- summarizer buffer basic error handing[0m
[32m- summarizer buffer dense vector input[0m
[32m- summarizer buffer sparse vector input[0m
[32m- summarizer buffer mixing dense and sparse vector input[0m
[32m- summarizer buffer merging two summarizers[0m
[32m- summarizer buffer zero variance test (SPARK-21818)[0m
[32m- summarizer buffer merging summarizer with empty summarizer[0m
[32m- summarizer buffer merging summarizer when one side has zero mean (SPARK-4355)[0m
[32m- summarizer buffer merging summarizer with weighted samples[0m
[32m- summarizer buffer test min/max with weighted samples[0m
[33m- performance test !!! IGNORED !!![0m
[32mBinarizerSuite:[0m
[32m- params[0m
[32m- Binarize continuous features with default parameter[0m
[32m- Binarize continuous features with setter[0m
[32m- Binarize vector of continuous features with default parameter[0m
[32m- Binarize vector of continuous features with setter[0m
[32m- read/write[0m
[32mGaussianMixtureSuite:[0m
[32m- gmm fails on high dimensional data[0m
[32m- single cluster[0m
[32m- two clusters[0m
[32m- two clusters with distributed decompositions[0m
[32m- single cluster with sparse data[0m
[32m- two clusters with sparse data[0m
[32m- model save / load[0m
[32m- model prediction, parallel and local[0m
[32mIDFSuite:[0m
[32m- params[0m
[32m- compute IDF with default parameter[0m
[32m- compute IDF with setter[0m
[32m- IDF read/write[0m
[32m- IDFModel read/write[0m
[32mANNSuite:[0m
[32m- ANN with Sigmoid learns XOR function with LBFGS optimizer[0m
[32m- ANN with SoftMax learns XOR function with 2-bit output and batch GD optimizer[0m
[32mFeatureHasherSuite:[0m
[32m- params[0m
[32m- specify input cols using varargs or array[0m
[32m- feature hashing[0m
[32m- setting explicit numerical columns to treat as categorical[0m
[32m- hashing works for all numeric types[0m
[32m- invalid input type should fail[0m
[32m- hash collisions sum feature values[0m
[32m- ignores null values in feature hashing[0m
[32m- unicode column names and values[0m
[32m- read/write[0m
[32mMatrixUDTSuite:[0m
[32m- preloaded MatrixUDT[0m
[32mVectorAssemblerSuite:[0m
[32m- params[0m
[32m- assemble[0m
[32m- assemble should compress vectors[0m
[32m- VectorAssembler[0m
[32m- transform should throw an exception in case of unsupported type[0m
[32m- ML attributes[0m
[32m- read/write[0m
[32m- SPARK-22446: VectorAssembler's UDF should not apply on filtered data[0m
[32m- assemble should keep nulls when keepInvalid is true[0m
[32m- assemble should throw errors when keepInvalid is false[0m
[32m- get lengths functions[0m
[32m- Handle Invalid should behave properly[0m
[32m- SPARK-25371: VectorAssembler with empty inputCols[0m
[32mWeightedLeastSquaresSuite:[0m
[32m- WLS with strong L1 regularization[0m
[32m- diagonal inverse of AtWA[0m
[32m- two collinear features[0m
[32m- WLS against lm[0m
[32m- WLS against lm when label is constant and no regularization[0m
[32m- WLS with regularization when label is constant[0m
[32m- WLS against glmnet with constant features[0m
[32m- WLS against glmnet with L1/ElasticNet regularization[0m
[32m- WLS against glmnet with L2 regularization[0m
[32mElementwiseProductSuite:[0m
[32m- elementwise (hadamard) product should properly apply vector to dense data set[0m
[32m- elementwise (hadamard) product should properly apply vector to sparse data set[0m
[32mBaggedPointSuite:[0m
[32m- BaggedPoint RDD: without subsampling[0m
[32m- BaggedPoint RDD: with subsampling with replacement (fraction = 1.0)[0m
[32m- BaggedPoint RDD: with subsampling with replacement (fraction = 0.5)[0m
[32m- BaggedPoint RDD: with subsampling without replacement (fraction = 1.0)[0m
[32m- BaggedPoint RDD: with subsampling without replacement (fraction = 0.5)[0m
[32mMultivariateOnlineSummarizerSuite:[0m
[32m- basic error handing[0m
[32m- dense vector input[0m
[32m- sparse vector input[0m
[32m- mixing dense and sparse vector input[0m
[32m- merging two summarizers[0m
[32m- merging summarizer with empty summarizer[0m
[32m- merging summarizer when one side has zero mean (SPARK-4355)[0m
[32m- merging summarizer with weighted samples[0m
[32m- test min/max with weighted samples (SPARK-16561)[0m
[32m- test zero variance (SPARK-21818)[0m
[32mLassoSuite:[0m
[32m- Lasso local random SGD[0m
[32m- Lasso local random SGD with initial weights[0m
[32m- model save/load[0m
[32mMLPairRDDFunctionsSuite:[0m
[32m- topByKey[0m
[32mLogisticAggregatorSuite:[0m
[32m- aggregator add method input size[0m
[32m- negative weight[0m
[32m- check sizes multinomial[0m
[32m- check sizes binomial[0m
[32m- check correctness multinomial[0m
[32m- check correctness binomial[0m
[32m- check with zero standard deviation[0m
[32mFPGrowthSuite:[0m
[32m- FP-Growth using String type[0m
[32m- FP-Growth String type association rule generation[0m
[32m- FP-Growth using Int type[0m
[32m- model save/load with String type[0m
[32m- model save/load with Int type[0m
[32mReadWriteSuite:[0m
[32m- unsupported/non existent export formats[0m
[32m- invalid paths fail[0m
[32m- dummy export format is called[0m
[32m- duplicate format raises error[0m
[36mRun completed in 30 minutes, 15 seconds.[0m
[36mTotal number of tests run: 1414[0m
[36mSuites: completed 194, aborted 0[0m
[36mTests: succeeded 1414, failed 0, canceled 0, ignored 7, pending 0[0m
[32mAll tests passed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------< [0;36morg.apache.spark:spark-tools_2.11[0;1m >------------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Tools 2.4.4                               [16/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/tools/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/tools/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/ow2/asm/asm-tree/5.1/asm-tree-5.1.jar:/home/test/.m2/repository/org/ow2/asm/asm-util/5.1/asm-util-5.1.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/ow2/asm/asm-commons/5.1/asm-commons-5.1.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/org/clapper/grizzled-scala_2.11/4.2.0/grizzled-scala_2.11-4.2.0.jar:/home/test/.m2/repository/org/ow2/asm/asm/5.1/asm-5.1.jar:/home/test/.m2/repository/org/clapper/classutil_2.11/1.1.2/classutil_2.11-1.1.2.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/tools/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:19:48 AM [0.041s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/tools/src/test/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/ow2/asm/asm-tree/5.1/asm-tree-5.1.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/ow2/asm/asm-commons/5.1/asm-commons-5.1.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/org/clapper/grizzled-scala_2.11/4.2.0/grizzled-scala_2.11-4.2.0.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/ow2/asm/asm-util/5.1/asm-util-5.1.jar:/home/test/.m2/repository/org/ow2/asm/asm/5.1/asm-5.1.jar:/home/test/.m2/repository/org/clapper/classutil_2.11/1.1.2/classutil_2.11-1.1.2.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-tools_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-tools_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 215 milliseconds.[0m
[36mRun starting. Expected test count is: 0[0m
[32mDiscoverySuite:[0m
[36mRun completed in 250 milliseconds.[0m
[36mTotal number of tests run: 0[0m
[36mSuites: completed 1, aborted 0[0m
[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m
[33mNo tests were executed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------< [0;36morg.apache.spark:spark-hive_2.11[0;1m >------------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Hive 2.4.4                                [17/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/sql/hive/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/sql/hive/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/iq80/snappy/snappy/0.2/snappy-0.2.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/test/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/.m2/repository/org/apache/calcite/calcite-core/1.2.0-incubating/calcite-core-1.2.0-incubating.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/.m2/repository/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/home/test/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/test/.m2/repository/org/apache/calcite/calcite-avatica/1.2.0-incubating/calcite-avatica-1.2.0-incubating.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/apache/calcite/calcite-linq4j/1.2.0-incubating/calcite-linq4j-1.2.0-incubating.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/net/hydromatic/eigenbase-properties/1.1.5/eigenbase-properties-1.1.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/org/spark-project/hive/hive-metastore/1.2.1.spark2/hive-metastore-1.2.1.spark2.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/org/apache/derby/derby/10.12.1.1/derby-10.12.1.1.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:20:19 AM [20.973s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 9266 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/calcite/calcite-avatica/1.2.0-incubating/calcite-avatica-1.2.0-incubating.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/calcite/calcite-linq4j/1.2.0-incubating/calcite-linq4j-1.2.0-incubating.jar:/home/test/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/net/hydromatic/eigenbase-properties/1.1.5/eigenbase-properties-1.1.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/derby/derby/10.12.1.1/derby-10.12.1.1.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/spark/sql/catalyst/target/spark-catalyst_2.11-2.4.4-tests.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/spark/sql/core/target/spark-sql_2.11-2.4.4-tests.jar:/home/test/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/iq80/snappy/snappy/0.2/snappy-0.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/core/target/scala-2.11/test-classes:/home/test/.m2/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/calcite/calcite-core/1.2.0-incubating/calcite-core-1.2.0-incubating.jar:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/home/test/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/org/spark-project/hive/hive-metastore/1.2.1.spark2/hive-metastore-1.2.1.spark2.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:20:47 AM [27.488s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-hive_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-hive_2.11[0;1m ---[m
[36mDiscovery starting.[0m
22:20:59.477 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22:21:09.514 WARN org.apache.hadoop.hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
22:21:09.906 WARN org.apache.hadoop.hive.metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
[36mDiscovery completed in 28 seconds, 664 milliseconds.[0m
[36mRun starting. Expected test count is: 2691[0m
[32mStatisticsSuite:[0m
22:21:27.066 WARN org.apache.spark.util.SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes
22:21:29.497 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user hadoop: id: 'hadoop': no such user
id: 'hadoop': no such user

22:21:29.497 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:21:29.497 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:21:30.057 WARN org.apache.hadoop.hive.metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
22:21:31.249 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/hive_tbl specified for non-external table:hive_tbl
[32m- SPARK-18856: non-empty partitioned table should not report zero size[0m
22:21:36.482 WARN org.apache.spark.util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
22:21:37.894 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/hive_tbl specified for non-external table:hive_tbl
[32m- conversion from CatalogStatistics to Statistics[0m
22:21:40.387 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/rel_est_hive_table specified for non-external table:rel_est_hive_table
[32m- size estimation for relations is based on row size * number of rows[0m
22:21:43.663 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<page_id:string,impressions:string>) is different from the schema when this table was created by Spark SQL(struct<page_id:int,impressions:int>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:21:43.790 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<page_id:string,impressions:string>) is different from the schema when this table was created by Spark SQL(struct<page_id:int,impressions:int>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:21:43.808 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<page_id:string,impressions:string>) is different from the schema when this table was created by Spark SQL(struct<page_id:int,impressions:int>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:21:43.837 WARN org.apache.spark.sql.hive.DetermineTableStats: Failed to get table size from hdfs.
java.io.FileNotFoundException: File file:/home/test/spark/sql/hive/target/tmp/spark-effba627-9784-40ce-9a1f-bc54c115115c does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:539)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:752)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.hadoop.fs.FileSystem.getContentSummary(FileSystem.java:1443)
	at org.apache.spark.sql.hive.DetermineTableStats$$anonfun$apply$2.applyOrElse(HiveStrategies.scala:126)
	at org.apache.spark.sql.hive.DetermineTableStats$$anonfun$apply$2.applyOrElse(HiveStrategies.scala:117)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$apply$6.apply(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$apply$6.apply(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators(AnalysisHelper.scala:73)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
	at org.apache.spark.sql.hive.DetermineTableStats.apply(HiveStrategies.scala:117)
	at org.apache.spark.sql.hive.DetermineTableStats.apply(HiveStrategies.scala:116)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed$lzycompute(TestHive.scala:589)
	at org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed(TestHive.scala:572)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:628)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:211)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.test.SQLTestUtilsBase$$anonfun$withTable$1.apply(SQLTestUtils.scala:288)
	at org.apache.spark.sql.test.SQLTestUtilsBase$$anonfun$withTable$1.apply(SQLTestUtils.scala:287)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.test.SQLTestUtilsBase$class.withTable(SQLTestUtils.scala:287)
	at org.apache.spark.sql.StatisticsCollectionTestBase.withTable(StatisticsCollectionTestBase.scala:41)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply$mcV$sp(StatisticsSuite.scala:75)
	at org.apache.spark.sql.catalyst.plans.PlanTestBase$class.withSQLConf(PlanTest.scala:198)
	at org.apache.spark.sql.StatisticsCollectionTestBase.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(StatisticsCollectionTestBase.scala:41)
	at org.apache.spark.sql.test.SQLTestUtilsBase$class.withSQLConf(SQLTestUtils.scala:167)
	at org.apache.spark.sql.StatisticsCollectionTestBase.withSQLConf(StatisticsCollectionTestBase.scala:41)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$2.apply$mcV$sp(StatisticsSuite.scala:74)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$2.apply(StatisticsSuite.scala:74)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$2.apply(StatisticsSuite.scala:74)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
[32m- Hive serde tables should fallback to HDFS for size estimation[0m
22:21:43.960 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/non_part_table specified for non-external table:non_part_table
22:21:44.105 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/src specified for non-external table:src
22:21:46.304 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/part_table specified for non-external table:part_table
[32m- analyze Hive serde tables[0m
22:21:49.438 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/checksizetable specified for non-external table:checksizetable
[32m- SPARK-24626 parallel file listing in Stats computation[0m
22:21:51.993 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: Persisting data source table `default`.`parquet_tab` into Hive metastore inSpark SQL specific format, which is NOT compatible with Hive.
[32m- analyze non hive compatible datasource tables[0m
22:21:54.299 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/hive_serde specified for non-external table:hive_serde
[32m- Analyze hive serde tables when schema is not same as schema in table properties[0m
22:21:55.749 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/hive_stats_part_table specified for non-external table:hive_stats_part_table
22:21:58.636 ERROR org.apache.hadoop.hive.ql.exec.mr.ExecDriver: local
22:21:58.758 WARN org.apache.hadoop.mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[32m- SPARK-22745 - read Hive's statistics for partition[0m
22:22:01.282 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/analyzetable_part specified for non-external table:analyzetable_part
[32m- SPARK-21079 - analyze table with location different than that of individual partitions[0m
[32m- SPARK-21079 - analyze partitioned table with only a subset of partitions visible[0m
22:22:07.188 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/analyzetable_part specified for non-external table:analyzetable_part
[32m- analyze single partition[0m
22:22:12.773 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/analyzetable_part specified for non-external table:analyzetable_part
[32m- analyze a set of partitions[0m
22:22:18.428 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/analyzetable_part specified for non-external table:analyzetable_part
[32m- analyze all partitions[0m
22:22:23.140 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/analyzetable_part specified for non-external table:analyzetable_part
[32m- analyze partitions for an empty table[0m
22:22:23.417 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/analyzetable_part specified for non-external table:analyzetable_part
[32m- analyze partitions case sensitivity[0m
22:22:24.761 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/analyzetable_part specified for non-external table:analyzetable_part
[32m- analyze partial partition specifications[0m
22:22:29.108 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/analyzetable_part specified for non-external table:analyzetable_part
[32m- analyze non-existent partition[0m
22:22:29.957 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/texttable specified for non-external table:texttable
[32m- test table-level statistics for hive tables created in HiveExternalCatalog[0m
22:22:31.103 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/texttable specified for non-external table:texttable
[32m- keep existing row count in stats with noscan if table is not changed[0m
22:22:32.206 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/update_col_stats_table specified for non-external table:update_col_stats_table
[32m- keep existing column stats if table is not changed[0m
22:22:36.045 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/tab1 specified for non-external table:tab1
[32m- get statistics when not analyzed in Hive or Spark[0m
22:22:36.968 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/tab1 specified for non-external table:tab1
22:22:37.952 ERROR org.apache.hadoop.hive.ql.exec.mr.ExecDriver: local
22:22:37.961 WARN org.apache.hadoop.mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
22:22:40.243 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/tab1 specified for non-external table:tab1
22:22:40.713 ERROR org.apache.hadoop.hive.ql.exec.mr.ExecDriver: local
22:22:40.719 WARN org.apache.hadoop.mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[32m- alter table rename after analyze table[0m
22:22:42.732 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/alter_table_side_effect specified for non-external table:alter_table_side_effect
[32m- alter table should not have the side effect to store statistics in Spark side[0m
22:22:43.801 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/set_prop_table specified for non-external table:set_prop_table
22:22:44.556 ERROR org.apache.hadoop.hive.ql.exec.mr.ExecDriver: local
22:22:44.563 WARN org.apache.hadoop.mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
22:22:46.402 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/set_prop_table specified for non-external table:set_prop_table
22:22:46.917 ERROR org.apache.hadoop.hive.ql.exec.mr.ExecDriver: local
22:22:46.935 WARN org.apache.hadoop.mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[32m- alter table SET TBLPROPERTIES after analyze table[0m
22:22:48.734 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/unset_prop_table specified for non-external table:unset_prop_table
22:22:49.588 ERROR org.apache.hadoop.hive.ql.exec.mr.ExecDriver: local
22:22:49.597 WARN org.apache.hadoop.mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
22:22:51.497 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/unset_prop_table specified for non-external table:unset_prop_table
22:22:52.001 ERROR org.apache.hadoop.hive.ql.exec.mr.ExecDriver: local
22:22:52.008 WARN org.apache.hadoop.mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[32m- alter table UNSET TBLPROPERTIES after analyze table[0m
22:22:53.776 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/change_stats_insert_hive_table specified for non-external table:change_stats_insert_hive_table
22:22:55.089 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/change_stats_insert_hive_table specified for non-external table:change_stats_insert_hive_table
[32m- change stats after insert command for hive table[0m
22:22:56.093 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/change_stats_load_table specified for non-external table:change_stats_load_table
22:22:57.186 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/change_stats_load_table specified for non-external table:change_stats_load_table
[32m- change stats after load data command[0m
22:22:58.391 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/change_stats_part_table specified for non-external table:change_stats_part_table
22:23:02.438 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/change_stats_part_table specified for non-external table:change_stats_part_table
[32m- change stats after add/drop partition command[0m
22:23:06.602 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/partitionedtable specified for non-external table:partitionedtable
[32m- add/drop partitions - managed table[0m
22:23:13.581 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/orc specified for non-external table:orc
22:23:15.291 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 135.0 (TID 197)
io.airlift.compress.MalformedInputException: Malformed input: offset=46
	at io.airlift.compress.snappy.SnappyRawDecompressor.uncompressAll(SnappyRawDecompressor.java:153)
	at io.airlift.compress.snappy.SnappyRawDecompressor.decompress(SnappyRawDecompressor.java:53)
	at io.airlift.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:44)
	at org.apache.orc.impl.AircompressorCodec.decompress(AircompressorCodec.java:90)
	at org.apache.orc.impl.SnappyCodec.decompress(SnappyCodec.java:44)
	at org.apache.orc.impl.InStream$CompressedStream.readHeader(InStream.java:233)
	at org.apache.orc.impl.InStream$CompressedStream.ensureUncompressed(InStream.java:263)
	at org.apache.orc.impl.InStream$CompressedStream.read(InStream.java:250)
	at java.io.InputStream.read(InputStream.java:101)
	at com.google.protobuf25.CodedInputStream.refillBuffer(CodedInputStream.java:737)
	at com.google.protobuf25.CodedInputStream.isAtEnd(CodedInputStream.java:701)
	at com.google.protobuf25.CodedInputStream.readTag(CodedInputStream.java:99)
	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16289)
	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16253)
	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16400)
	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16395)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:89)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:95)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:49)
	at org.apache.orc.OrcProto$Footer.parseFrom(OrcProto.java:16801)
	at org.apache.orc.impl.ReaderImpl.extractFooter(ReaderImpl.java:424)
	at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:608)
	at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:367)
	at org.apache.orc.OrcFile.createReader(OrcFile.java:342)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:181)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(generated.java:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(generated.java:37)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(generated.java:84)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)
22:23:15.310 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 135.0 (TID 197, localhost, executor driver): io.airlift.compress.MalformedInputException: Malformed input: offset=46
	at io.airlift.compress.snappy.SnappyRawDecompressor.uncompressAll(SnappyRawDecompressor.java:153)
	at io.airlift.compress.snappy.SnappyRawDecompressor.decompress(SnappyRawDecompressor.java:53)
	at io.airlift.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:44)
	at org.apache.orc.impl.AircompressorCodec.decompress(AircompressorCodec.java:90)
	at org.apache.orc.impl.SnappyCodec.decompress(SnappyCodec.java:44)
	at org.apache.orc.impl.InStream$CompressedStream.readHeader(InStream.java:233)
	at org.apache.orc.impl.InStream$CompressedStream.ensureUncompressed(InStream.java:263)
	at org.apache.orc.impl.InStream$CompressedStream.read(InStream.java:250)
	at java.io.InputStream.read(InputStream.java:101)
	at com.google.protobuf25.CodedInputStream.refillBuffer(CodedInputStream.java:737)
	at com.google.protobuf25.CodedInputStream.isAtEnd(CodedInputStream.java:701)
	at com.google.protobuf25.CodedInputStream.readTag(CodedInputStream.java:99)
	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16289)
	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16253)
	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16400)
	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16395)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:89)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:95)
	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:49)
	at org.apache.orc.OrcProto$Footer.parseFrom(OrcProto.java:16801)
	at org.apache.orc.impl.ReaderImpl.extractFooter(ReaderImpl.java:424)
	at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:608)
	at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:367)
	at org.apache.orc.OrcFile.createReader(OrcFile.java:342)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:181)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(generated.java:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(generated.java:37)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(generated.java:84)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:813)

22:23:15.311 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 135.0 failed 1 times; aborting job
[31m- test statistics of LogicalRelation converted from Hive serde tables *** FAILED ***[0m
[31m  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 135.0 failed 1 times, most recent failure: Lost task 0.0 in stage 135.0 (TID 197, localhost, executor driver): io.airlift.compress.MalformedInputException: Malformed input: offset=46[0m
[31m	at io.airlift.compress.snappy.SnappyRawDecompressor.uncompressAll(SnappyRawDecompressor.java:153)[0m
[31m	at io.airlift.compress.snappy.SnappyRawDecompressor.decompress(SnappyRawDecompressor.java:53)[0m
[31m	at io.airlift.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:44)[0m
[31m	at org.apache.orc.impl.AircompressorCodec.decompress(AircompressorCodec.java:90)[0m
[31m	at org.apache.orc.impl.SnappyCodec.decompress(SnappyCodec.java:44)[0m
[31m	at org.apache.orc.impl.InStream$CompressedStream.readHeader(InStream.java:233)[0m
[31m	at org.apache.orc.impl.InStream$CompressedStream.ensureUncompressed(InStream.java:263)[0m
[31m	at org.apache.orc.impl.InStream$CompressedStream.read(InStream.java:250)[0m
[31m	at java.io.InputStream.read(InputStream.java:101)[0m
[31m	at com.google.protobuf25.CodedInputStream.refillBuffer(CodedInputStream.java:737)[0m
[31m	at com.google.protobuf25.CodedInputStream.isAtEnd(CodedInputStream.java:701)[0m
[31m	at com.google.protobuf25.CodedInputStream.readTag(CodedInputStream.java:99)[0m
[31m	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16289)[0m
[31m	at org.apache.orc.OrcProto$Footer.<init>(OrcProto.java:16253)[0m
[31m	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16400)[0m
[31m	at org.apache.orc.OrcProto$Footer$1.parsePartialFrom(OrcProto.java:16395)[0m
[31m	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:89)[0m
[31m	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:95)[0m
[31m	at com.google.protobuf25.AbstractParser.parseFrom(AbstractParser.java:49)[0m
[31m	at org.apache.orc.OrcProto$Footer.parseFrom(OrcProto.java:16801)[0m
[31m	at org.apache.orc.impl.ReaderImpl.extractFooter(ReaderImpl.java:424)[0m
[31m	at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:608)[0m
[31m	at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:367)[0m
[31m	at org.apache.orc.OrcFile.createReader(OrcFile.java:342)[0m
[31m	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:181)[0m
[31m	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)[0m
[31m	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)[0m
[31m	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)[0m
[31m	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)[0m
[31m	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(generated.java:58)[0m
[31m	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(generated.java:37)[0m
[31m	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(generated.java:84)[0m
[31m	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[31m	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)[0m
[31m	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)[0m
[31m	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)[0m
[31m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[31m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)[0m
[31m	at org.apache.spark.scheduler.Task.run(Task.scala:123)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)[0m
[31m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[0m
[31m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[31m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[31m	at java.lang.Thread.run(Thread.java:813)[0m
[31m[0m
[31mDriver stacktrace:[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)[0m
[31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)[0m
[31m  at scala.Option.foreach(Option.scala:257)[0m
[31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)[0m
[31m  ...[0m
[31m  Cause: io.airlift.compress.MalformedInputException: Malformed input: offset=46[0m
[31m  at io.airlift.compress.snappy.SnappyRawDecompressor.uncompressAll(SnappyRawDecompressor.java:153)[0m
[31m  at io.airlift.compress.snappy.SnappyRawDecompressor.decompress(SnappyRawDecompressor.java:53)[0m
[31m  at io.airlift.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:44)[0m
[31m  at org.apache.orc.impl.AircompressorCodec.decompress(AircompressorCodec.java:90)[0m
[31m  at org.apache.orc.impl.SnappyCodec.decompress(SnappyCodec.java:44)[0m
[31m  at org.apache.orc.impl.InStream$CompressedStream.readHeader(InStream.java:233)[0m
[31m  at org.apache.orc.impl.InStream$CompressedStream.ensureUncompressed(InStream.java:263)[0m
[31m  at org.apache.orc.impl.InStream$CompressedStream.read(InStream.java:250)[0m
[31m  at java.io.InputStream.read(InputStream.java:101)[0m
[31m  at com.google.protobuf25.CodedInputStream.refillBuffer(CodedInputStream.java:737)[0m
[31m  ...[0m
[32m- verify serialized column stats after analyzing columns[0m
[32m- verify column stats can be deserialized from tblproperties[0m
22:23:23.671 WARN org.apache.spark.scheduler.TaskSetManager: Stage 146 contains a task of very large size (208 KB). The maximum recommended task size is 100 KB.
[32m- serialization and deserialization of histograms to/from hive metastore[0m
[32m- test table-level statistics for data source table created in HiveExternalCatalog[0m
[32m- test table-level statistics for partitioned data source table[0m
[32m- test refreshing table stats of cached data source table by `ANALYZE TABLE` statement[0m
[32m- estimates the size of a test Hive serde tables[0m
[32m- auto converts to broadcast hash join, by size estimate of a relation[0m
[32m- auto converts to broadcast left semi join, by size estimate of a relation[0m
22:23:34.301 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/warehouse-2dcca9b6-a512-468d-bc4c-597e570e0157/maybe_big specified for non-external table:maybe_big
[32m- Deals with wrong Hive's statistics (zero rowCount)[0m
22:23:34.808 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user hadoop: id: 'hadoop': no such user
id: 'hadoop': no such user

22:23:34.809 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:23:34.809 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32mVersionsSuite:[0m
22:23:48.006 WARN org.apache.hadoop.hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
22:23:48.166 WARN org.apache.hadoop.hive.metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
[32m- success sanity check[0m
log4j:WARN No appenders could be found for logger (org.apache.hadoop.hive.metastore.RetryingHMSHandler).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
22:23:54.519 WARN org.apache.hadoop.hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
22:23:54.709 WARN org.apache.hadoop.hive.metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
[32m- hadoop configuration preserved[0m
[33m- failure sanity check !!! IGNORED !!![0m
22:23:59.183 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:23:59.188 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: create client[0m
22:23:59.378 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:23:59.378 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:23:59.992 WARN com.jolbox.bonecp.BoneCPConfig: Max Connections < 1. Setting to 20
22:24:02.715 WARN com.jolbox.bonecp.BoneCPConfig: Max Connections < 1. Setting to 20
22:24:02.739 WARN org.apache.hadoop.hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.12.0
22:24:03.181 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.181 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: createDatabase[0m
22:24:03.292 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.292 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.351 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.351 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: createDatabase with null description[0m
22:24:03.455 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.455 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: setCurrentDatabase[0m
22:24:03.517 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.517 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.581 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.581 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.649 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.664 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getDatabase[0m
22:24:03.702 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.702 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: databaseExists[0m
22:24:03.751 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.758 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: listDatabases[0m
22:24:03.847 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.847 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.909 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.909 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.994 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:03.995 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: alterDatabase[0m
22:24:04.111 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:04.112 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:04.209 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:04.209 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:05.606 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user hadoop: id: 'hadoop': no such user
id: 'hadoop': no such user

22:24:05.607 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:05.607 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: dropDatabase[0m
22:24:05.741 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:05.741 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:06.032 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:06.032 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: createTable[0m
22:24:06.115 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:06.115 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: loadTable[0m
22:24:06.521 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:06.521 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:06.624 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:06.624 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: tableExists[0m
22:24:06.670 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:06.670 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getTable[0m
22:24:06.817 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:06.817 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:06.875 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:06.876 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getTableOption[0m
22:24:07.013 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.013 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.163 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.163 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: alterTable(table: CatalogTable)[0m
22:24:07.272 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.276 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.349 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.355 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.491 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.491 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: alterTable(dbName: String, tableName: String, table: CatalogTable)[0m
22:24:07.557 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.557 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.642 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.643 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.685 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:07.685 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.112 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.112 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.205 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.206 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: alterTable - rename[0m
22:24:08.281 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.282 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.354 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.354 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.440 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.440 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.495 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.495 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.591 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.591 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.686 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.686 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: alterTable - change database[0m
22:24:08.734 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.735 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.798 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.798 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.849 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.854 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.935 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:08.945 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:09.019 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:09.019 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: alterTable - change database and table names[0m
22:24:09.206 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:09.206 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: listTables(database)[0m
22:24:09.265 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:09.265 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:09.307 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:09.308 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: listTables(database, pattern)[0m
22:24:09.391 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:09.391 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:09.437 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:09.437 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:10.315 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:10.315 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: dropTable[0m
22:24:10.373 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:10.373 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: sql create partitioned table[0m
22:24:11.062 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:11.062 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: createPartitions[0m
22:24:11.375 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:11.375 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:11.432 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:11.432 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:11.498 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:11.498 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getPartitionNames(catalogTable)[0m
22:24:11.569 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:11.569 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getPartitions(catalogTable)[0m
22:24:11.722 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:11.722 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:11.788 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:11.788 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.007 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.008 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getPartitionsByFilter[0m
22:24:12.065 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.065 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getPartition[0m
22:24:12.258 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.258 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.326 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.326 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)[0m
22:24:12.427 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.428 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.513 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.513 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)[0m
22:24:12.596 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.596 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.681 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.681 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.814 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:12.814 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getPartitions(db: String, table: String)[0m
22:24:13.068 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.068 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: loadPartition[0m
[32m- 0.12: loadDynamicPartitions[0m
22:24:13.131 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.131 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.163 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.163 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.228 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.228 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.367 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.367 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.445 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.445 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: renamePartitions[0m
22:24:13.506 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.506 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.562 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.562 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.718 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.718 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.778 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.778 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.828 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.828 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.871 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:13.871 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: alterPartitions[0m
22:24:14.000 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.000 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.172 ERROR org.apache.spark.sql.hive.client.HiveClientImpl: 
======================
Attempt to drop the partition specs in table 'src_part' database 'default':
Map(key1 -> 1, key2 -> 3)
In this attempt, the following partitions have been dropped successfully:

The remaining partitions have not been dropped:
[1, 3]
======================
             
22:24:14.172 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.172 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.501 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.501 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.609 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.609 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: dropPartitions[0m
22:24:14.714 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.714 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: createFunction[0m
22:24:14.772 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.773 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.809 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.809 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: functionExists[0m
[32m- 0.12: renameFunction[0m
22:24:14.842 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.842 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: alterFunction[0m
22:24:14.890 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.890 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getFunction[0m
22:24:14.950 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.950 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: getFunctionOption[0m
22:24:14.996 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:14.997 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: listFunctions[0m
22:24:15.061 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.062 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: dropFunction[0m
22:24:15.143 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.143 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: sql set command[0m
22:24:15.187 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.187 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.295 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.296 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: sql create index and reset[0m
[32m- 0.12: sql read hive materialized view[0m
[32m- 0.12: version[0m
[32m- 0.12: getConf[0m
22:24:15.488 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.488 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: setOut[0m
22:24:15.556 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.556 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: setInfo[0m
22:24:15.592 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.592 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: setError[0m
22:24:15.678 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.678 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: newSession[0m
22:24:15.741 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.741 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.790 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.790 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.874 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:15.874 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: withHiveState and addJar[0m
22:24:16.956 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:16.956 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: reset[0m
22:24:17.180 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.180 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.266 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.266 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.327 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.327 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.442 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.442 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.582 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.582 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.645 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.645 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.704 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.704 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.798 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.798 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.957 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:17.957 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.049 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tbl specified for non-external table:tbl
22:24:18.069 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.069 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.106 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.107 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.148 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.149 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.599 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.600 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.638 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.638 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.739 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.739 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.783 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.783 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.815 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.815 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.894 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.894 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.967 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:18.967 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.271 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.272 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.305 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.305 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.339 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.339 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.379 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.379 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.453 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.453 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.485 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.486 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.545 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.546 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.583 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.583 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.652 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.653 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.699 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.699 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.758 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.759 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.800 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.801 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.826 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.826 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: CREATE TABLE AS SELECT[0m
22:24:19.909 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.909 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.940 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.940 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.980 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:19.980 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.023 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.023 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.064 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.064 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.105 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.106 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.187 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.187 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.222 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.222 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.260 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tbl specified for non-external table:tbl
22:24:20.281 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.281 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.637 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.639 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.679 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.679 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.709 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.709 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.759 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.760 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.802 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.802 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.899 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.899 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.966 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:20.966 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.007 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.007 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.086 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.086 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.182 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.182 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.304 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.304 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.348 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.348 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.388 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.388 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.457 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.457 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.659 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.659 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.743 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.743 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.805 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.805 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.843 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:21.843 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.019 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.019 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.084 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.084 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.132 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.132 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.180 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.180 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.281 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.282 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.337 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.338 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.414 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.414 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.450 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.451 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.486 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.486 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.542 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.542 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.575 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.575 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.646 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.646 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.741 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.741 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.804 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.804 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.850 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.850 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.901 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.902 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.948 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:22.948 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.005 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.005 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.120 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.121 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.175 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.176 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.227 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.227 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.265 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.265 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.296 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.296 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.403 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.403 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.495 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.498 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.528 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.531 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.566 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.571 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.608 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.608 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.647 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.647 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.676 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.676 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.752 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.752 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.816 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.823 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.865 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.865 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.888 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.894 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.924 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.924 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.967 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.967 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.998 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:23.998 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.052 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.059 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.086 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.087 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.135 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.135 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.171 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.171 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: CREATE Partitioned TABLE AS SELECT[0m
22:24:24.296 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.296 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.345 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.345 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.382 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.383 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.413 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.414 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.453 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.453 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.495 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.495 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.793 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.793 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.831 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.836 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.894 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.895 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.928 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.928 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.962 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:24.966 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.005 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.005 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.292 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.292 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.323 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.323 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.372 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.372 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.403 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.403 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.449 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.449 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.484 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.484 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.809 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.809 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.841 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.841 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.898 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:25.898 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.019 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.019 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.081 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.081 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.134 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.134 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.177 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.177 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.229 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.238 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.273 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.273 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.302 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.302 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.365 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.372 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.409 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.410 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.436 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.436 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.485 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.485 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.515 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.515 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: Delete the temporary staging directory and files after each insert[0m
22:24:26.734 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.734 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.778 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.778 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.800 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.800 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.846 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.847 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.879 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:26.879 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.156 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.156 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.186 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.186 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.210 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.210 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.281 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.281 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.304 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.304 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.369 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.369 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.884 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.884 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.917 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.917 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.998 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:27.998 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.028 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.028 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.064 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.064 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.132 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.132 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.160 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.160 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.192 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.196 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.255 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.255 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.277 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.278 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.303 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.303 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.347 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.347 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.378 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.378 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.452 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.452 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.488 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.488 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.550 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.550 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.626 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered AvroSerdeException determining schema. Returning signal schema to indicate problem
org.apache.hadoop.hive.serde2.avro.AvroSerdeException: Neither avro.schema.literal nor avro.schema.url specified, can't determine table schema
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:66)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:254)
	at org.apache.hadoop.hive.ql.metadata.Partition.getDeserializer(Partition.java:252)
	at org.apache.hadoop.hive.ql.metadata.Partition.initialize(Partition.java:218)
	at org.apache.hadoop.hive.ql.metadata.Partition.<init>(Partition.java:166)
	at org.apache.hadoop.hive.ql.metadata.Hive.createPartition(Hive.java:1513)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1.apply(HiveShim.scala:303)
	at org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1.apply(HiveShim.scala:291)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.sql.hive.client.Shim_v0_12.createPartitions(HiveShim.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply$mcV$sp(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createPartitions(HiveClientImpl.scala:541)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply$mcV$sp(HiveExternalCatalog.scala:961)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createPartitions(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createPartitions(ExternalCatalogWithListener.scala:193)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createPartitions(SessionCatalog.scala:832)
	at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.run(ddl.scala:450)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8$$anonfun$apply$22.apply$mcV$sp(VersionsSuite.scala:781)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:751)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply$mcV$sp(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:24:28.635 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered AvroSerdeException determining schema. Returning signal schema to indicate problem
org.apache.hadoop.hive.serde2.avro.AvroSerdeException: Neither avro.schema.literal nor avro.schema.url specified, can't determine table schema
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:66)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:254)
	at org.apache.hadoop.hive.ql.metadata.Partition.getDeserializer(Partition.java:252)
	at org.apache.hadoop.hive.ql.metadata.Partition.initialize(Partition.java:218)
	at org.apache.hadoop.hive.ql.metadata.Partition.<init>(Partition.java:108)
	at org.apache.hadoop.hive.ql.metadata.Hive.createPartition(Hive.java:1551)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1.apply(HiveShim.scala:303)
	at org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1.apply(HiveShim.scala:291)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.sql.hive.client.Shim_v0_12.createPartitions(HiveShim.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply$mcV$sp(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createPartitions(HiveClientImpl.scala:541)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply$mcV$sp(HiveExternalCatalog.scala:961)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createPartitions(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createPartitions(ExternalCatalogWithListener.scala:193)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createPartitions(SessionCatalog.scala:832)
	at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.run(ddl.scala:450)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8$$anonfun$apply$22.apply$mcV$sp(VersionsSuite.scala:781)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:751)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply$mcV$sp(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:24:28.637 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.637 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.759 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.759 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.786 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.786 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.814 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.814 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.847 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.847 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.881 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.881 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:28.978 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered AvroSerdeException determining schema. Returning signal schema to indicate problem
org.apache.hadoop.hive.serde2.avro.AvroSerdeException: Neither avro.schema.literal nor avro.schema.url specified, can't determine table schema
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:66)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:254)
	at org.apache.hadoop.hive.ql.metadata.Partition.getDeserializer(Partition.java:252)
	at org.apache.hadoop.hive.ql.metadata.Partition.initialize(Partition.java:218)
	at org.apache.hadoop.hive.ql.metadata.Partition.<init>(Partition.java:108)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitions(Hive.java:1781)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitions(Hive.java:1799)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitions$1.apply(HiveClientImpl.scala:670)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitions$1.apply(HiveClientImpl.scala:662)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitions(HiveClientImpl.scala:662)
	at org.apache.spark.sql.hive.client.HiveClient$class.getPartitions(HiveClient.scala:210)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitions(HiveClientImpl.scala:83)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitions$1.apply(HiveExternalCatalog.scala:1195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitions$1.apply(HiveExternalCatalog.scala:1193)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitions(HiveExternalCatalog.scala:1193)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.listPartitions(ExternalCatalogWithListener.scala:246)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitions(SessionCatalog.scala:948)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.rawPartitions$lzycompute(HiveTableScanExec.scala:178)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.rawPartitions(HiveTableScanExec.scala:166)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$11.apply(HiveTableScanExec.scala:192)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$11.apply(HiveTableScanExec.scala:192)
	at org.apache.spark.util.Utils$.withDummyCallSite(Utils.scala:2470)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:191)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8$$anonfun$apply$22.apply$mcV$sp(VersionsSuite.scala:789)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:751)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply$mcV$sp(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:24:29.176 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.176 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.206 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.206 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.236 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.239 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.272 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.273 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.330 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.330 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.381 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.386 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.416 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.416 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.450 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.457 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.486 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.486 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.507 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.507 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.573 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.573 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.610 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.610 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.635 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.635 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.670 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.670 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.769 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.772 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.813 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.813 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.863 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.867 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.912 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.912 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.944 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.944 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.971 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:29.972 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: SPARK-13709: reading partitioned Avro table with nested schema[0m
22:24:30.107 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.107 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.139 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.141 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.172 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.172 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.196 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.196 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.254 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.255 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.299 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.300 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.483 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.483 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.507 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.507 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.527 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.527 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.582 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.582 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.606 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.606 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.632 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: Could not persist `default`.`t` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.
java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
	at java.lang.Class.forNameImpl(Native Method)
	at java.lang.Class.forName(Class.java:402)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:238)
	at org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$toInputFormat(HiveClientImpl.scala:910)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$4.apply(HiveClientImpl.scala:946)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$4.apply(HiveClientImpl.scala:946)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:946)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:482)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:499)
	at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$createDataSourceTable(HiveExternalCatalog.scala:387)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:263)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:185)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:474)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:453)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23$$anonfun$apply$mcV$sp$3.apply$mcV$sp(VersionsSuite.scala:797)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply$mcV$sp(VersionsSuite.scala:796)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply(VersionsSuite.scala:796)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply(VersionsSuite.scala:796)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:24:30.643 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.643 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.679 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.688 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.909 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.909 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.959 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.959 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.986 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:30.986 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.010 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.010 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.041 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.041 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.253 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.253 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.281 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.281 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.310 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.310 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.331 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.331 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.355 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.355 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.375 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: Could not persist `default`.`t1` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.
java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
	at java.lang.Class.forNameImpl(Native Method)
	at java.lang.Class.forName(Class.java:402)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:238)
	at org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$toInputFormat(HiveClientImpl.scala:910)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$4.apply(HiveClientImpl.scala:946)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$4.apply(HiveClientImpl.scala:946)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:946)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:482)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:499)
	at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$createDataSourceTable(HiveExternalCatalog.scala:387)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:263)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:185)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:115)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23$$anonfun$apply$mcV$sp$3.apply$mcV$sp(VersionsSuite.scala:799)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply$mcV$sp(VersionsSuite.scala:796)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply(VersionsSuite.scala:796)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply(VersionsSuite.scala:796)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:24:31.376 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.376 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.433 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.433 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.560 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.560 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.592 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.592 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.626 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.626 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.661 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.661 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.697 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.697 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.756 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.756 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.794 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.794 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.902 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.903 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.977 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:31.977 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.028 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.029 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.180 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.180 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.213 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.213 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.253 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.253 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.306 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.306 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.356 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.356 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.421 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.421 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.514 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.514 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.540 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.540 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.569 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.569 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.605 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.605 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: CTAS for managed data source tables[0m
22:24:32.796 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.802 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.829 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.829 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.854 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.854 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.886 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.886 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.929 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.929 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.962 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:32.962 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.012 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.012 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.038 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.038 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.093 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tab1 specified for non-external table:tab1
22:24:33.130 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.130 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.168 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary,ds:string>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2),ds:string>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:24:33.182 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.182 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.224 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.224 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.297 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.298 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.340 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.340 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.391 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary,ds:string>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2),ds:string>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:24:33.391 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.391 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.423 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.423 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.456 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary,ds:string>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2),ds:string>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:24:33.465 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.465 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.573 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.574 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.647 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.650 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.700 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.711 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.870 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.870 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.892 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.892 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.914 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.914 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.935 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.935 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.958 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.958 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.990 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:33.990 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.041 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.041 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.087 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.088 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.130 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tab1 specified for non-external table:tab1
22:24:34.161 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.179 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.209 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2)>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:24:34.215 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.215 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.241 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.241 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.262 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.262 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.291 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.292 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.331 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2)>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:24:34.332 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.338 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.374 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.374 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.400 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2)>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:24:34.402 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.402 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.464 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.467 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.500 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.504 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.597 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.597 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: Decimal support of Avro Hive serde[0m
22:24:34.643 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.643 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.669 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.669 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.691 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.691 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.734 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.738 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.772 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.772 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.806 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:34.807 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.031 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.031 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.064 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.064 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.096 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.096 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.135 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.136 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.167 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.168 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.191 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.191 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.238 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.238 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.268 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.268 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.297 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.297 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.336 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.336 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: read avro file containing decimal[0m
22:24:35.391 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.391 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.473 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.473 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.501 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.502 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.543 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.543 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.571 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.571 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.686 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user hadoop: id: 'hadoop': no such user
id: 'hadoop': no such user

22:24:35.686 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.686 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.713 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.713 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.740 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.740 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.769 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.771 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.802 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.802 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.823 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.823 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.846 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.848 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.886 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.888 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.918 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tab1 specified for non-external table:tab1
22:24:35.924 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.924 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.954 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:35.954 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.182 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.183 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.220 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.221 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.267 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.267 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.288 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.288 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.351 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.354 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.410 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.410 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.612 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.612 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.722 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.722 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.766 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.766 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.989 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:36.989 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.024 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.024 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.070 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.070 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.098 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.098 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.136 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.136 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.185 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.187 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.353 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.360 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.401 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.405 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.437 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.438 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.465 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.465 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.492 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.492 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.538 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.538 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.571 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.571 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.657 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.657 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.684 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.684 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.724 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.724 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.780 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.780 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.810 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.810 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.836 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.836 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.862 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.862 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.886 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.887 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.947 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.947 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.979 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:37.983 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:38.051 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:38.051 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:38.092 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:38.093 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:38.113 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:24:38.113 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.12: SPARK-17920: Insert into/overwrite avro table[0m
[31m- 0.13: create client *** FAILED ***[0m
[31m  java.lang.ClassNotFoundException: java.lang.NoClassDefFoundError: org.apache.commons.logging.LogFactory when creating Hive client using classpath: file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/javax.inject_javax.inject-1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.directory.api_api-util-1.0.0-M20.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.ant_ant-launcher-1.9.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/xmlenc_xmlenc-0.52.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-codec_commons-codec-1.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/xerces_xercesImpl-2.9.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.google.inject_guice-3.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/jline_jline-0.9.94.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.directory.server_apacheds-i18n-2.0.0-M15.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/javax.xml.bind_jaxb-api-2.2.2.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.datanucleus_datanucleus-core-3.2.10.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.sonatype.sisu.inject_cglib-2.2.1-v20090111.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-hdfs-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-cli_commons-cli-1.2.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive_hive-common-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/javax.xml.stream_stax-api-1.0-2.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive.shims_hive-shims-common-secure-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.antlr_ST4-4.0.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive_hive-metastore-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive_hive-ant-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.tukaani_xz-1.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-io_commons-io-2.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.commons_commons-compress-1.4.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/oro_oro-2.0.8.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-yarn-server-common-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.sun.jersey_jersey-json-1.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.thoughtworks.paranamer_paranamer-2.3.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive.shims_hive-shims-0.23-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.codehaus.groovy_groovy-all-2.1.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.antlr_antlr-runtime-3.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.sun.jersey_jersey-server-1.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-yarn-api-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-mapreduce-client-common-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.datanucleus_datanucleus-rdbms-3.2.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/xml-apis_xml-apis-1.3.04.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-lang_commons-lang-2.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.curator_curator-recipes-2.6.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.codehaus.jackson_jackson-jaxrs-1.9.13.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-httpclient_commons-httpclient-3.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.directory.api_api-asn1-api-1.0.0-M20.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.zookeeper_zookeeper-3.4.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.xerial.snappy_snappy-java-1.0.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-auth-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive_hive-shims-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.codehaus.jettison_jettison-1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.commons_commons-lang3-3.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.sun.jersey.contribs_jersey-guice-1.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-mapreduce-client-shuffle-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.jolbox_bonecp-0.8.0.RELEASE.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive.shims_hive-shims-common-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-client-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.ant_ant-1.9.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive_hive-exec-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.mortbay.jetty_jetty-util-6.1.26.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/io.netty_netty-3.6.2.Final.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/antlr_antlr-2.7.7.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.fusesource.leveldbjni_leveldbjni-all-1.8.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.google.protobuf_protobuf-java-2.5.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.directory.server_apacheds-kerberos-codec-2.0.0-M15.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.curator_curator-framework-2.6.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/javax.jdo_jdo-api-3.0.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.velocity_velocity-1.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.mortbay.jetty_servlet-api-2.5-20081211.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-annotations-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.codehaus.jackson_jackson-xc-1.9.13.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/asm_asm-3.2.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive.shims_hive-shims-0.20-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.derby_derby-10.10.1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.codehaus.jackson_jackson-core-asl-1.9.13.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-collections_commons-collections-3.2.2.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.sun.jersey_jersey-core-1.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-mapreduce-client-jobclient-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.commons_commons-math3-3.1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive.shims_hive-shims-0.20S-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/javax.transaction_jta-1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.httpcomponents_httpclient-4.2.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-net_commons-net-3.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.hamcrest_hamcrest-core-1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.google.guava_guava-14.0.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/junit_junit-4.10.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-yarn-common-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-beanutils_commons-beanutils-core-1.8.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-logging_commons-logging-1.1.3.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-configuration_commons-configuration-1.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/javax.activation_activation-1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.sun.xml.bind_jaxb-impl-2.2.3-1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.sun.jersey_jersey-client-1.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.httpcomponents_httpcore-4.2.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-mapreduce-client-core-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-yarn-client-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.google.code.findbugs_jsr305-1.3.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.thrift_libthrift-0.9.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.avro_avro-1.7.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.mortbay.jetty_jetty-6.1.26.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-beanutils_commons-beanutils-1.7.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.slf4j_slf4j-log4j12-1.7.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/com.google.code.gson_gson-2.2.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-mapreduce-client-app-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.thrift_libfb303-0.9.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.slf4j_slf4j-api-1.7.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/log4j_log4j-1.2.17.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.datanucleus_datanucleus-api-jdo-3.2.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hive_hive-serde-0.13.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.hadoop_hadoop-common-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.antlr_stringtemplate-3.2.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/commons-digester_commons-digester-1.8.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/aopalliance_aopalliance-1.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/javax.servlet_servlet-api-2.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.apache.curator_curator-client-2.6.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/stax_stax-api-1.0.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v13-ed3ae32e-996f-4880-a35b-d1cae16e2967/org.htrace_htrace-core-3.0.4.jar[0m
[31mPlease make sure that jars for your version of hive and hadoop are included in the paths passed to spark.sql.hive.metastore.jars.[0m
[31m  at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:277)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientBuilder$.buildClient(HiveClientBuilder.scala:58)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:134)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:122)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  ...[0m
[31m  Cause: java.lang.reflect.InvocationTargetException:[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)[0m
[31m  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)[0m
[31m  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)[0m
[31m  at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientBuilder$.buildClient(HiveClientBuilder.scala:58)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:134)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:122)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.commons.logging.LogFactory[0m
[31m  at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:60)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:152)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:117)[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)[0m
[31m  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)[0m
[31m  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)[0m
[31m  at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientBuilder$.buildClient(HiveClientBuilder.scala:58)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:134)[0m
[31m  ...[0m
[31m  Cause: java.lang.ClassNotFoundException: org.apache.commons.logging.LogFactory[0m
[31m  at java.net.URLClassLoader.findClass(URLClassLoader.java:591)[0m
[31m  at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:934)[0m
[31m  at java.lang.ClassLoader.loadClass(ClassLoader.java:879)[0m
[31m  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:226)[0m
[31m  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:215)[0m
[31m  at java.lang.ClassLoader.loadClass(ClassLoader.java:862)[0m
[31m  at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:60)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:152)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:117)[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[31m  ...[0m
[31m- 0.13: createDatabase *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$1.apply$mcV$sp(VersionsSuite.scala:164)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$1.apply(VersionsSuite.scala:162)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$1.apply(VersionsSuite.scala:162)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
22:26:29.500 WARN org.apache.spark.network.util.JavaUtils: Attempt to delete using native Unix OS command failed for path = /home/test/spark/sql/hive/target/tmp/spark-a42022c7-6d48-4893-828e-e7c1b312ba90. Falling back to Java IO way
java.io.IOException: Failed to delete: /home/test/spark/sql/hive/target/tmp/spark-a42022c7-6d48-4893-828e-e7c1b312ba90
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:163)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply$mcV$sp(VersionsSuite.scala:171)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply(VersionsSuite.scala:171)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply(VersionsSuite.scala:171)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
Caused by: java.io.IOException: Cannot run program "rm": /dev/null (Too many open files)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:159)
	... 59 more
Caused by: java.io.FileNotFoundException: /dev/null (Too many open files)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.lang.ProcessImpl.start(ProcessImpl.java:115)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 60 more
[31m- 0.13: createDatabase with null description *** FAILED ***[0m
[31m  java.io.IOException: Failed to list files for dir: /home/test/spark/sql/hive/target/tmp/spark-a42022c7-6d48-4893-828e-e7c1b312ba90[0m
[31m  at org.apache.spark.network.util.JavaUtils.listFilesSafely(JavaUtils.java:179)[0m
[31m  at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:126)[0m
[31m  at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)[0m
[31m  at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)[0m
[31m  at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply$mcV$sp(VersionsSuite.scala:171)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply(VersionsSuite.scala:171)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply(VersionsSuite.scala:171)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  ...[0m
[31m- 0.13: setCurrentDatabase *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$3.apply$mcV$sp(VersionsSuite.scala:180)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$3.apply(VersionsSuite.scala:180)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$3.apply(VersionsSuite.scala:180)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.13: getDatabase *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$30.apply(VersionsSuite.scala:185)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$30.apply(VersionsSuite.scala:183)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: databaseExists *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$32.apply(VersionsSuite.scala:190)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$32.apply(VersionsSuite.scala:189)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: listDatabases *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$33.apply(VersionsSuite.scala:195)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$33.apply(VersionsSuite.scala:195)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: alterDatabase *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$34.apply(VersionsSuite.scala:199)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$34.apply(VersionsSuite.scala:198)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: dropDatabase *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$35.apply(VersionsSuite.scala:205)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$35.apply(VersionsSuite.scala:204)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: createTable *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$4.apply$mcV$sp(VersionsSuite.scala:215)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$4.apply(VersionsSuite.scala:214)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$4.apply(VersionsSuite.scala:214)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.13: loadTable *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$5.apply$mcV$sp(VersionsSuite.scala:220)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$5.apply(VersionsSuite.scala:220)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$5.apply(VersionsSuite.scala:220)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.13: tableExists *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$36.apply(VersionsSuite.scala:229)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$36.apply(VersionsSuite.scala:227)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getTable *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$37.apply(VersionsSuite.scala:235)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$37.apply(VersionsSuite.scala:235)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getTableOption *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$38.apply(VersionsSuite.scala:239)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$38.apply(VersionsSuite.scala:239)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: alterTable(table: CatalogTable) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$39.apply(VersionsSuite.scala:243)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$39.apply(VersionsSuite.scala:242)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: alterTable(dbName: String, tableName: String, table: CatalogTable) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$40.apply(VersionsSuite.scala:249)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$40.apply(VersionsSuite.scala:248)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: alterTable - rename *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$41.apply(VersionsSuite.scala:255)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$41.apply(VersionsSuite.scala:254)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: alterTable - change database *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$42.apply(VersionsSuite.scala:268)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$42.apply(VersionsSuite.scala:265)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: alterTable - change database and table names *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$43.apply(VersionsSuite.scala:281)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$43.apply(VersionsSuite.scala:280)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: listTables(database) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$44.apply(VersionsSuite.scala:292)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$44.apply(VersionsSuite.scala:292)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: listTables(database, pattern) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$45.apply(VersionsSuite.scala:296)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$45.apply(VersionsSuite.scala:295)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: dropTable *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$46.apply(VersionsSuite.scala:305)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$46.apply(VersionsSuite.scala:300)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: sql create partitioned table *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$47.apply(VersionsSuite.scala:330)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$47.apply(VersionsSuite.scala:330)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: createPartitions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$6.apply$mcV$sp(VersionsSuite.scala:339)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$6.apply(VersionsSuite.scala:335)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$6.apply(VersionsSuite.scala:335)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.13: getPartitionNames(catalogTable) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$48.apply(VersionsSuite.scala:345)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$48.apply(VersionsSuite.scala:343)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getPartitions(catalogTable) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$49.apply(VersionsSuite.scala:350)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$49.apply(VersionsSuite.scala:349)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getPartitionsByFilter *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$50.apply(VersionsSuite.scala:355)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getPartition *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$51.apply(VersionsSuite.scala:368)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$51.apply(VersionsSuite.scala:368)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getPartitionOption(db: String, table: String, spec: TablePartitionSpec) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$52.apply(VersionsSuite.scala:372)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$52.apply(VersionsSuite.scala:371)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$53.apply(VersionsSuite.scala:379)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$53.apply(VersionsSuite.scala:377)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getPartitions(db: String, table: String) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$54.apply(VersionsSuite.scala:384)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$54.apply(VersionsSuite.scala:384)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: loadPartition *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$7.apply$mcV$sp(VersionsSuite.scala:392)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$7.apply(VersionsSuite.scala:387)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$7.apply(VersionsSuite.scala:387)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.13: loadDynamicPartitions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$8.apply$mcV$sp(VersionsSuite.scala:407)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$8.apply(VersionsSuite.scala:402)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$8.apply(VersionsSuite.scala:402)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.13: renamePartitions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$55.apply(VersionsSuite.scala:419)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$55.apply(VersionsSuite.scala:416)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: alterPartitions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$56.apply(VersionsSuite.scala:434)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$56.apply(VersionsSuite.scala:425)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: dropPartitions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$57.apply(VersionsSuite.scala:447)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$57.apply(VersionsSuite.scala:441)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: createFunction *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$58.apply(VersionsSuite.scala:477)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: functionExists *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$59.apply(VersionsSuite.scala:486)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: renameFunction *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$60.apply(VersionsSuite.scala:497)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: alterFunction *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$61.apply(VersionsSuite.scala:510)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getFunction *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$62.apply(VersionsSuite.scala:522)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getFunctionOption *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$64.apply(VersionsSuite.scala:532)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: listFunctions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$65.apply(VersionsSuite.scala:542)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: dropFunction *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$66.apply(VersionsSuite.scala:554)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: sql set command *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$67.apply(VersionsSuite.scala:564)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$67.apply(VersionsSuite.scala:564)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: sql create index and reset *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$68.apply(VersionsSuite.scala:568)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$68.apply(VersionsSuite.scala:567)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[32m- 0.13: sql read hive materialized view[0m
[31m- 0.13: version *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$70.apply(VersionsSuite.scala:589)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$70.apply(VersionsSuite.scala:589)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: getConf *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$71.apply(VersionsSuite.scala:593)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$71.apply(VersionsSuite.scala:593)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: setOut *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$13.apply$mcV$sp(VersionsSuite.scala:597)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$13.apply(VersionsSuite.scala:597)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$13.apply(VersionsSuite.scala:597)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.13: setInfo *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$14.apply$mcV$sp(VersionsSuite.scala:601)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$14.apply(VersionsSuite.scala:601)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$14.apply(VersionsSuite.scala:601)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.13: setError *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$15.apply$mcV$sp(VersionsSuite.scala:605)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$15.apply(VersionsSuite.scala:605)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$15.apply(VersionsSuite.scala:605)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.13: newSession *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$72.apply(VersionsSuite.scala:609)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$72.apply(VersionsSuite.scala:608)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.13: withHiveState and addJar *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$16.apply$mcZ$sp(VersionsSuite.scala:615)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$16.apply(VersionsSuite.scala:613)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$16.apply(VersionsSuite.scala:613)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.13: reset *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$73.apply(VersionsSuite.scala:629)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$73.apply(VersionsSuite.scala:627)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
22:26:29.662 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user hadoop: id: 'hadoop': no such user
id: 'hadoop': no such user

22:26:29.662 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.662 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.710 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.710 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.744 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.744 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.781 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.781 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.803 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.803 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.841 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.846 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.888 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.888 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.925 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.925 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.957 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.957 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.988 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tbl specified for non-external table:tbl
22:26:29.993 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:29.993 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.055 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.055 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.083 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.083 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.284 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.284 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.307 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.307 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.363 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.363 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.389 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.389 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.411 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.411 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.445 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.445 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.654 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.654 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.796 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.796 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.825 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.825 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.879 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.879 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.926 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.926 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.954 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.954 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.982 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:30.982 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.004 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.004 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.062 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.062 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.094 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.094 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.133 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.133 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.167 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.167 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.187 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.187 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.255 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.255 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[31m- 0.13: CREATE TABLE AS SELECT *** FAILED ***[0m
[31m  None was empty (VersionsSuite.scala:647)[0m
22:26:31.280 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.280 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.299 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.299 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.332 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.332 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.362 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.362 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.387 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.388 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.431 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.431 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.461 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.461 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.485 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tbl specified for non-external table:tbl
22:26:31.493 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.493 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.778 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.778 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.809 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.809 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.927 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.927 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.951 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.951 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.979 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:31.979 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.033 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.034 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.070 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.070 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.100 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.100 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.158 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.158 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.225 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.225 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.262 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.274 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.311 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.311 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.348 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.348 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.381 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.383 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.494 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.494 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.519 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.519 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.549 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.549 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.582 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.582 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.647 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.647 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.711 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.711 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.764 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.769 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.807 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.807 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.908 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.908 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.935 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.940 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.973 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:32.974 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.010 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.011 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.125 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.128 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.171 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.171 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.197 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.197 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.229 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.229 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.258 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.264 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.285 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.286 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[31m- 0.13: CREATE Partitioned TABLE AS SELECT *** FAILED ***[0m
[31m  None was empty (VersionsSuite.scala:670)[0m
22:26:33.367 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.367 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.389 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.389 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.430 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.431 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.461 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.462 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.487 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.487 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.537 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.537 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.812 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.812 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.861 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.861 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.916 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.916 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.942 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.943 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.979 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:33.979 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.022 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.022 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.251 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.251 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.312 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.312 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.390 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.390 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.412 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.412 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.435 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.436 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.464 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.464 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.703 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.703 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.757 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.757 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.794 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.794 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.820 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.820 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.861 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.861 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.903 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.903 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.952 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.952 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.999 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:34.999 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.043 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.043 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.072 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.072 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.095 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.095 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.155 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.155 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.182 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.182 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.206 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.206 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.238 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.246 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.13: Delete the temporary staging directory and files after each insert[0m
22:26:35.361 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.361 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.386 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.386 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.404 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.404 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.523 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.523 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.549 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.554 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.588 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.588 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.632 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.632 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.660 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.663 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.696 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.696 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.742 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.747 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.785 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.789 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.993 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:35.993 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.020 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.020 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.087 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.087 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.113 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.113 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.141 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.141 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.177 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.177 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.198 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.198 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.250 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.250 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.282 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.282 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.305 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.305 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.347 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.347 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.372 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.372 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.403 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.403 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.453 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.453 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.491 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.491 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.664 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.664 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.697 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered AvroSerdeException determining schema. Returning signal schema to indicate problem
org.apache.hadoop.hive.serde2.avro.AvroSerdeException: Neither avro.schema.literal nor avro.schema.url specified, can't determine table schema
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:66)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:254)
	at org.apache.hadoop.hive.ql.metadata.Partition.getDeserializer(Partition.java:252)
	at org.apache.hadoop.hive.ql.metadata.Partition.initialize(Partition.java:218)
	at org.apache.hadoop.hive.ql.metadata.Partition.<init>(Partition.java:166)
	at org.apache.hadoop.hive.ql.metadata.Hive.createPartition(Hive.java:1513)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1.apply(HiveShim.scala:303)
	at org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1.apply(HiveShim.scala:291)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.sql.hive.client.Shim_v0_12.createPartitions(HiveShim.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply$mcV$sp(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createPartitions(HiveClientImpl.scala:541)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply$mcV$sp(HiveExternalCatalog.scala:961)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createPartitions(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createPartitions(ExternalCatalogWithListener.scala:193)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createPartitions(SessionCatalog.scala:832)
	at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.run(ddl.scala:450)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8$$anonfun$apply$22.apply$mcV$sp(VersionsSuite.scala:781)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:751)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply$mcV$sp(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:36.704 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered AvroSerdeException determining schema. Returning signal schema to indicate problem
org.apache.hadoop.hive.serde2.avro.AvroSerdeException: Neither avro.schema.literal nor avro.schema.url specified, can't determine table schema
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:66)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:254)
	at org.apache.hadoop.hive.ql.metadata.Partition.getDeserializer(Partition.java:252)
	at org.apache.hadoop.hive.ql.metadata.Partition.initialize(Partition.java:218)
	at org.apache.hadoop.hive.ql.metadata.Partition.<init>(Partition.java:108)
	at org.apache.hadoop.hive.ql.metadata.Hive.createPartition(Hive.java:1551)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1.apply(HiveShim.scala:303)
	at org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$createPartitions$1.apply(HiveShim.scala:291)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.sql.hive.client.Shim_v0_12.createPartitions(HiveShim.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply$mcV$sp(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createPartitions(HiveClientImpl.scala:541)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply$mcV$sp(HiveExternalCatalog.scala:961)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createPartitions(HiveExternalCatalog.scala:944)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createPartitions(ExternalCatalogWithListener.scala:193)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createPartitions(SessionCatalog.scala:832)
	at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.run(ddl.scala:450)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8$$anonfun$apply$22.apply$mcV$sp(VersionsSuite.scala:781)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:751)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply$mcV$sp(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:36.705 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.705 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.785 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.786 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.811 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.811 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.836 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.836 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.865 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.867 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.898 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.898 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:36.946 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered AvroSerdeException determining schema. Returning signal schema to indicate problem
org.apache.hadoop.hive.serde2.avro.AvroSerdeException: Neither avro.schema.literal nor avro.schema.url specified, can't determine table schema
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:66)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:254)
	at org.apache.hadoop.hive.ql.metadata.Partition.getDeserializer(Partition.java:252)
	at org.apache.hadoop.hive.ql.metadata.Partition.initialize(Partition.java:218)
	at org.apache.hadoop.hive.ql.metadata.Partition.<init>(Partition.java:108)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitions(Hive.java:1781)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitions(Hive.java:1799)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitions$1.apply(HiveClientImpl.scala:670)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitions$1.apply(HiveClientImpl.scala:662)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitions(HiveClientImpl.scala:662)
	at org.apache.spark.sql.hive.client.HiveClient$class.getPartitions(HiveClient.scala:210)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitions(HiveClientImpl.scala:83)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitions$1.apply(HiveExternalCatalog.scala:1195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitions$1.apply(HiveExternalCatalog.scala:1193)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitions(HiveExternalCatalog.scala:1193)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.listPartitions(ExternalCatalogWithListener.scala:246)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitions(SessionCatalog.scala:948)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.rawPartitions$lzycompute(HiveTableScanExec.scala:178)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.rawPartitions(HiveTableScanExec.scala:166)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$11.apply(HiveTableScanExec.scala:192)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$11.apply(HiveTableScanExec.scala:192)
	at org.apache.spark.util.Utils$.withDummyCallSite(Utils.scala:2470)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:191)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8$$anonfun$apply$22.apply$mcV$sp(VersionsSuite.scala:789)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:751)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21$$anonfun$apply$mcV$sp$8.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply$mcV$sp(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$21.apply(VersionsSuite.scala:719)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:37.009 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.009 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.074 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.076 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.113 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.114 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.142 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.142 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.180 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.180 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.204 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.204 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.254 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.263 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.291 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.291 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.395 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.395 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.429 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.429 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.479 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.479 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.525 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.525 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.557 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.557 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.585 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.585 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.634 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.634 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.665 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.665 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.725 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.725 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.752 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.752 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.791 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.791 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.836 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.836 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.13: SPARK-13709: reading partitioned Avro table with nested schema[0m
22:26:37.982 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:37.982 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.007 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.007 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.125 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.126 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.166 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.166 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.190 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.190 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.232 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.232 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.399 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.399 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.421 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.421 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.447 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.447 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.472 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.472 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.504 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.504 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.564 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: Could not persist `default`.`t` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.
java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
	at java.lang.Class.forNameImpl(Native Method)
	at java.lang.Class.forName(Class.java:402)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:238)
	at org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$toInputFormat(HiveClientImpl.scala:910)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$4.apply(HiveClientImpl.scala:946)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$4.apply(HiveClientImpl.scala:946)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:946)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:482)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:499)
	at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$createDataSourceTable(HiveExternalCatalog.scala:387)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:263)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:185)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:474)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:453)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23$$anonfun$apply$mcV$sp$3.apply$mcV$sp(VersionsSuite.scala:797)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply$mcV$sp(VersionsSuite.scala:796)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply(VersionsSuite.scala:796)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply(VersionsSuite.scala:796)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:38.569 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.569 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.610 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.615 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.710 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.710 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.753 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.753 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.781 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.781 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.802 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.802 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.898 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:38.898 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.002 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.002 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.025 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.025 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.046 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.046 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.067 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.067 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.088 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.088 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.140 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: Could not persist `default`.`t1` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.
java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
	at java.lang.Class.forNameImpl(Native Method)
	at java.lang.Class.forName(Class.java:402)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:238)
	at org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$toInputFormat(HiveClientImpl.scala:910)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$4.apply(HiveClientImpl.scala:946)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$4.apply(HiveClientImpl.scala:946)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:946)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:482)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:499)
	at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$createDataSourceTable(HiveExternalCatalog.scala:387)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:263)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:185)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:115)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23$$anonfun$apply$mcV$sp$3.apply$mcV$sp(VersionsSuite.scala:799)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply$mcV$sp(VersionsSuite.scala:796)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply(VersionsSuite.scala:796)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$23.apply(VersionsSuite.scala:796)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:39.141 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.141 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.169 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.171 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.299 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.299 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.324 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.324 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.359 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.359 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.389 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.390 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.413 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.414 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.443 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.443 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.473 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.473 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.496 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.497 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.549 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.549 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.587 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.587 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.653 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.653 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.687 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.687 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.751 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.751 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.778 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.778 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.856 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.856 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.902 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.903 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.931 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.931 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.957 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.957 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.987 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:39.987 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.039 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.039 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.13: CTAS for managed data source tables[0m
22:26:40.084 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.084 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.122 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.122 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.150 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.150 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.176 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.176 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.241 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.245 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.286 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.286 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.310 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.311 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.335 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.342 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.392 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tab1 specified for non-external table:tab1
22:26:40.404 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.404 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.433 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary,ds:string>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2),ds:string>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:26:40.440 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.440 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.475 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.475 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.503 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.503 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.567 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.567 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.604 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary,ds:string>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2),ds:string>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:26:40.604 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.604 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.638 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.638 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.669 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary,ds:string>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2),ds:string>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:26:40.676 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.676 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.749 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.749 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.781 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.781 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.810 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.810 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.861 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.861 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.890 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.890 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.997 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:40.997 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.018 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.018 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.044 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.044 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.115 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.116 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.155 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.155 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.186 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.186 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.207 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tab1 specified for non-external table:tab1
22:26:41.213 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.213 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.241 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2)>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:26:41.249 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.252 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.282 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.282 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.301 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.302 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.351 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.358 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.386 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2)>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:26:41.387 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.387 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.410 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.411 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.438 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: The table schema given by Hive metastore(struct<f0:binary>) is different from the schema when this table was created by Spark SQL(struct<f0:decimal(38,2)>). We have to fall back to the table schema from Hive metastore which is not case preserving.
22:26:41.439 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.440 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.458 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.458 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.480 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.480 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.500 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.500 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.13: Decimal support of Avro Hive serde[0m
22:26:41.555 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.555 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.578 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.578 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.633 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.639 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.670 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.670 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.691 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.695 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.759 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:41.759 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.011 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.011 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.046 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.046 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.076 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.076 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.130 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.130 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.179 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.181 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.203 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.203 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.256 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.256 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.278 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.278 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.304 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.304 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.344 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.344 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[32m- 0.13: read avro file containing decimal[0m
22:26:42.395 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.395 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.451 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.451 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.486 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.486 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.558 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.558 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.596 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.596 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.627 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.627 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.653 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.653 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.695 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.695 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.723 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.723 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.748 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.748 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.774 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.775 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.794 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.795 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.834 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.834 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.856 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tab1 specified for non-external table:tab1
22:26:42.861 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.861 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.886 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:42.886 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.137 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.137 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.164 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.164 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.200 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.200 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.230 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.230 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.263 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.263 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.324 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.325 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.378 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.378 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.439 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.440 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.460 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.460 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.482 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.482 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.540 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.540 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.624 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.624 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.650 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:265)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:597)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:170)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1.apply(HiveClientImpl.scala:368)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1.apply(HiveClientImpl.scala:366)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:366)
	at org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:83)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:118)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:699)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed$lzycompute(TestHive.scala:589)
	at org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed(TestHive.scala:572)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:628)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:211)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.652 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:175)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1.apply(HiveClientImpl.scala:368)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1.apply(HiveClientImpl.scala:366)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:366)
	at org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:83)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:118)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:699)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed$lzycompute(TestHive.scala:589)
	at org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed(TestHive.scala:572)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:628)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:211)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.656 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.656 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.683 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.683 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.728 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:265)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:597)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:170)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply$mcZ$sp(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:360)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:824)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:142)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:420)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.dropTable(SessionCatalog.scala:670)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:216)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.735 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:175)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply$mcZ$sp(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:360)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:824)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:142)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:420)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.dropTable(SessionCatalog.scala:670)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:216)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.738 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.738 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.773 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.773 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.842 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.842 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.877 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:265)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:597)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:170)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply$mcZ$sp(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:360)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:824)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:142)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:420)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:194)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.880 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:175)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply$mcZ$sp(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:360)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:824)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:142)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:420)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:194)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.881 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.881 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.905 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.905 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.939 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:265)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:597)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:170)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply$mcZ$sp(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:360)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:824)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:142)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:420)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireTableExists(SessionCatalog.scala:183)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:433)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:197)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.941 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:175)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply$mcZ$sp(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:360)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:824)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:142)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:420)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireTableExists(SessionCatalog.scala:183)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:433)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:197)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.943 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.944 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.974 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:265)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:597)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:170)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1.apply(HiveClientImpl.scala:368)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1.apply(HiveClientImpl.scala:366)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:366)
	at org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:83)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:118)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:699)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:434)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:197)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.975 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:175)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1.apply(HiveClientImpl.scala:368)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1.apply(HiveClientImpl.scala:366)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:366)
	at org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:83)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:118)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:699)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:434)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:197)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.978 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.978 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:43.997 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:265)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:597)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:170)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply$mcZ$sp(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:360)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:824)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:142)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:420)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:208)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:43.998 WARN org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: Encountered exception determining schema. Returning signal schema to indicate problem
java.io.FileNotFoundException: /home/test/spark/sql/hive/target/tmp/spark-c573b3f6-a10f-4030-a7ac-d3418c7d4400/avroDecimal.avsc (Too many open files)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at java.net.URL.openStream(URL.java:1045)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:75)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(AvroSerdeUtils.java:87)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:60)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:218)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:272)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:175)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:991)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$getRawTableOption(HiveClientImpl.scala:357)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply$mcZ$sp(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$tableExists$1.apply(HiveClientImpl.scala:361)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:360)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$tableExists$1.apply(HiveExternalCatalog.scala:825)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:824)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:142)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:420)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:208)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:73)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$withTable$1.apply(VersionsSuite.scala:72)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:72)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:26:44.000 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:44.000 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:44.081 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:44.081 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:44.111 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:44.111 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:44.150 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:44.150 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:44.179 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:26:44.180 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[31m- 0.13: SPARK-17920: Insert into/overwrite avro table *** FAILED ***[0m
[31m  java.lang.RuntimeException: Error while running command to get file permissions : java.io.IOException: Cannot run program "/bin/ls": error=24, Too many open files[0m
[31m	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)[0m
[31m	at org.apache.hadoop.util.Shell.runCommand(Shell.java:522)[0m
[31m	at org.apache.hadoop.util.Shell.run(Shell.java:478)[0m
[31m	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:766)[0m
[31m	at org.apache.hadoop.util.Shell.execCommand(Shell.java:859)[0m
[31m	at org.apache.hadoop.util.Shell.execCommand(Shell.java:842)[0m
[31m	at org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)[0m
[31m	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:587)[0m
[31m	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:562)[0m
[31m	at org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:47)[0m
[31m	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1701)[0m
[31m	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1681)[0m
[31m	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:268)[0m
[31m	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)[0m
[31m	at org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)[0m
[31m	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)[0m
[31m	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)[0m
[31m	at scala.Option.getOrElse(Option.scala:121)[0m
[31m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)[0m
[31m	at scala.Option.getOrElse(Option.scala:121)[0m
[31m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)[0m
[31m	at scala.Option.getOrElse(Option.scala:121)[0m
[31m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)[0m
[31m	at scala.Option.getOrElse(Option.scala:121)[0m
[31m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)[0m
[31m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)[0m
[31m	at scala.Option.getOrElse(Option.scala:121)[0m
[31m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)[0m
[31m	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)[0m
[31m	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)[0m
[31m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[31m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[31m	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)[0m
[31m	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)[0m
[31m	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)[0m
[31m	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)[0m
[31m	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)[0m
[31m	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)[0m
[31m	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)[0m
[31m	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)[0m
[31m	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)[0m
[31m	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)[0m
[31m	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10$$anonfun$apply$28.apply$mcV$sp(VersionsSuite.scala:972)[0m
[31m	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)[0m
[31m	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:945)[0m
[31m	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27$$anonfun$apply$mcV$sp$10.apply(VersionsSuite.scala:914)[0m
[31m	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)[0m
[31m	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply$mcV$sp(VersionsSuite.scala:914)[0m
[31m	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)[0m
[31m	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$27.apply(VersionsSuite.scala:911)[0m
[31m	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)[0m
[31m	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)[0m
[31m	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)[0m
[31m	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)[0m
[31m	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)[0m
[31m	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)[0m
[31m	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)[0m
[31m	at scala.collection.immutable.List.foreach(List.scala:392)[0m
[31m	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)[0m
[31m	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)[0m
[31m	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)[0m
[31m	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)[0m
[31m	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)[0m
[31m	at org.scalatest.Suite$class.run(Suite.scala:1147)[0m
[31m	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)[0m
[31m	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)[0m
[31m	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)[0m
[31m	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)[0m
[31m	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)[0m
[31m	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)[0m
[31m	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)[0m
[31m	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)[0m
[31m	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)[0m
[31m	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)[0m
[31m	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)[0m
[31m	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)[0m
[31m	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)[0m
[31m	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)[0m
[31m	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)[0m
[31m	at org.scalatest.Suite$class.run(Suite.scala:1144)[0m
[31m	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)[0m
[31m	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)[0m
[31m	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)[0m
[31m	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)[0m
[31m	at scala.collection.immutable.List.foreach(List.scala:392)[0m
[31m	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)[0m
[31m	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)[0m
[31m	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)[0m
[31m	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)[0m
[31m	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)[0m
[31m	at org.scalatest.tools.Runner$.main(Runner.scala:827)[0m
[31m	at org.scalatest.tools.Runner.main(Runner.scala)[0m
[31mCaused by: java.io.IOException: error=24, Too many open files[0m
[31m	at java.lang.UNIXProcess.forkAndExec(Native Method)[0m
[31m	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)[0m
[31m	at java.lang.ProcessImpl.start(ProcessImpl.java:134)[0m
[31m	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)[0m
[31m	... 116 more[0m
[31m  at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:625)[0m
[31m  at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:562)[0m
[31m  at org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:47)[0m
[31m  at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1701)[0m
[31m  at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1681)[0m
[31m  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:268)[0m
[31m  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)[0m
[31m  at org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)[0m
[31m  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)[0m
[31m  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)[0m
[31m  ...[0m
[31m- 0.14: create client *** FAILED ***[0m
[31m  java.lang.ClassNotFoundException: java.lang.NoClassDefFoundError: org.apache.hadoop.hive.conf.HiveConf when creating Hive client using classpath: file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/javax.inject_javax.inject-1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.directory.api_api-util-1.0.0-M20.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.ant_ant-launcher-1.9.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/xmlenc_xmlenc-0.52.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-codec_commons-codec-1.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/xerces_xercesImpl-2.9.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.google.inject_guice-3.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/jline_jline-0.9.94.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.directory.server_apacheds-i18n-2.0.0-M15.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/net.hydromatic_eigenbase-properties-1.1.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/javax.xml.bind_jaxb-api-2.2.2.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.datanucleus_datanucleus-core-3.2.10.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.sonatype.sisu.inject_cglib-2.2.1-v20090111.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive.shims_hive-shims-0.20S-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-hdfs-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-cli_commons-cli-1.2.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/javax.xml.stream_stax-api-1.0-2.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.calcite_calcite-linq4j-1.3.0-incubating.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.codehaus.janino_commons-compiler-2.7.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.antlr_ST4-4.0.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.tukaani_xz-1.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-io_commons-io-2.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.commons_commons-compress-1.4.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/oro_oro-2.0.8.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-yarn-server-common-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.sun.jersey_jersey-json-1.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.thoughtworks.paranamer_paranamer-2.3.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.codehaus.groovy_groovy-all-2.1.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.antlr_antlr-runtime-3.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.sun.jersey_jersey-server-1.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-yarn-api-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-mapreduce-client-common-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.datanucleus_datanucleus-rdbms-3.2.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/xml-apis_xml-apis-1.3.04.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-lang_commons-lang-2.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.curator_curator-recipes-2.6.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.codehaus.jackson_jackson-jaxrs-1.9.13.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive.shims_hive-shims-0.20-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-httpclient_commons-httpclient-3.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.directory.api_api-asn1-api-1.0.0-M20.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.zookeeper_zookeeper-3.4.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.xerial.snappy_snappy-java-1.0.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-auth-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/junit_junit-4.11.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive.shims_hive-shims-0.23-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.codehaus.jettison_jettison-1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.fasterxml.jackson.core_jackson-databind-2.1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.sun.jersey.contribs_jersey-guice-1.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-dbcp_commons-dbcp-1.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-mapreduce-client-shuffle-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.jolbox_bonecp-0.8.0.RELEASE.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-client-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.ant_ant-1.9.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.mortbay.jetty_jetty-util-6.1.26.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/io.netty_netty-3.6.2.Final.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/antlr_antlr-2.7.7.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.fusesource.leveldbjni_leveldbjni-all-1.8.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-pool_commons-pool-1.5.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.fasterxml.jackson.core_jackson-annotations-2.1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.calcite_calcite-core-1.3.0-incubating.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.google.protobuf_protobuf-java-2.5.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.directory.server_apacheds-kerberos-codec-2.0.0-M15.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.curator_curator-framework-2.6.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/javax.jdo_jdo-api-3.0.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.velocity_velocity-1.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive.shims_hive-shims-common-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-annotations-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.codehaus.jackson_jackson-xc-1.9.13.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/asm_asm-3.2.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.derby_derby-10.10.1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.codehaus.jackson_jackson-core-asl-1.9.13.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-collections_commons-collections-3.2.2.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.sun.jersey_jersey-core-1.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-mapreduce-client-jobclient-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.commons_commons-math3-3.1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/javax.transaction_jta-1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.httpcomponents_httpclient-4.2.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive_hive-serde-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive_hive-ant-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-net_commons-net-3.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.google.guava_guava-14.0.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-yarn-common-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-beanutils_commons-beanutils-core-1.8.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.hamcrest_hamcrest-core-1.3.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-logging_commons-logging-1.1.3.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-configuration_commons-configuration-1.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/javax.activation_activation-1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.sun.xml.bind_jaxb-impl-2.2.3-1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.sun.jersey_jersey-client-1.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive_hive-common-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.httpcomponents_httpcore-4.2.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-mapreduce-client-core-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive_hive-metastore-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.fusesource.jansi_jansi-1.11.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-yarn-client-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.google.code.findbugs_jsr305-1.3.9.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.thrift_libthrift-0.9.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.avro_avro-1.7.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-beanutils_commons-beanutils-1.7.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.codehaus.janino_janino-2.7.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/net.sf.opencsv_opencsv-2.3.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.slf4j_slf4j-log4j12-1.7.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.fasterxml.jackson.core_jackson-core-2.1.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/com.google.code.gson_gson-2.2.4.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-mapreduce-client-app-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive.shims_hive-shims-common-secure-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive_hive-exec-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.thrift_libfb303-0.9.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.slf4j_slf4j-api-1.7.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/log4j_log4j-1.2.17.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.datanucleus_datanucleus-api-jdo-3.2.6.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hadoop_hadoop-common-2.6.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.antlr_stringtemplate-3.2.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/commons-digester_commons-digester-1.8.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.calcite_calcite-avatica-1.3.0-incubating.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/aopalliance_aopalliance-1.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/javax.servlet_servlet-api-2.5.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.hive_hive-shims-0.14.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.apache.curator_curator-client-2.6.0.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/stax_stax-api-1.0.1.jar, file:/home/test/spark/sql/hive/target/tmp/hive-v14-d3fc788f-6da8-47cb-8ca2-f794192fb2b9/org.htrace_htrace-core-3.0.4.jar[0m
[31mPlease make sure that jars for your version of hive and hadoop are included in the paths passed to spark.sql.hive.metastore.jars.[0m
[31m  at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:277)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientBuilder$.buildClient(HiveClientBuilder.scala:58)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:134)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:122)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  ...[0m
[31m  Cause: java.lang.reflect.InvocationTargetException:[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)[0m
[31m  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)[0m
[31m  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)[0m
[31m  at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientBuilder$.buildClient(HiveClientBuilder.scala:58)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:134)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:122)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  ...[0m
[31m  Cause: java.lang.NoClassDefFoundError: org.apache.hadoop.hive.conf.HiveConf[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:152)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:117)[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)[0m
[31m  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)[0m
[31m  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)[0m
[31m  at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientBuilder$.buildClient(HiveClientBuilder.scala:58)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:134)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$29.apply(VersionsSuite.scala:122)[0m
[31m  ...[0m
[31m  Cause: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf[0m
[31m  at java.net.URLClassLoader.findClass(URLClassLoader.java:591)[0m
[31m  at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:934)[0m
[31m  at java.lang.ClassLoader.loadClass(ClassLoader.java:879)[0m
[31m  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:226)[0m
[31m  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:215)[0m
[31m  at java.lang.ClassLoader.loadClass(ClassLoader.java:862)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:152)[0m
[31m  at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:117)[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[31m  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)[0m
[31m  ...[0m
[31m- 0.14: createDatabase *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$1.apply$mcV$sp(VersionsSuite.scala:164)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$1.apply(VersionsSuite.scala:162)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$1.apply(VersionsSuite.scala:162)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
22:28:48.787 WARN org.apache.spark.network.util.JavaUtils: Attempt to delete using native Unix OS command failed for path = /home/test/spark/sql/hive/target/tmp/spark-31617a05-0eb8-451c-8b85-f1df3d1b00e2. Falling back to Java IO way
java.io.IOException: Failed to delete: /home/test/spark/sql/hive/target/tmp/spark-31617a05-0eb8-451c-8b85-f1df3d1b00e2
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:163)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply$mcV$sp(VersionsSuite.scala:171)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply(VersionsSuite.scala:171)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply(VersionsSuite.scala:171)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
Caused by: java.io.IOException: Cannot run program "rm": /dev/null (Too many open files)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:159)
	... 59 more
Caused by: java.io.FileNotFoundException: /dev/null (Too many open files)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.lang.ProcessImpl.start(ProcessImpl.java:115)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 60 more
[31m- 0.14: createDatabase with null description *** FAILED ***[0m
[31m  java.io.IOException: Failed to list files for dir: /home/test/spark/sql/hive/target/tmp/spark-31617a05-0eb8-451c-8b85-f1df3d1b00e2[0m
[31m  at org.apache.spark.network.util.JavaUtils.listFilesSafely(JavaUtils.java:179)[0m
[31m  at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:126)[0m
[31m  at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)[0m
[31m  at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)[0m
[31m  at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite.withTempDir(VersionsSuite.scala:64)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply$mcV$sp(VersionsSuite.scala:171)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply(VersionsSuite.scala:171)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$2.apply(VersionsSuite.scala:171)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  ...[0m
[31m- 0.14: setCurrentDatabase *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$3.apply$mcV$sp(VersionsSuite.scala:180)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$3.apply(VersionsSuite.scala:180)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$3.apply(VersionsSuite.scala:180)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.14: getDatabase *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$30.apply(VersionsSuite.scala:185)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$30.apply(VersionsSuite.scala:183)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: databaseExists *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$32.apply(VersionsSuite.scala:190)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$32.apply(VersionsSuite.scala:189)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: listDatabases *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$33.apply(VersionsSuite.scala:195)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$33.apply(VersionsSuite.scala:195)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: alterDatabase *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$34.apply(VersionsSuite.scala:199)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$34.apply(VersionsSuite.scala:198)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: dropDatabase *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$35.apply(VersionsSuite.scala:205)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$35.apply(VersionsSuite.scala:204)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: createTable *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$4.apply$mcV$sp(VersionsSuite.scala:215)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$4.apply(VersionsSuite.scala:214)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$4.apply(VersionsSuite.scala:214)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.14: loadTable *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$5.apply$mcV$sp(VersionsSuite.scala:220)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$5.apply(VersionsSuite.scala:220)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$5.apply(VersionsSuite.scala:220)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.14: tableExists *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$36.apply(VersionsSuite.scala:229)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$36.apply(VersionsSuite.scala:227)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getTable *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$37.apply(VersionsSuite.scala:235)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$37.apply(VersionsSuite.scala:235)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getTableOption *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$38.apply(VersionsSuite.scala:239)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$38.apply(VersionsSuite.scala:239)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: alterTable(table: CatalogTable) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$39.apply(VersionsSuite.scala:243)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$39.apply(VersionsSuite.scala:242)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: alterTable(dbName: String, tableName: String, table: CatalogTable) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$40.apply(VersionsSuite.scala:249)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$40.apply(VersionsSuite.scala:248)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: alterTable - rename *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$41.apply(VersionsSuite.scala:255)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$41.apply(VersionsSuite.scala:254)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: alterTable - change database *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$42.apply(VersionsSuite.scala:268)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$42.apply(VersionsSuite.scala:265)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: alterTable - change database and table names *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$43.apply(VersionsSuite.scala:281)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$43.apply(VersionsSuite.scala:280)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: listTables(database) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$44.apply(VersionsSuite.scala:292)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$44.apply(VersionsSuite.scala:292)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: listTables(database, pattern) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$45.apply(VersionsSuite.scala:296)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$45.apply(VersionsSuite.scala:295)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: dropTable *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$46.apply(VersionsSuite.scala:305)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$46.apply(VersionsSuite.scala:300)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: sql create partitioned table *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$47.apply(VersionsSuite.scala:330)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$47.apply(VersionsSuite.scala:330)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: createPartitions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$6.apply$mcV$sp(VersionsSuite.scala:339)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$6.apply(VersionsSuite.scala:335)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$6.apply(VersionsSuite.scala:335)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.14: getPartitionNames(catalogTable) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$48.apply(VersionsSuite.scala:345)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$48.apply(VersionsSuite.scala:343)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getPartitions(catalogTable) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$49.apply(VersionsSuite.scala:350)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$49.apply(VersionsSuite.scala:349)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getPartitionsByFilter *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$50.apply(VersionsSuite.scala:355)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getPartition *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$51.apply(VersionsSuite.scala:368)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$51.apply(VersionsSuite.scala:368)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getPartitionOption(db: String, table: String, spec: TablePartitionSpec) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$52.apply(VersionsSuite.scala:372)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$52.apply(VersionsSuite.scala:371)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$53.apply(VersionsSuite.scala:379)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$53.apply(VersionsSuite.scala:377)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getPartitions(db: String, table: String) *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$54.apply(VersionsSuite.scala:384)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$54.apply(VersionsSuite.scala:384)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: loadPartition *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$7.apply$mcV$sp(VersionsSuite.scala:392)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$7.apply(VersionsSuite.scala:387)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$7.apply(VersionsSuite.scala:387)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.14: loadDynamicPartitions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$8.apply$mcV$sp(VersionsSuite.scala:407)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$8.apply(VersionsSuite.scala:402)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$8.apply(VersionsSuite.scala:402)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.14: renamePartitions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$55.apply(VersionsSuite.scala:419)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$55.apply(VersionsSuite.scala:416)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: alterPartitions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$56.apply(VersionsSuite.scala:434)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$56.apply(VersionsSuite.scala:425)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: dropPartitions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$57.apply(VersionsSuite.scala:447)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$57.apply(VersionsSuite.scala:441)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: createFunction *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$58.apply(VersionsSuite.scala:477)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: functionExists *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$59.apply(VersionsSuite.scala:486)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: renameFunction *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$60.apply(VersionsSuite.scala:497)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: alterFunction *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$61.apply(VersionsSuite.scala:510)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getFunction *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$62.apply(VersionsSuite.scala:522)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getFunctionOption *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$64.apply(VersionsSuite.scala:532)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: listFunctions *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$65.apply(VersionsSuite.scala:542)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: dropFunction *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$66.apply(VersionsSuite.scala:554)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: sql set command *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$67.apply(VersionsSuite.scala:564)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$67.apply(VersionsSuite.scala:564)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: sql create index and reset *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$68.apply(VersionsSuite.scala:568)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$68.apply(VersionsSuite.scala:567)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[32m- 0.14: sql read hive materialized view[0m
[31m- 0.14: version *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$70.apply(VersionsSuite.scala:589)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$70.apply(VersionsSuite.scala:589)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: getConf *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$71.apply(VersionsSuite.scala:593)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$71.apply(VersionsSuite.scala:593)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: setOut *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$13.apply$mcV$sp(VersionsSuite.scala:597)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$13.apply(VersionsSuite.scala:597)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$13.apply(VersionsSuite.scala:597)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.14: setInfo *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$14.apply$mcV$sp(VersionsSuite.scala:601)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$14.apply(VersionsSuite.scala:601)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$14.apply(VersionsSuite.scala:601)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.14: setError *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$15.apply$mcV$sp(VersionsSuite.scala:605)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$15.apply(VersionsSuite.scala:605)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$15.apply(VersionsSuite.scala:605)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.14: newSession *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$72.apply(VersionsSuite.scala:609)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$72.apply(VersionsSuite.scala:608)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
[31m- 0.14: withHiveState and addJar *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$16.apply$mcZ$sp(VersionsSuite.scala:615)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$16.apply(VersionsSuite.scala:613)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$16.apply(VersionsSuite.scala:613)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  ...[0m
[31m- 0.14: reset *** FAILED ***[0m
[31m  java.lang.NullPointerException:[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$73.apply(VersionsSuite.scala:629)[0m
[31m  at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$73.apply(VersionsSuite.scala:627)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[0m
[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)[0m
[31m  at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[0m
[31m  at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[0m
[31m  ...[0m
22:28:48.907 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:48.907 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:48.939 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:48.939 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:48.968 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:48.968 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:48.988 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:48.988 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.037 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.037 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.057 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.057 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.077 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.077 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.097 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.097 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.151 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.151 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.177 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/home/test/spark/sql/hive/target/tmp/spark-df9bb619-f936-40c0-a34a-1386add0f852/tbl specified for non-external table:tbl
22:28:49.183 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.183 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.227 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.228 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.272 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.272 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.320 WARN org.apache.hadoop.hive.common.FileUtils: Error setting permissions of file:/home/test/spark/sql/hive/target/tmp/scratch-79c35f02-f66f-4a84-8a41-29228a98605e/hive_2019-10-20_22-28-49_303_7126774570363027911-1
java.lang.RuntimeException: Error while running command to get file permissions : java.io.IOException: Cannot run program "/bin/ls": error=24, Too many open files
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:522)
	at org.apache.hadoop.util.Shell.run(Shell.java:478)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:766)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:859)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:842)
	at org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)
	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:587)
	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getGroup(RawLocalFileSystem.java:578)
	at org.apache.hadoop.hive.shims.Hadoop23Shims.setFullFileStatus(Hadoop23Shims.java:724)
	at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:527)
	at org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.oldVersionExternalTempPath(SaveAsHiveFile.scala:171)
	at org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.getExternalTmpPath(SaveAsHiveFile.scala:129)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.getExternalTmpPath(InsertIntoHiveTable.scala:66)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:96)
	at org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:88)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:115)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$17$$anonfun$apply$mcV$sp$1.apply$mcV$sp(VersionsSuite.scala:639)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$17.apply$mcV$sp(VersionsSuite.scala:638)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$17.apply(VersionsSuite.scala:638)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$17.apply(VersionsSuite.scala:638)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
Caused by: java.io.IOException: error=24, Too many open files
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 86 more

	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:625)
	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getGroup(RawLocalFileSystem.java:578)
	at org.apache.hadoop.hive.shims.Hadoop23Shims.setFullFileStatus(Hadoop23Shims.java:724)
	at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:527)
	at org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.oldVersionExternalTempPath(SaveAsHiveFile.scala:171)
	at org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.getExternalTmpPath(SaveAsHiveFile.scala:129)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.getExternalTmpPath(InsertIntoHiveTable.scala:66)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:96)
	at org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:88)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:115)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$17$$anonfun$apply$mcV$sp$1.apply$mcV$sp(VersionsSuite.scala:639)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:71)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$17.apply$mcV$sp(VersionsSuite.scala:638)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$17.apply(VersionsSuite.scala:638)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$17.apply(VersionsSuite.scala:638)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:103)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1210)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1257)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1255)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1255)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:30)
	at org.scalatest.Suite$class.run(Suite.scala:1144)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.main(Runner.scala:827)
	at org.scalatest.tools.Runner.main(Runner.scala)
22:28:49.335 WARN org.apache.hadoop.fs.FileUtil: Failed to delete file or dir [/home/test/spark/sql/hive/target/tmp/scratch-79c35f02-f66f-4a84-8a41-29228a98605e/hive_2019-10-20_22-28-49_303_7126774570363027911-1]: it still exists.
22:28:49.341 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.341 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.377 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.378 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.403 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.403 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.435 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.440 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.529 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user hadoop: id: 'hadoop': no such user
id: 'hadoop': no such user

22:28:49.530 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.530 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.583 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.583 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.614 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.614 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.639 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.641 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.673 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.673 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.694 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
22:28:49.694 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user hadoop
[31m*** RUN ABORTED ***[0m
[31m  java.lang.NoClassDefFoundError: org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$9[0m
[31m  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)[0m
[31m  at org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.saveAsHiveFile(SaveAsHiveFile.scala:86)[0m
[31m  at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.saveAsHiveFile(InsertIntoHiveTable.scala:66)[0m
[31m  at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.processInsert(InsertIntoHiveTable.scala:195)[0m
[31m  at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:99)[0m
[31m  at org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:88)[0m
[31m  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)[0m
[31m  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)[0m
[31m  at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:115)[0m
[31m  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)[0m
[31m  ...[0m
[31m  Cause: java.lang.ClassNotFoundException: org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$9[0m
[31m  at java.net.URLClassLoader$ClassFinder.run(URLClassLoader.java:1088)[0m
[31m  at java.security.AccessController.doPrivileged(AccessController.java:739)[0m
[31m  at java.net.URLClassLoader.findClass(URLClassLoader.java:589)[0m
[31m  at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:934)[0m
[31m  at java.lang.ClassLoader.loadClass(ClassLoader.java:879)[0m
[31m  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)[0m
[31m  at java.lang.ClassLoader.loadClass(ClassLoader.java:862)[0m
[31m  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)[0m
[31m  at org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.saveAsHiveFile(SaveAsHiveFile.scala:86)[0m
[31m  at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.saveAsHiveFile(InsertIntoHiveTable.scala:66)[0m
[31m  ...[0m
[31m  Cause: java.io.FileNotFoundException: /home/test/spark/sql/core/target/scala-2.11/classes/org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$9.class (Too many open files)[0m
[31m  at java.io.FileInputStream.open0(Native Method)[0m
[31m  at java.io.FileInputStream.open(FileInputStream.java:195)[0m
[31m  at java.io.FileInputStream.<init>(FileInputStream.java:138)[0m
[31m  at sun.misc.URLClassPath$FileLoader$1.getInputStream(URLClassPath.java:1455)[0m
[31m  at sun.misc.Resource.cachedInputStream(Resource.java:100)[0m
[31m  at sun.misc.Resource.getByteBuffer(Resource.java:183)[0m
[31m  at java.net.URLClassLoader.defineClass(URLClassLoader.java:671)[0m
[31m  at java.net.URLClassLoader.access$400(URLClassLoader.java:89)[0m
[31m  at java.net.URLClassLoader$ClassFinder.run(URLClassLoader.java:1086)[0m
[31m  at java.security.AccessController.doPrivileged(AccessController.java:739)[0m
[31m  ...[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------< [0;36morg.apache.spark:spark-repl_2.11[0;1m >------------------[m
[[1;34mINFO[m] [1mBuilding Spark Project REPL 2.4.4                                [18/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/repl/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/repl/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/spark/graphx/target/scala-2.11/classes:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/spark/mllib-local/target/scala-2.11/classes:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/spark/streaming/target/scala-2.11/classes:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/spark/mllib/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mbuild-helper-maven-plugin:3.0.0:add-source[m [1m(add-scala-sources)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Source directory: /home/test/spark/repl/src/main/scala-2.11 added.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/repl/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:29:09 AM [17.897s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mbuild-helper-maven-plugin:3.0.0:add-test-source[m [1m(add-scala-test-sources)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Test Source directory: /home/test/spark/repl/src/test/scala-2.11 added.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/spark/graphx/target/scala-2.11/classes:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/spark/mllib-local/target/scala-2.11/classes:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/core/target/scala-2.11/test-classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/spark/streaming/target/scala-2.11/classes:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/spark/mllib/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:29:35 AM [25.912s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-repl_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-repl_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 261 milliseconds.[0m
[36mRun starting. Expected test count is: 36[0m
[32mExecutorClassLoaderSuite:[0m
[32m- child over system classloader[0m
[32m- child first[0m
[32m- parent first[0m
[32m- child first can fall back[0m
[32m- child first can fail[0m
[32m- resource from parent[0m
[32m- resources from parent[0m
[32m- fetch classes using Spark's RpcEnv[0m
[32mSingletonReplSuite:[0m
Spark context available as 'sc' (master = local-cluster[2,1,1024], app id = app-20191021053000-0000).
Spark session available as 'spark'.
[32m- simple foreach with accumulator[0m
[32m- external vars[0m
[32m- external classes[0m
[32m- external functions[0m
[32m- external functions that access vars[0m
[32m- broadcast vars[0m
[32m- interacting with files[0m
[32m- local-cluster mode[0m
[32m- SPARK-1199 two instances of same class don't type check.[0m
[32m- SPARK-2452 compound statements.[0m
[32m- SPARK-2576 importing implicits[0m
[32m- Datasets and encoders[0m
[32m- SPARK-2632 importing a method from non serializable class and not using it.[0m
[32m- collecting objects of class defined in repl[0m
[32m- collecting objects of class defined in repl - shuffling[0m
[32m- replicating blocks of object with class defined in repl[0m
[31m- should clone and clean line object in ClosureCleaner *** FAILED ***[0m
[31m  isContain was true Interpreter output contained 'AssertionError':[0m
  
[31m  scala> import org.apache.spark.rdd.RDD[0m
  
[31m  scala> [0m
[31m  scala> lines: org.apache.spark.rdd.RDD[String] = pom.xml MapPartitionsRDD[46] at textFile at <console>:40[0m
  
[31m  scala> defined class Data[0m
  
[31m  scala> dataRDD: org.apache.spark.rdd.RDD[Data] = MapPartitionsRDD[47] at map at <console>:43[0m
  
[31m  scala> res28: Long = 170[0m
  
[31m  scala> repartitioned: org.apache.spark.rdd.RDD[Data] = MapPartitionsRDD[51] at repartition at <console>:41[0m
  
[31m  scala> res29: Long = 170[0m
  
[31m  scala> [0m
[31m  scala>      |      | getCacheSize: (rdd: org.apache.spark.rdd.RDD[_])Long[0m
  
[31m  scala> cacheSize1: Long = 23408[0m
  
[31m  scala> cacheSize2: Long = 16776[0m
  
[31m  scala> [0m
[31m  scala> [0m
[31m  scala> deviation: Double = 0.2833219412166781[0m
  
[31m  scala>      | java.lang.AssertionError: assertion failed: deviation too large: 0.2833219412166781, first size: 23408, second size: 16776[0m
[31m    at scala.Predef$.assert(Predef.scala:170)[0m
[31m    ... 44 elided[0m
  
[31m  scala>      | _result_1571635851326: Int = 1[0m
  
[31m  scala> (SingletonReplSuite.scala:121)[0m
[32m- newProductSeqEncoder with REPL defined class[0m
[32mReplSuite:[0m
[32m- propagation of local properties[0m
Spark context available as 'sc' (master = local, app id = local-1571635860671).
Spark session available as 'spark'.
[32m- SPARK-15236: use Hive catalog[0m
Spark context available as 'sc' (master = local, app id = local-1571635865197).
Spark session available as 'spark'.
[32m- SPARK-15236: use in-memory catalog[0m
Spark context available as 'sc' (master = local, app id = local-1571635868458).
Spark session available as 'spark'.
[32m- broadcast vars[0m
Spark context available as 'sc' (master = local, app id = local-1571635872669).
Spark session available as 'spark'.
[32m- line wrapper only initialized once when used as encoder outer scope[0m
Spark context available as 'sc' (master = local-cluster[1,1,1024], app id = app-20191021053118-0000).
Spark session available as 'spark'.

// Exiting paste mode, now interpreting.

[32m- define case class and create Dataset together with paste mode[0m
Spark context available as 'sc' (master = local, app id = local-1571635882371).
Spark session available as 'spark'.
Spark context available as 'sc' (master = local, app id = local-1571635882371).
Spark session available as 'spark'.
[32m- :replay should work correctly[0m
Spark context available as 'sc' (master = local, app id = local-1571635889275).
Spark session available as 'spark'.
[32m- spark-shell should find imported types in class constructors and extends clause[0m
Spark context available as 'sc' (master = local, app id = local-1571635892302).
Spark session available as 'spark'.
Spark context available as 'sc' (master = local, app id = local-1571635895263).
Spark session available as 'spark'.
[32m- spark-shell should shadow val/def definitions correctly[0m
Spark context available as 'sc' (master = local-cluster[1,1,1024], app id = app-20191021053140-0000).
Spark session available as 'spark'.

// Exiting paste mode, now interpreting.

[32m- SPARK-26633: ExecutorClassLoader.getResourceAsStream find REPL classes[0m
[36mRun completed in 2 minutes, 2 seconds.[0m
[36mTotal number of tests run: 36[0m
[36mSuites: completed 4, aborted 0[0m
[36mTests: succeeded 35, failed 1, canceled 0, ignored 0, pending 0[0m
[31m*** 1 TEST FAILED ***[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------< [0;36morg.apache.spark:spark-assembly_2.11[0;1m >----------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Assembly 2.4.4                            [19/24][m
[[1;34mINFO[m] [1m--------------------------------[ pom ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/assembly/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/assembly/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/spark/graphx/target/scala-2.11/classes:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/spark/mllib-local/target/scala-2.11/classes:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/spark/streaming/target/scala-2.11/classes:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/spark/repl/target/scala-2.11/classes:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/spark/mllib/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/spark/graphx/target/scala-2.11/classes:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/spark/mllib-local/target/scala-2.11/classes:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/spark/streaming/target/scala-2.11/classes:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/spark/repl/target/scala-2.11/classes:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/spark/mllib/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-assembly_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 81 milliseconds.[0m
[36mRun starting. Expected test count is: 0[0m
[32mDiscoverySuite:[0m
[36mRun completed in 187 milliseconds.[0m
[36mTotal number of tests run: 0[0m
[36mSuites: completed 1, aborted 0[0m
[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m
[33mNo tests were executed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------< [0;36morg.apache.spark:spark-streaming-kafka-0-10_2.11[0;1m >----------[m
[[1;34mINFO[m] [1mBuilding Spark Integration for Kafka 0.10 2.4.4                  [20/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/external/kafka-0-10/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/external/kafka-0-10/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/external/kafka-0-10/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:31:56 AM [2.355s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/typesafe/scala-logging/scala-logging_2.11/3.9.0/scala-logging_2.11-3.9.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/core/target/scala-2.11/test-classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/streaming/target/scala-2.11/classes:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/net/sf/jopt-simple/jopt-simple/3.2/jopt-simple-3.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/apache/kafka/kafka_2.11/2.0.0/kafka_2.11-2.0.0.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:32:03 AM [6.527s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-streaming-kafka-0-10_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 552 milliseconds.[0m
[36mRun starting. Expected test count is: 20[0m
[32mKafkaRDDSuite:[0m
[32m- basic usage[0m
[32m- compacted topic[0m
[32m- iterator boundary conditions[0m
[32m- executor sorting[0m
[32mDirectKafkaStreamSuite:[0m
[32m- basic stream receiving with multiple topics and smallest starting offset[0m
[32m- pattern based subscription[0m
[32m- receiving from largest starting offset[0m
[32m- creating stream by offset[0m
[32m- offset recovery[0m
[32m- offset recovery from kafka[0m
[32m- Direct Kafka stream report input information[0m
[32m- maxMessagesPerPartition with backpressure disabled[0m
[32m- maxMessagesPerPartition with no lag[0m
[32m- maxMessagesPerPartition respects max rate[0m
[32m- using rate controller[0m
[32m- backpressure.initialRate should honor maxRatePerPartition[0m
[32m- use backpressure.initialRate with backpressure[0m
[32m- maxMessagesPerPartition with zero offset and rate equal to the specified minimum with default 1[0m
[32mKafkaDataConsumerSuite:[0m
[32m- KafkaDataConsumer reuse in case of same groupId and TopicPartition[0m
[32m- concurrent use of KafkaDataConsumer[0m
[36mRun completed in 4 minutes, 53 seconds.[0m
[36mTotal number of tests run: 20[0m
[36mSuites: completed 4, aborted 0[0m
[36mTests: succeeded 20, failed 0, canceled 0, ignored 0, pending 0[0m
[32mAll tests passed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------< [0;36morg.apache.spark:spark-sql-kafka-0-10_2.11[0;1m >-------------[m
[[1;34mINFO[m] [1mBuilding Kafka 0.10+ Source for Structured Streaming 2.4.4       [21/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/external/kafka-0-10-sql/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/external/kafka-0-10-sql/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:37:17 AM [18.172s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 4 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/typesafe/scala-logging/scala-logging_2.11/3.9.0/scala-logging_2.11-3.9.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/spark/sql/core/target/spark-sql_2.11-2.4.4-tests.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/spark/common/tags/target/scala-2.11/test-classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/core/target/scala-2.11/test-classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/spark/sql/catalyst/target/spark-catalyst_2.11-2.4.4-tests.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/net/sf/jopt-simple/jopt-simple/3.2/jopt-simple-3.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/apache/kafka/kafka_2.11/2.0.0/kafka_2.11-2.0.0.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:37:40 AM [22.691s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-sql-kafka-0-10_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 949 milliseconds.[0m
[36mRun starting. Expected test count is: 171[0m
[32mKafkaMicroBatchV2SourceSuite:[0m
[32m- cannot stop Kafka stream[0m
[32m- assign from latest offsets (failOnDataLoss: true)[0m
[32m- assign from earliest offsets (failOnDataLoss: true)[0m
[32m- assign from specific offsets (failOnDataLoss: true)[0m
[32m- subscribing topic by name from latest offsets (failOnDataLoss: true)[0m
[32m- subscribing topic by name from earliest offsets (failOnDataLoss: true)[0m
[32m- subscribing topic by name from specific offsets (failOnDataLoss: true)[0m
[32m- subscribing topic by pattern from latest offsets (failOnDataLoss: true)[0m
[32m- subscribing topic by pattern from earliest offsets (failOnDataLoss: true)[0m
[32m- subscribing topic by pattern from specific offsets (failOnDataLoss: true)[0m
[32m- assign from latest offsets (failOnDataLoss: false)[0m
[32m- assign from earliest offsets (failOnDataLoss: false)[0m
[32m- assign from specific offsets (failOnDataLoss: false)[0m
[32m- subscribing topic by name from latest offsets (failOnDataLoss: false)[0m
[32m- subscribing topic by name from earliest offsets (failOnDataLoss: false)[0m
[32m- subscribing topic by name from specific offsets (failOnDataLoss: false)[0m
[32m- subscribing topic by pattern from latest offsets (failOnDataLoss: false)[0m
[32m- subscribing topic by pattern from earliest offsets (failOnDataLoss: false)[0m
[32m- subscribing topic by pattern from specific offsets (failOnDataLoss: false)[0m
[32m- bad source options[0m
[32m- unsupported kafka configs[0m
[32m- get offsets from case insensitive parameters[0m
[32m- Kafka column types[0m
[32m- (de)serialization of initial offsets[0m
[32m- SPARK-26718 Rate limit set to Long.Max should not overflow integer during end offset calculation[0m
[32m- maxOffsetsPerTrigger[0m
[32m- input row metrics[0m
[32m- subscribing topic by pattern with topic deletions[0m
[32m- subscribe topic by pattern with topic recreation between batches[0m
[32m- ensure that initial offset are written with an extra byte in the beginning (SPARK-19517)[0m
[32m- deserialization of initial offset written by Spark 2.1.0 (SPARK-19517)[0m
[32m- deserialization of initial offset written by future version[0m
[32m- KafkaSource with watermark[0m
[32m- delete a topic when a Spark job is running[0m
[32m- SPARK-22956: currentPartitionOffsets should be set when no new data comes in[0m
[32m- ensure stream-stream self-join generates only one offset in log and correct metrics[0m
[32m- read Kafka transactional messages: read_committed[0m
[32m- read Kafka transactional messages: read_uncommitted[0m
[32m- SPARK-25495: FetchedData.reset should reset all fields[0m
[32m- SPARK-27494: read kafka record containing null key/values.[0m
[32m- V2 Source is used by default[0m
[32m- minPartitions is supported[0m
[32mKafkaContinuousSourceStressForDontFailOnDataLossSuite:[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------< [0;36morg.apache.spark:spark-examples_2.11[0;1m >----------------[m
[[1;34mINFO[m] [1mBuilding Spark Project Examples 2.4.4                            [22/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/examples/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/examples/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/com/github/scopt/scopt_2.11/3.7.0/scopt_2.11-3.7.0.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 10 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:40:51 AM [27.688s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/examples/src/test/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/test/.m2/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/test/.m2/repository/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/jpmml/pmml-model/1.2.15/pmml-model-1.2.15.jar:/home/test/.m2/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/home/test/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/spark/graphx/target/scala-2.11/classes:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/spark/mllib-local/target/scala-2.11/classes:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/spark/sql/hive/target/scala-2.11/classes:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/jpmml/pmml-schema/1.2.15/pmml-schema-1.2.15.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/spark/streaming/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/calcite/calcite-avatica/1.2.0-incubating/calcite-avatica-1.2.0-incubating.jar:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/calcite/calcite-linq4j/1.2.0-incubating/calcite-linq4j-1.2.0-incubating.jar:/home/test/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/test/.m2/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/net/hydromatic/eigenbase-properties/1.1.5/eigenbase-properties-1.1.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/spark/external/kafka-0-10-sql/target/scala-2.11/classes:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/derby/derby/10.12.1.1/derby-10.12.1.1.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/iq80/snappy/snappy/0.2/snappy-0.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/calcite/calcite-core/1.2.0-incubating/calcite-core-1.2.0-incubating.jar:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/home/test/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/com/github/scopt/scopt_2.11/3.7.0/scopt_2.11-3.7.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/org/spark-project/hive/hive-metastore/1.2.1.spark2/hive-metastore-1.2.1.spark2.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/spark/mllib/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/spark/external/kafka-0-10/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-examples_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-examples_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 688 milliseconds.[0m
[36mRun starting. Expected test count is: 0[0m
[32mDiscoverySuite:[0m
[36mRun completed in 730 milliseconds.[0m
[36mTotal number of tests run: 0[0m
[36mSuites: completed 1, aborted 0[0m
[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m
[33mNo tests were executed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----< [0;36morg.apache.spark:spark-streaming-kafka-0-10-assembly_2.11[0;1m >------[m
[[1;34mINFO[m] [1mBuilding Spark Integration for Kafka 0.10 Assembly 2.4.4         [23/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/external/kafka-0-10-assembly/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/external/kafka-0-10-assembly/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/spark/external/kafka-0-10/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/external/kafka-0-10-assembly/src/main/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/test/spark/external/kafka-0-10-assembly/src/test/resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/streaming/target/scala-2.11/classes:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/spark/external/kafka-0-10/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-streaming-kafka-0-10-assembly_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 78 milliseconds.[0m
[36mRun starting. Expected test count is: 0[0m
[32mDiscoverySuite:[0m
[36mRun completed in 121 milliseconds.[0m
[36mTotal number of tests run: 0[0m
[36mSuites: completed 1, aborted 0[0m
[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m
[33mNo tests were executed.[0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------< [0;36morg.apache.spark:spark-avro_2.11[0;1m >------------------[m
[[1;34mINFO[m] [1mBuilding Spark Avro 2.4.4                                        [24/24][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M1:enforce[m [1m(enforce-versions)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:add-source[m [1m(eclipse-add-source)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Add Source directory: /home/test/spark/external/avro/src/main/scala
[[1;34mINFO[m] Add Test Source directory: /home/test/spark/external/avro/src/test/scala
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(default-cli)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-remote-resources-plugin:1.5:process[m [1m(process-resource-bundles)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:resources[m [1m(default-resources)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:compile[m [1m(default-compile)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling main sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:compile[m [1m(scala-compile-first)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:41:22 AM [19.632s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-antrun-plugin:1.8:run[m [1m(create-tmp-dir)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Executing tasks

main:
[[1;34mINFO[m] Executed tasks
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.7:testResources[m [1m(default-testResources)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 16 resources
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.7.0:testCompile[m [1m(default-testCompile)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Not compiling test sources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.0.2:build-classpath[m [1m(generate-test-classpath)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Dependencies classpath:
/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/test/.m2/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/test/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/test/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/test/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/test/.m2/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/test/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/test/.m2/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/test/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/test/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/test/spark/sql/core/target/spark-sql_2.11-2.4.4-tests.jar:/home/test/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/test/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/test/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/test/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/test/spark/launcher/target/scala-2.11/classes:/home/test/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/test/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/test/spark/sql/catalyst/target/scala-2.11/classes:/home/test/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/test/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/home/test/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/test/.m2/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/test/.m2/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/test/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/test/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/test/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/test/spark/sql/core/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/test/.m2/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/test/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/test/.m2/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/test/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/test/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/test/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/test/.m2/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/test/spark/common/unsafe/target/scala-2.11/classes:/home/test/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/test/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/test/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/test/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/test/.m2/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/test/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/test/.m2/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/test/.m2/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/test/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/test/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/test/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/test/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/test/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/test/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/test/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/test/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/test/spark/core/target/scala-2.11/test-classes:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/test/spark/common/tags/target/scala-2.11/classes:/home/test/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/test/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/test/.m2/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/test/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/test/spark/common/network-shuffle/target/scala-2.11/classes:/home/test/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/test/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/test/spark/common/sketch/target/scala-2.11/classes:/home/test/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/test/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/test/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/test/spark/core/target/scala-2.11/classes:/home/test/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/test/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/test/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/test/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/test/.m2/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/test/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/test/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/test/spark/common/kvstore/target/scala-2.11/classes:/home/test/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/test/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/test/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/test/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/test/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/test/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/test/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/test/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/test/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/test/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/test/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/test/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/test/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/test/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/test/.m2/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/test/.m2/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/test/spark/sql/catalyst/target/spark-catalyst_2.11-2.4.4-tests.jar:/home/test/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/test/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/test/.m2/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/test/.m2/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/test/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/test/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/test/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/test/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/test/.m2/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/test/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/test/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/test/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/test/.m2/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/test/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/test/.m2/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/test/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/test/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/test/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/test/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/test/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/test/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/test/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/test/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/test/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/test/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/test/.m2/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/test/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/test/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/test/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/test/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/test/.m2/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/test/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/test/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/test/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/test/spark/common/network-common/target/scala-2.11/classes:/home/test/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/test/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/test/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.2.2:testCompile[m [1m(scala-test-compile-first)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Using zinc server for incremental compilation
[0m[[0minfo[0m] [0mCompile success at Oct 21, 2019 5:41:46 AM [23.255s][0m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(default-test)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.22.0:test[m [1m(test)[m @ [36mspark-avro_2.11[0;1m ---[m
[[1;34mINFO[m] Skipping execution of surefire because it has already been run for this configuration
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscalatest-maven-plugin:1.0:test[m [1m(test)[m @ [36mspark-avro_2.11[0;1m ---[m
[36mDiscovery starting.[0m
[36mDiscovery completed in 548 milliseconds.[0m
[36mRun starting. Expected test count is: 95[0m
[32mAvroCatalystDataConversionSuite:[0m
[32m- single BooleanType with seed -6055389607023935864[0m
[32m- single ByteType with seed -1672218704630943555[0m
[32m- single ShortType with seed 675586950067932336[0m
[32m- single IntegerType with seed 1700229586725242861[0m
[32m- single LongType with seed -1444379235340175229[0m
[32m- single FloatType with seed 9044532787234505327[0m
[32m- single DoubleType with seed 728811365811154565[0m
[32m- single DecimalType(8,0) with seed 92381372176458903[0m
[32m- single DecimalType(8,4) with seed -1555722181749375325[0m
[32m- single DecimalType(16,0) with seed 7850906653177733463[0m
[32m- single DecimalType(16,11) with seed 3548500414206690862[0m
[32m- single DecimalType(38,0) with seed 3685379912953920997[0m
[32m- single DecimalType(38,38) with seed 8929365983729940007[0m
[32m- single StringType with seed -6265682854784349285[0m
[32m- single BinaryType with seed 2430036848985356733[0m
[32m- flat schema struct<col_0:decimal(38,0),col_1:decimal(8,0),col_2:decimal(8,0),col_3:int,col_4:binary> with seed -9173844264822776801[0m
[32m- flat schema struct<col_0:decimal(8,4),col_1:decimal(16,0),col_2:int,col_3:decimal(8,4),col_4:smallint> with seed -1927228656526540525[0m
[32m- flat schema struct<col_0:smallint,col_1:string,col_2:decimal(16,0),col_3:int,col_4:string> with seed -19753121466330173[0m
[32m- flat schema struct<col_0:smallint,col_1:tinyint,col_2:decimal(16,11),col_3:smallint,col_4:decimal(38,38)> with seed -2002636490986572731[0m
[32m- flat schema struct<col_0:bigint,col_1:float,col_2:decimal(38,0),col_3:decimal(16,0),col_4:smallint> with seed 476354994644008494[0m
[32m- nested schema struct<col_0:array<smallint>,col_1:struct<col_0:decimal(8,4),col_1:struct<col_0:array<boolean>>,col_2:array<smallint>>,col_2:struct<col_0:struct<col_0:float>>,col_3:struct<col_0:int,col_1:decimal(38,38),col_2:struct<col_0:decimal(38,38)>,col_3:array<decimal(16,11)>>,col_4:array<float>> with seed -15626876087971156[0m
[32m- nested schema struct<col_0:struct<col_0:array<int>,col_1:array<bigint>,col_2:array<float>,col_3:struct<col_0:struct<col_0:array<decimal(38,0)>>>,col_4:struct<col_0:struct<col_0:struct<col_0:struct<col_0:array<tinyint>>>>>,col_5:decimal(8,0),col_6:struct<col_0:array<decimal(16,11)>>>,col_1:array<double>,col_2:struct<col_0:float>,col_3:array<decimal(38,0)>> with seed -5655674468084414149[0m
[32m- nested schema struct<col_0:decimal(38,0),col_1:struct<col_0:boolean,col_1:struct<col_0:struct<col_0:array<decimal(16,11)>>,col_1:decimal(38,38),col_2:decimal(8,4)>,col_2:struct<col_0:smallint,col_1:float,col_2:struct<col_0:array<decimal(8,4)>>>,col_3:binary>,col_2:array<decimal(38,38)>> with seed -5417138365781523871[0m
[32m- nested schema struct<col_0:array<int>,col_1:array<decimal(38,0)>,col_2:struct<col_0:array<decimal(38,38)>,col_1:decimal(38,0),col_2:struct<col_0:array<decimal(16,0)>>>,col_3:array<decimal(16,11)>,col_4:decimal(38,0),col_5:array<decimal(8,0)>,col_6:array<int>,col_7:array<string>> with seed -7268325197863138783[0m
[32m- nested schema struct<col_0:struct<col_0:array<string>,col_1:struct<col_0:struct<col_0:float>,col_1:smallint>,col_2:struct<col_0:struct<col_0:decimal(38,38)>>>,col_1:array<decimal(16,11)>,col_2:array<string>,col_3:struct<col_0:array<boolean>,col_1:decimal(16,0),col_2:array<bigint>>,col_4:array<decimal(38,38)>> with seed 7357191720038224896[0m
[32m- read int as string[0m
[32m- read string as int[0m
[32m- read float as double[0m
[32m- read double as float[0m
[32mAvroFunctionsSuite:[0m
[32m- roundtrip in to_avro and from_avro - int and string[0m
[32m- roundtrip in to_avro and from_avro - struct[0m
[32m- roundtrip in to_avro and from_avro - array with null[0m
[32m- SPARK-27798: from_avro produces same value when converted to local relation[0m
[32mAvroLogicalTypeSuite:[0m
[32m- Logical type: date[0m
[32m- Logical type: timestamp_millis[0m
[32m- Logical type: timestamp_micros[0m
[32m- Logical type: user specified output schema with different timestamp types[0m
[32m- Read Long type as Timestamp[0m
[32m- Logical type: user specified read schema[0m
[32m- Logical type: Decimal[0m
[32m- Logical type: write Decimal with BYTES type[0m
[32m- Logical type: Decimal with too large precision[0m
[32mAvroSuite:[0m
[32m- resolve avro data source[0m
[32m- reading from multiple paths[0m
[32m- reading and writing partitioned data[0m
[32m- request no fields[0m
[32m- convert formats[0m
[32m- rearrange internal schema[0m
[32m- test NULL avro type[0m
[32m- union(int, long) is read as long[0m
[32m- union(float, double) is read as double[0m
[32m- union(float, double, null) is read as nullable double[0m
[32m- Union of a single type[0m
[32m- SPARK-27858 Union type: More than one non-null type[0m
[32m- Complex Union Type[0m
[32m- Lots of nulls[0m
[32m- Struct field type[0m
[32m- Date field type[0m
[32m- Array data types[0m
[32m- write with compression - sql configs[0m
[32m- dsl test[0m
[32m- old avro data source name works[0m
[32m- support of various data types[0m
[32m- sql test[0m
[32m- conversion to avro and back[0m
[32m- conversion to avro and back with namespace[0m
[32m- converting some specific sparkSQL types to avro[0m
[32m- correctly read long as date/timestamp type[0m
[32m- support of globbed paths[0m
[32m- does not coerce null date/timestamp value to 0 epoch.[0m
[32m- support user provided avro schema[0m
[32m- support user provided avro schema with defaults for missing fields[0m
[32m- support user provided avro schema for writing nullable enum type[0m
[32m- support user provided avro schema for writing non-nullable enum type[0m
[32m- support user provided avro schema for writing nullable fixed type[0m
[32m- support user provided avro schema for writing non-nullable fixed type[0m
[32m- throw exception if unable to write with user provided Avro schema[0m
[32m- reading from invalid path throws exception[0m
[32m- SQL test insert overwrite[0m
[32m- test save and load[0m
[32m- test load with non-Avro file[0m
[32m- read avro with user defined schema: read partial columns[0m
[32m- read avro with user defined schema: read non-exist columns[0m
[32m- read avro file partitioned[0m
[32m- Validate namespace in avro file that has nested records with the same name[0m
[32m- saving avro that has nested records with the same name[0m
[32m- check namespace - toAvroType[0m
[32m- check empty namespace - toAvroType[0m
[32m- saving avro that has nested records with the same name inside an array[0m
[32m- saving avro that has nested records with the same name inside a map[0m
[32m- SPARK-24805: do not ignore files without .avro extension by default[0m
[32m- SPARK-24836: checking the ignoreExtension option[0m
[32m- SPARK-24836: ignoreExtension must override hadoop's config[0m
[32m- SPARK-24881: write with compression - avro options[0m
[32m- Detect recursive loop[0m
[36mRun completed in 45 seconds, 58 milliseconds.[0m
[36mTotal number of tests run: 95[0m
[36mSuites: completed 5, aborted 0[0m
[36mTests: succeeded 95, failed 0, canceled 0, ignored 0, pending 0[0m
[32mAll tests passed.[0m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Summary for Spark Project Parent POM 2.4.4:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Spark Project Parent POM ........................... [1;32mSUCCESS[m [ 11.259 s]
[[1;34mINFO[m] Spark Project Tags ................................. [1;32mSUCCESS[m [ 12.246 s]
[[1;34mINFO[m] Spark Project Sketch ............................... [1;32mSUCCESS[m [ 44.113 s]
[[1;34mINFO[m] Spark Project Local DB ............................. [1;32mSUCCESS[m [  4.818 s]
[[1;34mINFO[m] Spark Project Networking ........................... [1;32mSUCCESS[m [ 11.668 s]
[[1;34mINFO[m] Spark Project Shuffle Streaming Service ............ [1;32mSUCCESS[m [  7.608 s]
[[1;34mINFO[m] Spark Project Unsafe ............................... [1;32mSUCCESS[m [ 21.475 s]
[[1;34mINFO[m] Spark Project Launcher ............................. [1;32mSUCCESS[m [ 10.521 s]
[[1;34mINFO[m] Spark Project Core ................................. [1;31mFAILURE[m [04:32 min]
[[1;34mINFO[m] Spark Project ML Local Library ..................... [1;32mSUCCESS[m [  3.870 s]
[[1;34mINFO[m] Spark Project GraphX ............................... [1;32mSUCCESS[m [02:45 min]
[[1;34mINFO[m] Spark Project Streaming ............................ [1;32mSUCCESS[m [05:41 min]
[[1;34mINFO[m] Spark Project Catalyst ............................. [1;31mFAILURE[m [15:31 min]
[[1;34mINFO[m] Spark Project SQL .................................. [1;31mFAILURE[m [06:31 min]
[[1;34mINFO[m] Spark Project ML Library ........................... [1;32mSUCCESS[m [31:18 min]
[[1;34mINFO[m] Spark Project Tools ................................ [1;32mSUCCESS[m [  8.605 s]
[[1;34mINFO[m] Spark Project Hive ................................. [1;31mFAILURE[m [08:53 min]
[[1;34mINFO[m] Spark Project REPL ................................. [1;31mFAILURE[m [02:58 min]
[[1;34mINFO[m] Spark Project Assembly ............................. [1;32mSUCCESS[m [  4.820 s]
[[1;34mINFO[m] Spark Integration for Kafka 0.10 ................... [1;32mSUCCESS[m [05:05 min]
[[1;34mINFO[m] Kafka 0.10+ Source for Structured Streaming ........ [1;31mFAILURE[m [03:23 min]
[[1;34mINFO[m] Spark Project Examples ............................. [1;32mSUCCESS[m [ 33.827 s]
[[1;34mINFO[m] Spark Integration for Kafka 0.10 Assembly .......... [1;32mSUCCESS[m [  6.646 s]
[[1;34mINFO[m] Spark Avro ......................................... [1;32mSUCCESS[m [01:36 min]
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;31mBUILD FAILURE[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  01:31 h
[[1;34mINFO[m] Finished at: 2019-10-21T05:42:40Z
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;31mERROR[m] Failed to execute goal [32morg.scalatest:scalatest-maven-plugin:1.0:test[m [1m(test)[m on project [36mspark-core_2.11[m: [1;31mThere are test failures[m -> [1m[Help 1][m
[[1;31mERROR[m] Failed to execute goal [32morg.scalatest:scalatest-maven-plugin:1.0:test[m [1m(test)[m on project [36mspark-catalyst_2.11[m: [1;31mThere are test failures[m -> [1m[Help 1][m
[[1;31mERROR[m] Failed to execute goal [32morg.scalatest:scalatest-maven-plugin:1.0:test[m [1m(test)[m on project [36mspark-sql_2.11[m: [1;31mThere are test failures[m -> [1m[Help 1][m
[[1;31mERROR[m] Failed to execute goal [32morg.scalatest:scalatest-maven-plugin:1.0:test[m [1m(test)[m on project [36mspark-hive_2.11[m: [1;31mThere are test failures[m -> [1m[Help 1][m
[[1;31mERROR[m] Failed to execute goal [32morg.scalatest:scalatest-maven-plugin:1.0:test[m [1m(test)[m on project [36mspark-repl_2.11[m: [1;31mThere are test failures[m -> [1m[Help 1][m
[[1;31mERROR[m] Failed to execute goal [32morg.scalatest:scalatest-maven-plugin:1.0:test[m [1m(test)[m on project [36mspark-sql-kafka-0-10_2.11[m: [1;31mThere are test failures[m -> [1m[Help 1][m
[[1;31mERROR[m] 
[[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.
[[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.
[[1;31mERROR[m] 
[[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:
[[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[[1;31mERROR[m] 
[[1;31mERROR[m] After correcting the problems, you can resume the build with the command
[[1;31mERROR[m]   [1mmvn <goals> -rf :spark-core_2.11[m
[[1;34mINFO[m] Build failures were ignored.
